{"meta":{"title":"金子爸爸の家","subtitle":"热爱生活，喜欢软件","description":"欢迎来到金子爸爸博客の家","author":"金子爸爸","url":"https://zhangxin66666.github.io","root":"/"},"pages":[{"title":"标签","date":"2022-01-11T01:48:58.203Z","updated":"2022-01-11T01:48:58.203Z","comments":true,"path":"tags/index.html","permalink":"https://zhangxin66666.github.io/tags/index.html","excerpt":"","text":""},{"title":"","date":"2024-08-22T03:23:14.738Z","updated":"2024-08-22T03:23:14.738Z","comments":true,"path":"js/chocolate.js","permalink":"https://zhangxin66666.github.io/js/chocolate.js","excerpt":"","text":"// 友情链接页面 头像找不到时 替换图片 if (location.href.indexOf(\"link\") !== -1) { var imgObj = document.getElementsByTagName(\"img\"); for (i = 0; i < imgObj.length; i++) { imgObj[i].onerror = function() { this.src = \"https://cdn.jsdelivr.net/gh/tzy13755126023/BLOG_SOURCE/theme_f/friend_404.gif\" } } } $(function() { // 气泡 function bubble() { $('#page-header').circleMagic({ radius: 10, density: .2, color: 'rgba(255,255,255,.4)', clearOffset: 0.99 }); }! function(p) { p.fn.circleMagic = function(t) { var o, a, n, r, e = !0, i = [], d = p.extend({ color: \"rgba(255,0,0,.5)\", radius: 10, density: .3, clearOffset: .2 }, t), l = this[0]; function c() { e = !(document.body.scrollTop > a) } function s() { o = l.clientWidth, a = l.clientHeight, l.height = a + \"px\", n.width = o, n.height = a } function h() { if (e) for (var t in r.clearRect(0, 0, o, a), i) i[t].draw(); requestAnimationFrame(h) } function f() { var t = this; function e() { t.pos.x = Math.random() * o, t.pos.y = a + 100 * Math.random(), t.alpha = .1 + Math.random() * d.clearOffset, t.scale = .1 + .3 * Math.random(), t.speed = Math.random(), \"random\" === d.color ? t.color = \"rgba(\" + Math.floor(255 * Math.random()) + \", \" + Math.floor(0 * Math.random()) + \", \" + Math.floor(0 * Math.random()) + \", \" + Math.random().toPrecision(2) + \")\" : t.color = d.color } t.pos = {}, e(), this.draw = function() { t.alpha"},{"title":"留言板","date":"2022-01-11T01:48:58.203Z","updated":"2022-01-11T01:48:58.203Z","comments":true,"path":"message/index.html","permalink":"https://zhangxin66666.github.io/message/index.html","excerpt":"","text":"本页面还在开发中……"},{"title":"关于我","date":"2024-08-22T03:23:14.738Z","updated":"2024-08-22T03:23:14.738Z","comments":true,"path":"关于我/index.html","permalink":"https://zhangxin66666.github.io/%E5%85%B3%E4%BA%8E%E6%88%91/index.html","excerpt":"","text":"关于我十年生死两茫茫,写程序，到天亮。千行代码，Bug何处藏。纵使上线又怎样，朝令改，夕断肠。领导每天新想法，天天改，日日忙。 相顾无言，惟有泪千行。每晚灯火阑珊处，程序员，又加班，工作狂~ 来一张楼主无美颜生活照，见笑了！🤣🤣🤣 基本信息 类别 信息 出生年月 1990年2月 现居地 北京市朝阳区 籍贯 辽宁省锦州市 邮箱 &#x38;&#x33;&#x34;&#x36;&#x31;&#x33;&#50;&#x40;&#113;&#113;&#46;&#x63;&#111;&#x6d; 教育经历 时间 学校 专业 备注 2009.09~2013.07 沈阳化工大学 电子科学与技术 统招本科 2006.09~2009.07 辽宁省北镇市高级中学 高级基础教育 市重点 工作经历 时间 公司 职位 2021.04~至今 龙湖集团 Java 开发工程师 2017.05~2021.04 包头市包银消费金融股份有限公司 Java 开发工程师 2016.09~2017.04 大连灵动科技发展有限公司 Java 开发工程师 2013.07~2016.08 大连安吉尼尔科技有限公司 Java 开发工程师 专业技能 Java 基础扎实、掌握 JVM 原理、多线程、网络原理、设计模式、常用的数据结构和算法 熟悉 Windows、Mac、Linux 操作系统，熟练使用 linux 常用操作指令 熟练使用 IntelliJ IDEA 开发工具(及各种插件)、熟练使用 Git 版本同步工具 阅读过 Spring、SpringMVC、等开源框架源码，理解其设计原理及底层架构，具备框架定制开发能力 理解 Redis 线程模型，Netty 线程模型，掌握基于响应式的异步非阻塞模型的基本原理，了解 Webflux 熟练掌握分布式缓存 Redis、Elaticsearch，对分布式锁，幂等等常见问题有深入研究及多年实战经验 熟悉常见消息中间件的使用，有多年 RabbitMQ 的实战开发经验，对高级消息队列有深入理解 熟练掌握 Mysql 事务，索引，锁，SQL 优化相关知识，可根据业务场景给出详细及高性能设计方案 熟练使用数据库操作框架 Mybatis、Mybatsi-plus 进行高效业务功能开发 掌握 springCloud 相关框架，对 SpringBoot、SpringCloud 原理有一定了解，有成熟项目经验 熟悉定时任务及延迟任务等业务相关设计，如 xxl-job，延迟消息等相关技术有多年开发经验 熟悉微服务思想，MVC 分层，DDD 理论，服务拆分，治理，监控，服务熔断，降级等相关能力 熟悉 jvm 原理，熟悉垃圾回收以 jvm 性能调优技术，有过线上服务器性能监测及调优经验 熟悉多线程及线程池使用，有多年多线程业务处理经验，封装过多线程批处理工具类等公用组建 了解 Mysql 分库分表相关原理，如 Sardingsphere、Mycat 等框架有相关使用经验 了解操作系统底层原理以及 C、C++程序开发，对计算机底层原理有初步了解 了解前端开发，了解 html，css，js，vue 等前端技术，对前端开发有一定的了解 研究过单片机等硬件开发，喜欢科技产品，喜欢软件，喜欢折腾各种电子产品以及软件 项目经验​ 项目经验只写了在北京之后参与过的相关项目，在大连做的项目偏向于传统，项目也都是单点部署，主要用的框架都是spring、mybatis、Hibernate、springMVC等相互结合使用，即：SSM，SSH，相比于springBoot来说不值一提，现在应该没有几个公司还没用springBoot了吧(#^.^#)，值得一提的是，刚毕业那会，做了半年的C语言嵌入式开发，外包到大连东软做对日的佳能相机系统，虽然目前我已经做了多年的java开发，但是那毕竟是我第一次参加开发项目，人都是有初恋情节的嘛，对自己的第一次念念不忘(我指得是工作🤭)，那半年让我对硬件底层有了一些理解，也算是最大的收获了吧。 —— 包银消费金融(现名：蒙商消费) ——​ 包头市包银消费金融股份有限公司是经中国银监会批准成立的持牌消费金融公司，由包商银行发起设立，包银消费金融为个人消费者提供消费信贷服务。其主要合作渠道有：证大财富，京东金条，微粒贷，去哪儿，京东借贷平台，分期乐，小米等多家放款渠道。目前，已累计完成 1200 多万客户注册和 320 多亿放款规模。 acs：核心交易系统主要包含信贷核算业务和虚拟账户业务，信贷核算主要负责借还款、核销减免交易、利息、罚息计提、各种费用等业务的计算和落地，日终生成交易流水文件供会计核算系统生成会计分录；虚拟账户部分主要为清结算人员提供交易产生的不同资金账户金额的变化及资金流向，为清结算人员对账提供数据支持；系统可以处理每分钟峰值 640 多笔授信申请，每小时峰值 2 万笔授信申请，贷后支持每小时 17 万客户数据。核心日终处理量达 152 万笔(借据数量)。 gls：财务核算系统对核心交易系统日终生成的交易流水解析之后按照分录借贷规则生成财务分录文件交给金蝶系统，17 年财务核算系统是买来的系统，19 年由我重新开发出属于公司自己的财务核算系统，并增加了财务对账功能(核心交易系统和财务分录对账)，不但及时发现线上交易的错误数据，及时解决问题，更提升了月终对账，年结的效率，实现了财务对账自动化，解决了年结人工对账的痛点。 ecif：渠道系统要负责对接第三方引流渠道，客户通过第三方渠道授信、借款，渠道引流到包银，由于不同渠道的授信、放款业务逻辑不同，针对不同渠道提供不同的功能开发。其中部分渠道积累一天客户授信请求指定时间统一发送授信申请到包银，系统可处理每分钟 800 笔授信请求。 css：清结算系统此系统是公司内部清结算人员使用的内部业务系统，主要功能有个人溢缴款账户管理、对公付款、对私付款、退款以及清结算同事转账业务的发起和审核功能 联合贷款系统主要功能有联合贷款协议，路由配置，路由规则，借据还款计划拆分，资方授信，借据、交易流水文件，联合贷款授信影音资料推送 监管报送系统按照监管报送需求，对借据(每月报送全量数据)，还款计划，还款流水，客户，产品，核销，借款申请，资产证券化，五级分类等信息进行加工之后推送给监管报送系统 自我评价业精于勤，荒于嬉 、行成于思，毁于随。 性格乐观开朗，话痨，热爱生活，喜欢拍视频记录生活，喜欢分享知识，分享技术，分享生活中的点点滴滴，脾气好，怕老婆 个人爱好 电子科技产品 软件 —— 喜欢win、Mac、Android 好用无广告软件研究及分享 足球 —— 喜欢但是没机会玩，怀念高中时代呀…. 三国杀 —— 基本已经被凉企逼到退游了….而且生活中也很难找到一起玩这个游戏的朋友了🤣 逛B站 —— 生活区和科技区一个不知名阿婆主😂，佛系更新视频 👉 点击打开我的B站个人空间 记得三连 👍👍👍 看电影 —— 喜欢科幻、漫威粉、追斗罗大陆….."},{"title":"","date":"2024-08-22T03:23:14.709Z","updated":"2024-08-22T03:23:14.709Z","comments":true,"path":"css/custom.css","permalink":"https://zhangxin66666.github.io/css/custom.css","excerpt":"","text":"/* 文章页H1-H6图标样式效果 */ h1::before, h2::before, h3::before, h4::before, h5::before, h6::before { -webkit-animation: ccc 1.6s linear infinite ; animation: ccc 1.6s linear infinite ; } @-webkit-keyframes ccc { 0% { -webkit-transform: rotate(0deg); transform: rotate(0deg) } to { -webkit-transform: rotate(-1turn); transform: rotate(-1turn) } } @keyframes ccc { 0% { -webkit-transform: rotate(0deg); transform: rotate(0deg) } to { -webkit-transform: rotate(-1turn); transform: rotate(-1turn) } } #content-inner.layout h1::before { color: #ef50a8 ; margin-left: -1.55rem; font-size: 1.3rem; margin-top: -0.23rem; } #content-inner.layout h2::before { color: #fb7061 ; margin-left: -1.35rem; font-size: 1.1rem; margin-top: -0.12rem; } #content-inner.layout h3::before { color: #ffbf00 ; margin-left: -1.22rem; font-size: 0.95rem; margin-top: -0.09rem; } #content-inner.layout h4::before { color: #a9e000 ; margin-left: -1.05rem; font-size: 0.8rem; margin-top: -0.09rem; } #content-inner.layout h5::before { color: #57c850 ; margin-left: -0.9rem; font-size: 0.7rem; margin-top: 0.0rem; } #content-inner.layout h6::before { color: #5ec1e0 ; margin-left: -0.9rem; font-size: 0.66rem; margin-top: 0.0rem; } #content-inner.layout h1:hover, #content-inner.layout h2:hover, #content-inner.layout h3:hover, #content-inner.layout h4:hover, #content-inner.layout h5:hover, #content-inner.layout h6:hover { color: #49b1f5 ; } #content-inner.layout h1:hover::before, #content-inner.layout h2:hover::before, #content-inner.layout h3:hover::before, #content-inner.layout h4:hover::before, #content-inner.layout h5:hover::before, #content-inner.layout h6:hover::before { color: #49b1f5 ; -webkit-animation: ccc 3.2s linear infinite ; animation: ccc 3.2s linear infinite ; } /* 页面设置icon转动速度调整 */ #rightside_config i.fas.fa-cog.fa-spin { animation: fa-spin 5s linear infinite ; } /*--------更换字体------------*/ @font-face { font-family: 'tzy'; /* 字体名自定义即可 */ src: url('https://cdn.jsdelivr.net/gh/tzy13755126023/BLOG_SOURCE/font/ZhuZiAWan.woff2'); /* 字体文件路径 */ font-display: swap; } /*body,*/ /*.gitcalendar {*/ /* font-family: tzy !important;*/ /*}*/ .categoryBar-list { max-height: 400px; } .clock-row { overflow: hidden; text-overflow: ellipsis; } /*3s为加载动画的时间，1为加载动画的次数，ease-in-out为动画效果*/ #page-header, #web_bg { -webkit-animation: imgblur 2s 1 ease-in-out; animation: imgblur 2s 1 ease-in-out; } @keyframes imgblur { 0% { filter: blur(5px); } 100% { filter: blur(0px); } } /*适配使用-webkit内核的浏览器 */ @-webkit-keyframes imgblur { 0% { -webkit-filter: blur(5px); } 100% { -webkit-filter: blur(0px); } } .table-wrap img { margin: .6rem auto .1rem !important; } /* 标签外挂 网站卡片 start */ .site-card-group img { margin: 0 auto .1rem !important; } .site-card-group .info a img { margin-right: 10px !important; } [data-theme='dark'] .site-card-group .site-card .info .title { color: #f0f0f0 !important; } [data-theme='dark'] .site-card-group .site-card .info .desc { color: rgba(255, 255, 255, .7) !important; } .site-card-group .info .desc { margin-top: 4px !important; } /* 代码块颜色 */ figure.highlight pre .addition { color: #00bf03 !important; }"},{"title":"技术笔记","date":"2022-01-11T01:48:58.204Z","updated":"2022-01-11T01:48:58.204Z","comments":true,"path":"技术笔记/index.html","permalink":"https://zhangxin66666.github.io/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/index.html","excerpt":"","text":"我是技术笔记"},{"title":"分类","date":"2022-01-11T01:48:58.103Z","updated":"2022-01-11T01:48:58.103Z","comments":true,"path":"categories/index.html","permalink":"https://zhangxin66666.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Redis内部数据结构详解(7)——intset","slug":"5.缓存/Redis内部数据结构详解(7)——intset","date":"2025-01-01T16:00:00.000Z","updated":"2025-01-02T12:15:32.121Z","comments":true,"path":"2025/01/02/5.缓存/Redis内部数据结构详解(7)——intset/","link":"","permalink":"https://zhangxin66666.github.io/2025/01/02/5.%E7%BC%93%E5%AD%98/Redis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3(7)%E2%80%94%E2%80%94intset/","excerpt":"","text":"Redis里面使用intset是为了实现集合(set)这种对外的数据结构。set结构类似于数学上的集合的概念，它包含的元素无序，且不能重复。Redis里的set结构还实现了基础的集合并、交、差的操作。与Redis对外暴露的其它数据结构类似，set的底层实现，随着元素类型是否是整型以及添加的元素的数目多少，而有所变化。概括来讲，当set中添加的元素都是整型且元素数目较少时，set使用intset作为底层数据结构，否则，set使用dict作为底层数据结构。 在本文中我们将大体分成三个部分进行介绍： 集中介绍intset数据结构。讨论set是如何在intset和dict基础上构建起来的。集中讨论set的并、交、差的算法实现以及时间复杂度。注意，其中差集的计算在Redis中实现了两种算法。我们在讨论中还会涉及到一个Redis配置（在redis.conf中的ADVANCED CONFIG部分）： 1set-max-intset-entries 512 注：本文讨论的代码实现基于Redis源码的3.2分支。 intset数据结构简介intset顾名思义，是由整数组成的集合。实际上，intset是一个由整数组成的有序集合，从而便于在上面进行二分查找，用于快速地判断一个元素是否属于这个集合。它在内存分配上与ziplist有些类似，是连续的一整块内存空间，而且对于大整数和小整数（按绝对值）采取了不同的编码，尽量对内存的使用进行了优化。 intset的数据结构定义如下（出自intset.h和intset.c）： 123456789typedef struct intset &#123;uint32_t encoding;uint32_t length;int8_t contents[];&#125; intset;#define INTSET_ENC_INT16 (sizeof(int16_t))#define INTSET_ENC_INT32 (sizeof(int32_t))#define INTSET_ENC_INT64 (sizeof(int64_t)) 各个字段含义如下： encoding: 数据编码，表示intset中的每个数据元素用几个字节来存储。它有三种可能的取值：INTSET_ENC_INT16表示每个元素用2个字节存储，INTSET_ENC_INT32表示每个元素用4个字节存储，INTSET_ENC_INT64表示每个元素用8个字节存储。因此，intset中存储的整数最多只能占用64bit。length: 表示intset中的元素个数。encoding和length两个字段构成了intset的头部（header）。contents: 是一个柔性数组（flexible array member），表示intset的header后面紧跟着数据元素。这个数组的总长度（即总字节数）等于encoding * length。柔性数组在Redis的很多数据结构的定义中都出现过（例如sds, quicklist, skiplist），用于表达一个偏移量。contents需要单独为其分配空间，这部分内存不包含在intset结构当中。其中需要注意的是，intset可能会随着数据的添加而改变它的数据编码： 最开始，新创建的intset使用占内存最小的INTSET_ENC_INT16（值为2）作为数据编码。每添加一个新元素，则根据元素大小决定是否对数据编码进行升级。下图给出了一个添加数据的具体例子（点击看大图）。 在上图中： 新创建的intset只有一个header，总共8个字节。其中encoding = 2, length = 0。添加13, 5两个元素之后，因为它们是比较小的整数，都能使用2个字节表示，所以encoding不变，值还是2。当添加32768的时候，它不再能用2个字节来表示了（2个字节能表达的数据范围是-215~215-1，而32768等于215，超出范围了），因此encoding必须升级到INTSET_ENC_INT32（值为4），即用4个字节表示一个元素。在添加每个元素的过程中，intset始终保持从小到大有序。与ziplist类似，intset也是按小端（little endian）模式存储的（参见维基百科词条Endianness）。比如，在上图中intset添加完所有数据之后，表示encoding字段的4个字节应该解释成0x00000004，而第5个数据应该解释成0x000186A0 = 100000。intset与ziplist相比： ziplist可以存储任意二进制串，而intset只能存储整数。ziplist是无序的，而intset是从小到大有序的。因此，在ziplist上查找只能遍历，而在intset上可以进行二分查找，性能更高。ziplist可以对每个数据项进行不同的变长编码（每个数据项前面都有数据长度字段len），而intset只能整体使用一个统一的编码（encoding）。intset的查找和添加操作要理解intset的一些实现细节，只需要关注intset的两个关键操作基本就可以了：查找（intsetFind）和添加（intsetAdd）元素。 intsetFind的关键代码如下所示（出自intset.c）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445uint8_t intsetFind(intset *is, int64_t value) &#123;uint8_t valenc = _intsetValueEncoding(value);return valenc &lt;= intrev32ifbe(is-&gt;encoding) &amp;&amp; intsetSearch(is,value,NULL);&#125;static uint8_t intsetSearch(intset *is, int64_t value, uint32_t *pos) &#123;int min = 0, max = intrev32ifbe(is-&gt;length)-1, mid = -1;int64_t cur = -1; /* The value can never be found when the set is empty */ if (intrev32ifbe(is-&gt;length) == 0) &#123; if (pos) *pos = 0; return 0; &#125; else &#123; /* Check for the case where we know we cannot find the value, * but do know the insert position. */ if (value &gt; _intsetGet(is,intrev32ifbe(is-&gt;length)-1)) &#123; if (pos) *pos = intrev32ifbe(is-&gt;length); return 0; &#125; else if (value &lt; _intsetGet(is,0)) &#123; if (pos) *pos = 0; return 0; &#125; &#125; while(max &gt;= min) &#123; mid = ((unsigned int)min + (unsigned int)max) &gt;&gt; 1; cur = _intsetGet(is,mid); if (value &gt; cur) &#123; min = mid+1; &#125; else if (value &lt; cur) &#123; max = mid-1; &#125; else &#123; break; &#125; &#125; if (value == cur) &#123; if (pos) *pos = mid; return 1; &#125; else &#123; if (pos) *pos = min; return 0; &#125;&#125; 关于以上代码，我们需要注意的地方包括： intsetFind在指定的intset中查找指定的元素value，找到返回1，没找到返回0。_intsetValueEncoding函数会根据要查找的value落在哪个范围而计算出相应的数据编码（即它应该用几个字节来存储）。如果value所需的数据编码比当前intset的编码要大，则它肯定在当前intset所能存储的数据范围之外（特别大或特别小），所以这时会直接返回0；否则调用intsetSearch执行一个二分查找算法。intsetSearch在指定的intset中查找指定的元素value，如果找到，则返回1并且将参数pos指向找到的元素位置；如果没找到，则返回0并且将参数pos指向能插入该元素的位置。intsetSearch是对于二分查找算法的一个实现，它大致分为三个部分：特殊处理intset为空的情况。特殊处理两个边界情况：当要查找的value比最后一个元素还要大或者比第一个元素还要小的时候。实际上，这两部分的特殊处理，在二分查找中并不是必须的，但它们在这里提供了特殊情况下快速失败的可能。真正执行二分查找过程。注意：如果最后没找到，插入位置在min指定的位置。代码中出现的intrev32ifbe是为了在需要的时候做大小端转换的。前面我们提到过，intset里的数据是按小端（little endian）模式存储的，因此在大端（big endian）机器上运行时，这里的intrev32ifbe会做相应的转换。这个查找算法的总的时间复杂度为O(log n)。而intsetAdd的关键代码如下所示（出自intset.c）： 12345678910111213141516171819202122232425262728intset *intsetAdd(intset *is, int64_t value, uint8_t *success) &#123;uint8_t valenc = _intsetValueEncoding(value);uint32_t pos;if (success) *success = 1; /* Upgrade encoding if necessary. If we need to upgrade, we know that * this value should be either appended (if &gt; 0) or prepended (if &lt; 0), * because it lies outside the range of existing values. */ if (valenc &gt; intrev32ifbe(is-&gt;encoding)) &#123; /* This always succeeds, so we don&#x27;t need to curry *success. */ return intsetUpgradeAndAdd(is,value); &#125; else &#123; /* Abort if the value is already present in the set. * This call will populate &quot;pos&quot; with the right position to insert * the value when it cannot be found. */ if (intsetSearch(is,value,&amp;pos)) &#123; if (success) *success = 0; return is; &#125; is = intsetResize(is,intrev32ifbe(is-&gt;length)+1); if (pos &lt; intrev32ifbe(is-&gt;length)) intsetMoveTail(is,pos,pos+1); &#125; _intsetSet(is,pos,value); is-&gt;length = intrev32ifbe(intrev32ifbe(is-&gt;length)+1); return is;&#125; 关于以上代码，我们需要注意的地方包括： intsetAdd在intset中添加新元素value。如果value在添加前已经存在，则不会重复添加，这时参数success被置为0；如果value在原来intset中不存在，则将value插入到适当位置，这时参数success被置为0。如果要添加的元素value所需的数据编码比当前intset的编码要大，那么则调用intsetUpgradeAndAdd将intset的编码进行升级后再插入value。调用intsetSearch，如果能查到，则不会重复添加。如果没查到，则调用intsetResize对intset进行内存扩充，使得它能够容纳新添加的元素。因为intset是一块连续空间，因此这个操作会引发内存的realloc（参见http://man.cx/realloc）。这有可能带来一次数据拷贝。同时调用intsetMoveTail将待插入位置后面的元素统一向后移动1个位置，这也涉及到一次数据拷贝。值得注意的是，在intsetMoveTail中是调用memmove完成这次数据拷贝的。memmove保证了在拷贝过程中不会造成数据重叠或覆盖，具体参见http://man.cx/memmove。intsetUpgradeAndAdd的实现中也会调用intsetResize来完成内存扩充。在进行编码升级时，intsetUpgradeAndAdd的实现会把原来intset中的每个元素取出来，再用新的编码重新写入新的位置。注意一下intsetAdd的返回值，它返回一个新的intset指针。它可能与传入的intset指针is相同，也可能不同。调用方必须用这里返回的新的intset，替换之前传进来的旧的intset变量。类似这种接口使用模式，在Redis的实现代码中是很常见的，比如我们之前在介绍sds和ziplist的时候都碰到过类似的情况。显然，这个intsetAdd算法总的时间复杂度为O(n)。Redis的set为了更好地理解Redis对外暴露的set数据结构，我们先看一下set的一些关键的命令。下面是一些命令举例： 上面这些命令的含义： sadd用于分别向集合s1和s2中添加元素。添加的元素既有数字，也有非数字（”a”和”b”）。sismember用于判断指定的元素是否在集合内存在。sinter, sunion和sdiff分别用于计算集合的交集、并集和差集。我们前面提到过，set的底层实现，随着元素类型是否是整型以及添加的元素的数目多少，而有所变化。例如，具体到上述命令的执行过程中，集合s1的底层数据结构会发生如下变化： 在开始执行完sadd s1 13 5之后，由于添加的都是比较小的整数，所以s1底层是一个intset，其数据编码encoding = 2。在执行完sadd s1 32768 10 100000之后，s1底层仍然是一个intset，但其数据编码encoding从2升级到了4。在执行完sadd s1 a b之后，由于添加的元素不再是数字，s1底层的实现会转成一个dict。我们知道，dict是一个用于维护key和value映射关系的数据结构，那么当set底层用dict表示的时候，它的key和value分别是什么呢？实际上，key就是要添加的集合元素，而value是NULL。 除了前面提到的由于添加非数字元素造成集合底层由intset转成dict之外，还有两种情况可能造成这种转换： 添加了一个数字，但它无法用64bit的有符号数来表达。intset能够表达的最大的整数范围为-264~264-1，因此，如果添加的数字超出了这个范围，这也会导致intset转成dict。添加的集合元素个数超过了set-max-intset-entries配置的值的时候，也会导致intset转成dict（具体的触发条件参见t_set.c中的setTypeAdd相关代码）。对于小集合使用intset来存储，主要的原因是节省内存。特别是当存储的元素个数较少的时候，dict所带来的内存开销要大得多（包含两个哈希表、链表指针以及大量的其它元数据）。所以，当存储大量的小集合而且集合元素都是数字的时候，用intset能节省下一笔可观的内存空间。 实际上，从时间复杂度上比较，intset的平均情况是没有dict性能高的。以查找为例，intset是O(log n)的，而dict可以认为是O(1)的。但是，由于使用intset的时候集合元素个数比较少，所以这个影响不大。 Redis set的并、交、差算法Redis set的并、交、差算法的实现代码，在t_set.c中。其中计算交集调用的是sinterGenericCommand，计算并集和差集调用的是sunionDiffGenericCommand。它们都能同时对多个（可以多于2个）集合进行运算。当对多个集合进行差集运算时，它表达的含义是：用第一个集合与第二个集合做差集，所得结果再与第三个集合做差集，依次向后类推。 我们在这里简要介绍一下三个算法的实现思路。 交集计算交集的过程大概可以分为三部分： 检查各个集合，对于不存在的集合当做空集来处理。一旦出现空集，则不用继续计算了，最终的交集就是空集。对各个集合按照元素个数由少到多进行排序。这个排序有利于后面计算的时候从最小的集合开始，需要处理的元素个数较少。对排序后第一个集合（也就是最小集合）进行遍历，对于它的每一个元素，依次在后面的所有集合中进行查找。只有在所有集合中都能找到的元素，才加入到最后的结果集合中。需要注意的是，上述第3步在集合中进行查找，对于intset和dict的存储来说时间复杂度分别是O(log n)和O(1)。但由于只有小集合才使用intset，所以可以粗略地认为intset的查找也是常数时间复杂度的。因此，如Redis官方文档上所说（http://redis.io/commands/sinter），sinter命令的时间复杂度为： O(N*M) worst case where N is the cardinality of the smallest set and M is the number of sets. 并集计算并集最简单，只需要遍历所有集合，将每一个元素都添加到最后的结果集合中。向集合中添加元素会自动去重。 由于要遍历所有集合的每个元素，所以Redis官方文档给出的sunion命令的时间复杂度为（http://redis.io/commands/sunion）： O(N) where N is the total number of elements in all given sets. 注意，这里同前面讨论交集计算一样，将元素插入到结果集合的过程，忽略intset的情况，认为时间复杂度为O(1)。 差集计算差集有两种可能的算法，它们的时间复杂度有所区别。 第一种算法： 对第一个集合进行遍历，对于它的每一个元素，依次在后面的所有集合中进行查找。只有在所有集合中都找不到的元素，才加入到最后的结果集合中。这种算法的时间复杂度为O(N*M)，其中N是第一个集合的元素个数，M是集合数目。 第二种算法： 将第一个集合的所有元素都加入到一个中间集合中。遍历后面所有的集合，对于碰到的每一个元素，从中间集合中删掉它。最后中间集合剩下的元素就构成了差集。这种算法的时间复杂度为O(N)，其中N是所有集合的元素个数总和。 在计算差集的开始部分，会先分别估算一下两种算法预期的时间复杂度，然后选择复杂度低的算法来进行运算。还有两点需要注意： 在一定程度上优先选择第一种算法，因为它涉及到的操作比较少，只用添加，而第二种算法要先添加再删除。如果选择了第一种算法，那么在执行该算法之前，Redis的实现中对于第二个集合之后的所有集合，按照元素个数由多到少进行了排序。这个排序有利于以更大的概率查找到元素，从而更快地结束查找。对于sdiff的时间复杂度，Redis官方文档（http://redis.io/commands/sdiff）只给出了第二种算法的结果，是不准确的。","categories":[{"name":"5.缓存","slug":"5-缓存","permalink":"https://zhangxin66666.github.io/categories/5-%E7%BC%93%E5%AD%98/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://zhangxin66666.github.io/tags/redis/"}]},{"title":"Redis内部数据结构详解(6)——skiplist","slug":"5.缓存/Redis内部数据结构详解(6)——skiplist","date":"2025-01-01T16:00:00.000Z","updated":"2025-01-02T12:12:02.655Z","comments":true,"path":"2025/01/02/5.缓存/Redis内部数据结构详解(6)——skiplist/","link":"","permalink":"https://zhangxin66666.github.io/2025/01/02/5.%E7%BC%93%E5%AD%98/Redis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3(6)%E2%80%94%E2%80%94skiplist/","excerpt":"","text":"Redis里面使用skiplist是为了实现sorted set这种对外的数据结构。sorted set提供的操作非常丰富，可以满足非常多的应用场景。这也意味着，sorted set相对来说实现比较复杂。同时，skiplist这种数据结构对于很多人来说都比较陌生，因为大部分学校里的算法课都没有对这种数据结构进行过详细的介绍。因此，为了介绍得足够清楚，本文会比这个系列的其它几篇花费更多的篇幅。 我们将大体分成三个部分进行介绍： 介绍经典的skiplist数据结构，并进行简单的算法分析。这一部分的介绍，与Redis没有直接关系。我会尝试尽量使用通俗易懂的语言进行描述。讨论Redis里的skiplist的具体实现。为了支持sorted set本身的一些要求，在经典的skiplist基础上，Redis里的相应实现做了若干改动。讨论sorted set是如何在skiplist, dict和ziplist基础上构建起来的。我们在讨论中还会涉及到两个Redis配置（在redis.conf中的ADVANCED CONFIG部分）： 12zset-max-ziplist-entries 128zset-max-ziplist-value 64 我们在讨论中会详细解释这两个配置的含义。 注：本文讨论的代码实现基于Redis源码的3.2分支。 skiplist数据结构简介skiplist本质上也是一种查找结构，用于解决算法中的查找问题（Searching），即根据给定的key，快速查到它所在的位置（或者对应的value）。 我们在《Redis内部数据结构详解》系列的第一篇中介绍dict的时候，曾经讨论过：一般查找问题的解法分为两个大类：一个是基于各种平衡树，一个是基于哈希表。但skiplist却比较特殊，它没法归属到这两大类里面。 这种数据结构是由William Pugh发明的，最早出现于他在1990年发表的论文《Skip Lists: A Probabilistic Alternative to Balanced Trees》。对细节感兴趣的同学可以下载论文原文来阅读。 skiplist，顾名思义，首先它是一个list。实际上，它是在有序链表的基础上发展起来的。 我们先来看一个有序链表，如下图（最左侧的灰色节点表示一个空的头结点）： 在这样一个链表中，如果我们要查找某个数据，那么需要从头开始逐个进行比较，直到找到包含数据的那个节点，或者找到第一个比给定数据大的节点为止（没找到）。也就是说，时间复杂度为O(n)。同样，当我们要插入新数据的时候，也要经历同样的查找过程，从而确定插入位置。 假如我们每相邻两个节点增加一个指针，让指针指向下下个节点，如下图： 这样所有新增加的指针连成了一个新的链表，但它包含的节点个数只有原来的一半（上图中是7, 19, 26）。现在当我们想查找数据的时候，可以先沿着这个新链表进行查找。当碰到比待查数据大的节点时，再回到原来的链表中进行查找。比如，我们想查找23，查找的路径是沿着下图中标红的指针所指向的方向进行的： 23首先和7比较，再和19比较，比它们都大，继续向后比较。但23和26比较的时候，比26要小，因此回到下面的链表（原链表），与22比较。23比22要大，沿下面的指针继续向后和26比较。23比26小，说明待查数据23在原链表中不存在，而且它的插入位置应该在22和26之间。在这个查找过程中，由于新增加的指针，我们不再需要与链表中每个节点逐个进行比较了。需要比较的节点数大概只有原来的一半。 利用同样的方式，我们可以在上层新产生的链表上，继续为每相邻的两个节点增加一个指针，从而产生第三层链表。如下图： 在这个新的三层链表结构上，如果我们还是查找23，那么沿着最上层链表首先要比较的是19，发现23比19大，接下来我们就知道只需要到19的后面去继续查找，从而一下子跳过了19前面的所有节点。可以想象，当链表足够长的时候，这种多层链表的查找方式能让我们跳过很多下层节点，大大加快查找的速度。 skiplist正是受这种多层链表的想法的启发而设计出来的。实际上，按照上面生成链表的方式，上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到O(log n)。但是，这种方法在插入数据的时候有很大的问题。新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格的2:1的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点（也包括新插入的节点）重新进行调整，这会让时间复杂度重新蜕化成O(n)。删除数据也有同样的问题。 skiplist为了避免这一问题，它不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是为每个节点随机出一个层数(level)。比如，一个节点随机出的层数是3，那么就把它链入到第1层到第3层这三层链表中。为了表达清楚，下图展示了如何通过一步步的插入操作从而形成一个skiplist的过程： 从上面skiplist的创建和插入过程可以看出，每一个节点的层数（level）是随机出来的，而且新插入一个节点不会影响其它节点的层数。因此，插入操作只需要修改插入节点前后的指针，而不需要对很多节点都进行调整。这就降低了插入操作的复杂度。实际上，这是skiplist的一个很重要的特性，这让它在插入性能上明显优于平衡树的方案。这在后面我们还会提到。 根据上图中的skiplist结构，我们很容易理解这种数据结构的名字的由来。skiplist，翻译成中文，可以翻译成“跳表”或“跳跃表”，指的就是除了最下面第1层链表之外，它会产生若干层稀疏的链表，这些链表里面的指针故意跳过了一些节点（而且越高层的链表跳过的节点越多）。这就使得我们在查找数据的时候能够先在高层的链表中进行查找，然后逐层降低，最终降到第1层链表来精确地确定数据位置。在这个过程中，我们跳过了一些节点，从而也就加快了查找速度。 刚刚创建的这个skiplist总共包含4层链表，现在假设我们在它里面依然查找23，下图给出了查找路径： 需要注意的是，前面演示的各个节点的插入过程，实际上在插入之前也要先经历一个类似的查找过程，在确定插入位置后，再完成插入操作。 至此，skiplist的查找和插入操作，我们已经很清楚了。而删除操作与插入操作类似，我们也很容易想象出来。这些操作我们也应该能很容易地用代码实现出来。 当然，实际应用中的skiplist每个节点应该包含key和value两部分。前面的描述中我们没有具体区分key和value，但实际上列表中是按照key进行排序的，查找过程也是根据key在比较。 但是，如果你是第一次接触skiplist，那么一定会产生一个疑问：节点插入时随机出一个层数，仅仅依靠这样一个简单的随机数操作而构建出来的多层链表结构，能保证它有一个良好的查找性能吗？为了回答这个疑问，我们需要分析skiplist的统计性能。 在分析之前，我们还需要着重指出的是，执行插入操作时计算随机数的过程，是一个很关键的过程，它对skiplist的统计特性有着很重要的影响。这并不是一个普通的服从均匀分布的随机数，它的计算过程如下： 首先，每个节点肯定都有第1层指针（每个节点都在第1层链表里）。如果一个节点有第i层(i&gt;=1)指针（即节点已经在第1层到第i层链表中），那么它有第(i+1)层指针的概率为p。节点最大的层数不允许超过一个最大值，记为MaxLevel。这个计算随机层数的伪码如下所示： 123456randomLevel()level := 1// random()返回一个[0...1)的随机数while random() &lt; p and level &lt; MaxLevel dolevel := level + 1return level randomLevel()的伪码中包含两个参数，一个是p，一个是MaxLevel。在Redis的skiplist实现中，这两个参数的取值为： 12p = 1/4MaxLevel = 32 skiplist的算法性能分析在这一部分，我们来简单分析一下skiplist的时间复杂度和空间复杂度，以便对于skiplist的性能有一个直观的了解。如果你不是特别偏执于算法的性能分析，那么可以暂时跳过这一小节的内容。 我们先来计算一下每个节点所包含的平均指针数目（概率期望）。节点包含的指针数目，相当于这个算法在空间上的额外开销(overhead)，可以用来度量空间复杂度。 根据前面randomLevel()的伪码，我们很容易看出，产生越高的节点层数，概率越低。定量的分析如下： 节点层数至少为1。而大于1的节点层数，满足一个概率分布。节点层数恰好等于1的概率为1-p。节点层数大于等于2的概率为p，而节点层数恰好等于2的概率为p(1-p)。节点层数大于等于3的概率为p2，而节点层数恰好等于3的概率为p2(1-p)。节点层数大于等于4的概率为p3，而节点层数恰好等于4的概率为p3(1-p)。……因此，一个节点的平均层数（也即包含的平均指针数目），计算如下： skiplist平均层数计算 现在很容易计算出： 当p=1/2时，每个节点所包含的平均指针数目为2；当p=1/4时，每个节点所包含的平均指针数目为1.33。这也是Redis里的skiplist实现在空间上的开销。接下来，为了分析时间复杂度，我们计算一下skiplist的平均查找长度。查找长度指的是查找路径上跨越的跳数，而查找过程中的比较次数就等于查找长度加1。以前面图中标出的查找23的查找路径为例，从左上角的头结点开始，一直到结点22，查找长度为6。 为了计算查找长度，这里我们需要利用一点小技巧。我们注意到，每个节点插入的时候，它的层数是由随机函数randomLevel()计算出来的，而且随机的计算不依赖于其它节点，每次插入过程都是完全独立的。所以，从统计上来说，一个skiplist结构的形成与节点的插入顺序无关。 这样的话，为了计算查找长度，我们可以将查找过程倒过来看，从右下方第1层上最后到达的那个节点开始，沿着查找路径向左向上回溯，类似于爬楼梯的过程。我们假设当回溯到某个节点的时候，它才被插入，这虽然相当于改变了节点的插入顺序，但从统计上不影响整个skiplist的形成结构。 现在假设我们从一个层数为i的节点x出发，需要向左向上攀爬k层。这时我们有两种可能： 如果节点x有第(i+1)层指针，那么我们需要向上走。这种情况概率为p。如果节点x没有第(i+1)层指针，那么我们需要向左走。这种情况概率为(1-p)。这两种情形如下图所示： 用C(k)表示向上攀爬k个层级所需要走过的平均查找路径长度（概率期望），那么： 12C(0)=0C(k)=(1-p)×(上图中情况b的查找长度) + p×(上图中情况c的查找长度) 代入，得到一个差分方程并化简： 123C(k)=(1-p)(C(k)+1) + p(C(k-1)+1)C(k)=1/p+C(k-1)C(k)=k/p 这个结果的意思是，我们每爬升1个层级，需要在查找路径上走1/p步。而我们总共需要攀爬的层级数等于整个skiplist的总层数-1。 那么接下来我们需要分析一下当skiplist中有n个节点的时候，它的总层数的概率均值是多少。这个问题直观上比较好理解。根据节点的层数随机算法，容易得出： 第1层链表固定有n个节点；第2层链表平均有np个节点；第3层链表平均有np2个节点；…所以，从第1层到最高层，各层链表的平均节点数是一个指数递减的等比数列。容易推算出，总层数的均值为log1/pn，而最高层的平均节点数为1/p。 综上，粗略来计算的话，平均查找长度约等于： C(log1/pn-1)=(log1/pn-1)/p即，平均时间复杂度为O(log n)。 当然，这里的时间复杂度分析还是比较粗略的。比如，沿着查找路径向左向上回溯的时候，可能先到达左侧头结点，然后沿头结点一路向上；还可能先到达最高层的节点，然后沿着最高层链表一路向左。但这些细节不影响平均时间复杂度的最后结果。另外，这里给出的时间复杂度只是一个概率平均值，但实际上计算一个精细的概率分布也是有可能的。详情还请参见William Pugh的论文《Skip Lists: A Probabilistic Alternative to Balanced Trees》。 skiplist与平衡树、哈希表的比较skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，在哈希表上只能做单个key的查找，不适宜做范围查找。所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。在做范围查找的时候，平衡树比skiplist操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。查找单个key，skiplist和平衡树的时间复杂度都为O(log n)，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种Map或dictionary结构，大都是基于哈希表实现的。从算法实现难度上来比较，skiplist比平衡树要简单得多。Redis中的skiplist实现在这一部分，我们讨论Redis中的skiplist实现。 在Redis中，skiplist被用于实现暴露给外部的一个数据结构：sorted set。准确地说，sorted set底层不仅仅使用了skiplist，还使用了ziplist和dict。这几个数据结构的关系，我们下一章再讨论。现在，我们先花点时间把sorted set的关键命令看一下。这些命令对于Redis里skiplist的实现，有重要的影响。 sorted set的命令举例sorted set是一个有序的数据集合，对于像类似排行榜这样的应用场景特别适合。 现在我们来看一个例子，用sorted set来存储代数课（algebra）的成绩表。原始数据如下： Alice 87.5Bob 89.0Charles 65.5David 78.0Emily 93.5Fred 87.5这份数据给出了每位同学的名字和分数。下面我们将这份数据存储到sorted set里面去： sorted set命令举例 对于上面的这些命令，我们需要的注意的地方包括： 前面的6个zadd命令，将6位同学的名字和分数(score)都输入到一个key值为algebra的sorted set里面了。注意Alice和Fred的分数相同，都是87.5分。zrevrank命令查询Alice的排名（命令中的rev表示按照倒序排列，也就是从大到小），返回3。排在Alice前面的分别是Emily、Bob、Fred，而排名(rank)从0开始计数，所以Alice的排名是3。注意，其实Alice和Fred的分数相同，这种情况下sorted set会把分数相同的元素，按照字典顺序来排列。按照倒序，Fred排在了Alice的前面。zscore命令查询了Charles对应的分数。zrevrange命令查询了从大到小排名为0~3的4位同学。zrevrangebyscore命令查询了分数在80.0和90.0之间的所有同学，并按分数从大到小排列。总结一下，sorted set中的每个元素主要表现出3个属性： 数据本身（在前面的例子中我们把名字存成了数据）。每个数据对应一个分数(score)。根据分数大小和数据本身的字典排序，每个数据会产生一个排名(rank)。可以按正序或倒序。Redis中skiplist实现的特殊性我们简单分析一下前面出现的几个查询命令： zrevrank由数据查询它对应的排名，这在前面介绍的skiplist中并不支持。zscore由数据查询它对应的分数，这也不是skiplist所支持的。zrevrange根据一个排名范围，查询排名在这个范围内的数据。这在前面介绍的skiplist中也不支持。zrevrangebyscore根据分数区间查询数据集合，是一个skiplist所支持的典型的范围查找（score相当于key）。实际上，Redis中sorted set的实现是这样的： 当数据较少时，sorted set是由一个ziplist来实现的。当数据多的时候，sorted set是由一个dict + 一个skiplist来实现的。简单来讲，dict用来查询数据到分数的对应关系，而skiplist用来根据分数查询数据（可能是范围查找）。这里sorted set的构成我们在下一章还会再详细地讨论。现在我们集中精力来看一下sorted set与skiplist的关系，： zscore的查询，不是由skiplist来提供的，而是由那个dict来提供的。为了支持排名(rank)，Redis里对skiplist做了扩展，使得根据排名能够快速查到数据，或者根据分数查到数据之后，也同时很容易获得排名。而且，根据排名的查找，时间复杂度也为O(log n)。zrevrange的查询，是根据排名查数据，由扩展后的skiplist来提供。zrevrank是先在dict中由数据查到分数，再拿分数到skiplist中去查找，查到后也同时获得了排名。前述的查询过程，也暗示了各个操作的时间复杂度： zscore只用查询一个dict，所以时间复杂度为O(1)zrevrank, zrevrange, zrevrangebyscore由于要查询skiplist，所以zrevrank的时间复杂度为O(log n)，而zrevrange, zrevrangebyscore的时间复杂度为O(log(n)+M)，其中M是当前查询返回的元素个数。总结起来，Redis中的skiplist跟前面介绍的经典的skiplist相比，有如下不同： 分数(score)允许重复，即skiplist的key允许重复。这在最开始介绍的经典skiplist中是不允许的。在比较时，不仅比较分数（相当于skiplist的key），还比较数据本身。在Redis的skiplist实现中，数据本身的内容唯一标识这份数据，而不是由key来唯一标识。另外，当多个元素分数相同的时候，还需要根据数据内容来进字典排序。第1层链表不是一个单向链表，而是一个双向链表。这是为了方便以倒序方式获取一个范围内的元素。在skiplist中可以很方便地计算出每个元素的排名(rank)。skiplist的数据结构定义 123456789101112131415161718#define ZSKIPLIST_MAXLEVEL 32#define ZSKIPLIST_P 0.25typedef struct zskiplistNode &#123;robj *obj;double score;struct zskiplistNode *backward;struct zskiplistLevel &#123;struct zskiplistNode *forward;unsigned int span;&#125; level[];&#125; zskiplistNode;typedef struct zskiplist &#123;struct zskiplistNode *header, *tail;unsigned long length;int level;&#125; zskiplist; 这段代码出自server.h，我们来简要分析一下： 开头定义了两个常量，ZSKIPLIST_MAXLEVEL和ZSKIPLIST_P，分别对应我们前面讲到的skiplist的两个参数：一个是MaxLevel，一个是p。zskiplistNode定义了skiplist的节点结构。obj字段存放的是节点数据，它的类型是一个string robj。本来一个string robj可能存放的不是sds，而是long型，但zadd命令在将数据插入到skiplist里面之前先进行了解码，所以这里的obj字段里存储的一定是一个sds。有关robj的详情可以参见系列文章的第三篇：《Redis内部数据结构详解(3)——robj》。这样做的目的应该是为了方便在查找的时候对数据进行字典序的比较，而且，skiplist里的数据部分是数字的可能性也比较小。score字段是数据对应的分数。backward字段是指向链表前一个节点的指针（前向指针）。节点只有1个前向指针，所以只有第1层链表是一个双向链表。level[]存放指向各层链表后一个节点的指针（后向指针）。每层对应1个后向指针，用forward字段表示。另外，每个后向指针还对应了一个span值，它表示当前的指针跨越了多少个节点。span用于计算元素排名(rank)，这正是前面我们提到的Redis对于skiplist所做的一个扩展。需要注意的是，level[]是一个柔性数组（flexible array member），因此它占用的内存不在zskiplistNode结构里面，而需要插入节点的时候单独为它分配。也正因为如此，skiplist的每个节点所包含的指针数目才是不固定的，我们前面分析过的结论——skiplist每个节点包含的指针数目平均为1/(1-p)——才能有意义。zskiplist定义了真正的skiplist结构，它包含：头指针header和尾指针tail。链表长度length，即链表包含的节点总数。注意，新创建的skiplist包含一个空的头指针，这个头指针不包含在length计数中。level表示skiplist的总层数，即所有节点层数的最大值。下图以前面插入的代数课成绩表为例，展示了Redis中一个skiplist的可能结构： 注意：图中前向指针上面括号中的数字，表示对应的span的值。即当前指针跨越了多少个节点，这个计数不包括指针的起点节点，但包括指针的终点节点。 假设我们在这个skiplist中查找score=89.0的元素（即Bob的成绩数据），在查找路径中，我们会跨域图中标红的指针，这些指针上面的span值累加起来，就得到了Bob的排名(2+2+1)-1=4（减1是因为rank值以0起始）。需要注意这里算的是从小到大的排名，而如果要算从大到小的排名，只需要用skiplist长度减去查找路径上的span累加值，即6-(2+2+1)=1。 可见，在查找skiplist的过程中，通过累加span值的方式，我们就能很容易算出排名。相反，如果指定排名来查找数据（类似zrange和zrevrange那样），也可以不断累加span并时刻保持累加值不超过指定的排名，通过这种方式就能得到一条O(log n)的查找路径。 Redis中的sorted set我们前面提到过，Redis中的sorted set，是在skiplist, dict和ziplist基础上构建起来的: 当数据较少时，sorted set是由一个ziplist来实现的。当数据多的时候，sorted set是由一个叫zset的数据结构来实现的，这个zset包含一个dict + 一个skiplist。dict用来查询数据到分数(score)的对应关系，而skiplist用来根据分数查询数据（可能是范围查找）。在这里我们先来讨论一下前一种情况——基于ziplist实现的sorted set。在本系列前面关于ziplist的文章里，我们介绍过，ziplist就是由很多数据项组成的一大块连续内存。由于sorted set的每一项元素都由数据和score组成，因此，当使用zadd命令插入一个(数据, score)对的时候，底层在相应的ziplist上就插入两个数据项：数据在前，score在后。 ziplist的主要优点是节省内存，但它上面的查找操作只能按顺序查找（可以正序也可以倒序）。因此，sorted set的各个查询操作，就是在ziplist上从前向后（或从后向前）一步步查找，每一步前进两个数据项，跨域一个(数据, score)对。 随着数据的插入，sorted set底层的这个ziplist就可能会转成zset的实现（转换过程详见t_zset.c的zsetConvert）。那么到底插入多少才会转呢？ 还记得本文开头提到的两个Redis配置吗？ 12zset-max-ziplist-entries 128zset-max-ziplist-value 64 这个配置的意思是说，在如下两个条件之一满足的时候，ziplist会转成zset（具体的触发条件参见t_zset.c中的zaddGenericCommand相关代码）： 当sorted set中的元素个数，即(数据, score)对的数目超过128的时候，也就是ziplist数据项超过256的时候。当sorted set中插入的任意一个数据的长度超过了64的时候。最后，zset结构的代码定义如下： 1234typedef struct zset &#123;dict *dict;zskiplist *zsl;&#125; zset; Redis为什么用skiplist而不用平衡树？在前面我们对于skiplist和平衡树、哈希表的比较中，其实已经不难看出Redis里使用skiplist而不用平衡树的原因了。现在我们看看，对于这个问题，Redis的作者 @antirez 是怎么说的： 1234567There are a few reasons:1) They are not very memory intensive. It’s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees.2) A sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees.3) They are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code. 这段话原文出处： https://news.ycombinator.com/item?id=1171423 这里从内存占用、对范围查找的支持和实现难易程度这三方面总结的原因，我们在前面其实也都涉及到了。","categories":[{"name":"5.缓存","slug":"5-缓存","permalink":"https://zhangxin66666.github.io/categories/5-%E7%BC%93%E5%AD%98/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://zhangxin66666.github.io/tags/redis/"}]},{"title":"Redis内部数据结构详解(3)——robj","slug":"5.缓存/Redis内部数据结构详解(3)——robj","date":"2024-12-29T16:00:00.000Z","updated":"2024-12-30T06:13:49.761Z","comments":true,"path":"2024/12/30/5.缓存/Redis内部数据结构详解(3)——robj/","link":"","permalink":"https://zhangxin66666.github.io/2024/12/30/5.%E7%BC%93%E5%AD%98/Redis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3(3)%E2%80%94%E2%80%94robj/","excerpt":"","text":"从Redis的使用者的角度来看，一个Redis节点包含多个database（非cluster模式下默认是16个，cluster模式下只能是1个），而一个database维护了从key space到object space的映射关系。这个映射关系的key是string类型，而value可以是多种数据类型，比如：string, list, hash等。我们可以看到，key的类型固定是string，而value可能的类型是多个。 而从Redis内部实现的角度来看，在前面第一篇文章中，我们已经提到过，一个database内的这个映射关系是用一个dict来维护的。dict的key固定用一种数据结构来表达就够了，这就是动态字符串sds。而value则比较复杂，为了在同一个dict内能够存储不同类型的value，这就需要一个通用的数据结构，这个通用的数据结构就是robj（全名是redisObject）。举个例子：如果value是一个list，那么它的内部存储结构是一个quicklist（quicklist的具体实现我们放在后面的文章讨论）；如果value是一个string，那么它的内部存储结构一般情况下是一个sds。当然实际情况更复杂一点，比如一个string类型的value，如果它的值是一个数字，那么Redis内部还会把它转成long型来存储，从而减小内存使用。而一个robj既能表示一个sds，也能表示一个quicklist，甚至还能表示一个long型。 robj的数据结构定义在server.h中我们找到跟robj定义相关的代码，如下（注意，本系列文章中的代码片段全部来源于Redis源码的3.2分支）： 1234567891011121314151617181920212223242526272829/* Object types */#define OBJ_STRING 0#define OBJ_LIST 1#define OBJ_SET 2#define OBJ_ZSET 3#define OBJ_HASH 4/* Objects encoding. Some kind of objects like Strings and Hashes can be* internally represented in multiple ways. The &#x27;encoding&#x27; field of the object* is set to one of this fields for this object. */ #define OBJ_ENCODING_RAW 0 /* Raw representation */ #define OBJ_ENCODING_INT 1 /* Encoded as integer */ #define OBJ_ENCODING_HT 2 /* Encoded as hash table */ #define OBJ_ENCODING_ZIPMAP 3 /* Encoded as zipmap */ #define OBJ_ENCODING_LINKEDLIST 4 /* Encoded as regular linked list */ #define OBJ_ENCODING_ZIPLIST 5 /* Encoded as ziplist */ #define OBJ_ENCODING_INTSET 6 /* Encoded as intset */ #define OBJ_ENCODING_SKIPLIST 7 /* Encoded as skiplist */ #define OBJ_ENCODING_EMBSTR 8 /* Embedded sds string encoding */ #define OBJ_ENCODING_QUICKLIST 9 /* Encoded as linked list of ziplists */#define LRU_BITS 24typedef struct redisObject &#123;unsigned type:4;unsigned encoding:4;unsigned lru:LRU_BITS; /* lru time (relative to server.lruclock) */int refcount;void *ptr;&#125; robj; 一个robj包含如下5个字段： type: 对象的数据类型。占4个bit。可能的取值有5种：OBJ_STRING, OBJ_LIST, OBJ_SET, OBJ_ZSET, OBJ_HASH，分别对应Redis对外暴露的5种数据结构（即我们在第一篇文章中提到的第一个层面的5种数据结构）。encoding: 对象的内部表示方式（也可以称为编码）。占4个bit。可能的取值有10种，即前面代码中的10个OBJ_ENCODING_XXX常量。lru: 做LRU替换算法用，占24个bit。这个不是我们这里讨论的重点，暂时忽略。refcount: 引用计数。它允许robj对象在某些情况下被共享。ptr: 数据指针。指向真正的数据。比如，一个代表string的robj，它的ptr可能指向一个sds结构；一个代表list的robj，它的ptr可能指向一个quicklist。这里特别需要仔细察看的是encoding字段。对于同一个type，还可能对应不同的encoding，这说明同样的一个数据类型，可能存在不同的内部表示方式。而不同的内部表示，在内存占用和查找性能上会有所不同。 比如，当type = OBJ_STRING的时候，表示这个robj存储的是一个string，这时encoding可以是下面3种中的一种： OBJ_ENCODING_RAW: string采用原生的表示方式，即用sds来表示。OBJ_ENCODING_INT: string采用数字的表示方式，实际上是一个long型。OBJ_ENCODING_EMBSTR: string采用一种特殊的嵌入式的sds来表示。接下来我们会讨论到这个细节。再举一个例子：当type = OBJ_HASH的时候，表示这个robj存储的是一个hash，这时encoding可以是下面2种中的一种： OBJ_ENCODING_HT: hash采用一个dict来表示。OBJ_ENCODING_ZIPLIST: hash采用一个ziplist来表示（ziplist的具体实现我们放在后面的文章讨论）。本文剩余主要部分将针对表示string的robj对象，围绕它的3种不同的encoding来深入讨论。前面代码段中出现的所有10种encoding，在这里我们先简单解释一下，在这个系列后面的文章中，我们应该还有机会碰到它们。 OBJ_ENCODING_RAW: 最原生的表示方式。其实只有string类型才会用这个encoding值（表示成sds）。OBJ_ENCODING_INT: 表示成数字。实际用long表示。OBJ_ENCODING_HT: 表示成dict。OBJ_ENCODING_ZIPMAP: 是个旧的表示方式，已不再用。在小于Redis 2.6的版本中才有。OBJ_ENCODING_LINKEDLIST: 也是个旧的表示方式，已不再用。OBJ_ENCODING_ZIPLIST: 表示成ziplist。OBJ_ENCODING_INTSET: 表示成intset。用于set数据结构。OBJ_ENCODING_SKIPLIST: 表示成skiplist。用于sorted set数据结构。OBJ_ENCODING_EMBSTR: 表示成一种特殊的嵌入式的sds。OBJ_ENCODING_QUICKLIST: 表示成quicklist。用于list数据结构。我们来总结一下robj的作用： 为多种数据类型提供一种统一的表示方式。允许同一类型的数据采用不同的内部表示，从而在某些情况下尽量节省内存。支持对象共享和引用计数。当对象被共享的时候，只占用一份内存拷贝，进一步节省内存。string robj的编码过程当我们执行Redis的set命令的时候，Redis首先将接收到的value值（string类型）表示成一个type = OBJ_STRING并且encoding = OBJ_ENCODING_RAW的robj对象，然后在存入内部存储之前先执行一个编码过程，试图将它表示成另一种更节省内存的encoding方式。这一过程的核心代码，是object.c中的tryObjectEncoding函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778robj *tryObjectEncoding(robj *o) &#123;long value;sds s = o-&gt;ptr;size_t len; /* Make sure this is a string object, the only type we encode * in this function. Other types use encoded memory efficient * representations but are handled by the commands implementing * the type. */ serverAssertWithInfo(NULL,o,o-&gt;type == OBJ_STRING); /* We try some specialized encoding only for objects that are * RAW or EMBSTR encoded, in other words objects that are still * in represented by an actually array of chars. */ if (!sdsEncodedObject(o)) return o; /* It&#x27;s not safe to encode shared objects: shared objects can be shared * everywhere in the &quot;object space&quot; of Redis and may end in places where * they are not handled. We handle them only as values in the keyspace. */ if (o-&gt;refcount &gt; 1) return o; /* Check if we can represent this string as a long integer. * Note that we are sure that a string larger than 21 chars is not * representable as a 32 nor 64 bit integer. */ len = sdslen(s); if (len &lt;= 21 &amp;&amp; string2l(s,len,&amp;value)) &#123; /* This object is encodable as a long. Try to use a shared object. * Note that we avoid using shared integers when maxmemory is used * because every object needs to have a private LRU field for the LRU * algorithm to work well. */ if ((server.maxmemory == 0 || (server.maxmemory_policy != MAXMEMORY_VOLATILE_LRU &amp;&amp; server.maxmemory_policy != MAXMEMORY_ALLKEYS_LRU)) &amp;&amp; value &gt;= 0 &amp;&amp; value &lt; OBJ_SHARED_INTEGERS) &#123; decrRefCount(o); incrRefCount(shared.integers[value]); return shared.integers[value]; &#125; else &#123; if (o-&gt;encoding == OBJ_ENCODING_RAW) sdsfree(o-&gt;ptr); o-&gt;encoding = OBJ_ENCODING_INT; o-&gt;ptr = (void*) value; return o; &#125; &#125; /* If the string is small and is still RAW encoded, * try the EMBSTR encoding which is more efficient. * In this representation the object and the SDS string are allocated * in the same chunk of memory to save space and cache misses. */ if (len &lt;= OBJ_ENCODING_EMBSTR_SIZE_LIMIT) &#123; robj *emb; if (o-&gt;encoding == OBJ_ENCODING_EMBSTR) return o; emb = createEmbeddedStringObject(s,sdslen(s)); decrRefCount(o); return emb; &#125; /* We can&#x27;t encode the object... * * Do the last try, and at least optimize the SDS string inside * the string object to require little space, in case there * is more than 10% of free space at the end of the SDS string. * * We do that only for relatively large strings as this branch * is only entered if the length of the string is greater than * OBJ_ENCODING_EMBSTR_SIZE_LIMIT. */ if (o-&gt;encoding == OBJ_ENCODING_RAW &amp;&amp; sdsavail(s) &gt; len/10) &#123; o-&gt;ptr = sdsRemoveFreeSpace(o-&gt;ptr); &#125; /* Return the original object. */ return o;&#125; 这段代码执行的操作比较复杂，我们有必要仔细看一下每一步的操作： 第1步检查，检查type。确保只对string类型的对象进行操作。第2步检查，检查encoding。sdsEncodedObject是定义在server.h中的一个宏，确保只对OBJ_ENCODING_RAW和OBJ_ENCODING_EMBSTR编码的string对象进行操作。这两种编码的string都采用sds来存储，可以尝试进一步编码处理。#define sdsEncodedObject(objptr) (objptr-&gt;encoding == OBJ_ENCODING_RAW || objptr-&gt;encoding == OBJ_ENCODING_EMBSTR)第3步检查，检查refcount。引用计数大于1的共享对象，在多处被引用。由于编码过程结束后robj的对象指针可能会变化（我们在前一篇介绍sdscatlen函数的时候提到过类似这种接口使用模式），这样对于引用计数大于1的对象，就需要更新所有地方的引用，这不容易做到。因此，对于计数大于1的对象不做编码处理。试图将字符串转成64位的long。64位的long所能表达的数据范围是-2^63到2^63-1，用十进制表达出来最长是20位数（包括负号）。这里判断小于等于21，似乎是写多了，实际判断小于等于20就够了（如果我算错了请一定告诉我哦）。string2l如果将字符串转成long转成功了，那么会返回1并且将转好的long存到value变量里。在转成long成功时，又分为两种情况。第一种情况：如果Redis的配置不要求运行LRU替换算法，且转成的long型数字的值又比较小（小于OBJ_SHARED_INTEGERS，在目前的实现中这个值是10000），那么会使用共享数字对象来表示。之所以这里的判断跟LRU有关，是因为LRU算法要求每个robj有不同的lru字段值，所以用了LRU就不能共享robj。shared.integers是一个长度为10000的数组，里面预存了10000个小的数字对象。这些小数字对象都是encoding = OBJ_ENCODING_INT的string robj对象。第二种情况：如果前一步不能使用共享小对象来表示，那么将原来的robj编码成encoding = OBJ_ENCODING_INT，这时ptr字段直接存成这个long型的值。注意ptr字段本来是一个void *指针（即存储的是内存地址），因此在64位机器上有64位宽度，正好能存储一个64位的long型值。这样，除了robj本身之外，它就不再需要额外的内存空间来存储字符串值。接下来是对于那些不能转成64位long的字符串进行处理。最后再做两步处理：如果字符串长度足够小（小于等于OBJ_ENCODING_EMBSTR_SIZE_LIMIT，定义为44），那么调用createEmbeddedStringObject编码成encoding = OBJ_ENCODING_EMBSTR；如果前面所有的编码尝试都没有成功（仍然是OBJ_ENCODING_RAW），且sds里空余字节过多，那么做最后一次努力，调用sds的sdsRemoveFreeSpace接口来释放空余字节。其中调用的createEmbeddedStringObject，我们有必要看一下它的代码： 123456789101112131415161718192021robj *createEmbeddedStringObject(const char *ptr, size_t len) &#123;robj *o = zmalloc(sizeof(robj)+sizeof(struct sdshdr8)+len+1);struct sdshdr8 *sh = (void*)(o+1); o-&gt;type = OBJ_STRING; o-&gt;encoding = OBJ_ENCODING_EMBSTR; o-&gt;ptr = sh+1; o-&gt;refcount = 1; o-&gt;lru = LRU_CLOCK(); sh-&gt;len = len; sh-&gt;alloc = len; sh-&gt;flags = SDS_TYPE_8; if (ptr) &#123; memcpy(sh-&gt;buf,ptr,len); sh-&gt;buf[len] = &#x27;\\0&#x27;; &#125; else &#123; memset(sh-&gt;buf,0,len+1); &#125; return o;&#125; createEmbeddedStringObject对sds重新分配内存，将robj和sds放在一个连续的内存块中分配，这样对于短字符串的存储有利于减少内存碎片。这个连续的内存块包含如下几部分： 16个字节的robj结构。3个字节的sdshdr8头。最多44个字节的sds字符数组。1个NULL结束符。加起来一共不超过64字节（16+3+44+1），因此这样的一个短字符串可以完全分配在一个64字节长度的内存块中。 string robj的解码过程当我们需要获取字符串的值，比如执行get命令的时候，我们需要执行与前面讲的编码过程相反的操作——解码。 这一解码过程的核心代码，是object.c中的getDecodedObject函数。 1234567891011121314151617robj *getDecodedObject(robj *o) &#123;robj *dec; if (sdsEncodedObject(o)) &#123; incrRefCount(o); return o; &#125; if (o-&gt;type == OBJ_STRING &amp;&amp; o-&gt;encoding == OBJ_ENCODING_INT) &#123; char buf[32]; ll2string(buf,32,(long)o-&gt;ptr); dec = createStringObject(buf,strlen(buf)); return dec; &#125; else &#123; serverPanic(&quot;Unknown encoding type&quot;); &#125;&#125; 这个过程比较简单，需要我们注意的点有： 编码为OBJ_ENCODING_RAW和OBJ_ENCODING_EMBSTR的字符串robj对象，不做变化，原封不动返回。站在使用者的角度，这两种编码没有什么区别，内部都是封装的sds。编码为数字的字符串robj对象，将long重新转为十进制字符串的形式，然后调用createStringObject转为sds的表示。注意：这里由long转成的sds字符串长度肯定不超过20，而根据createStringObject的实现，它们肯定会被编码成OBJ_ENCODING_EMBSTR的对象。createStringObject的代码如下： 123456robj *createStringObject(const char *ptr, size_t len) &#123;if (len &lt;= OBJ_ENCODING_EMBSTR_SIZE_LIMIT)return createEmbeddedStringObject(ptr,len);elsereturn createRawStringObject(ptr,len);&#125; 再谈sds与string的关系在上一篇文章中，我们简单地提到了sds与string的关系；在本文介绍了robj的概念之后，我们重新总结一下sds与string的关系。 确切地说，string在Redis中是用一个robj来表示的。用来表示string的robj可能编码成3种内部表示：OBJ_ENCODING_RAW, OBJ_ENCODING_EMBSTR, OBJ_ENCODING_INT。其中前两种编码使用的是sds来存储，最后一种OBJ_ENCODING_INT编码直接把string存成了long型。在对string进行incr, decr等操作的时候，如果它内部是OBJ_ENCODING_INT编码，那么可以直接进行加减操作；如果它内部是OBJ_ENCODING_RAW或OBJ_ENCODING_EMBSTR编码，那么Redis会先试图把sds存储的字符串转成long型，如果能转成功，再进行加减操作。对一个内部表示成long型的string执行append, setbit, getrange这些命令，针对的仍然是string的值（即十进制表示的字符串），而不是针对内部表示的long型进行操作。比如字符串”32”，如果按照字符数组来解释，它包含两个字符，它们的ASCII码分别是0x33和0x32。当我们执行命令setbit key 7 0的时候，相当于把字符0x33变成了0x32，这样字符串的值就变成了”22”。而如果将字符串”32”按照内部的64位long型来解释，那么它是0x0000000000000020，在这个基础上执行setbit位操作，结果就完全不对了。因此，在这些命令的实现中，会把long型先转成字符串再进行相应的操作。由于篇幅原因，这三个命令的实现代码这里就不详细介绍了，有兴趣的读者可以参考Redis源码：t_string.c中的appendCommand函数；biops.c中的setbitCommand函数；t_string.c中的getrangeCommand函数。值得一提的是，append和setbit命令的实现中，都会最终调用到db.c中的dbUnshareStringValue函数，将string对象的内部编码转成OBJ_ENCODING_RAW的（只有这种编码的robj对象，其内部的sds 才能在后面自由追加新的内容），并解除可能存在的对象共享状态。这里面调用了前面提到的getDecodedObject。 12345678910robj *dbUnshareStringValue(redisDb *db, robj *key, robj *o) &#123;serverAssert(o-&gt;type == OBJ_STRING);if (o-&gt;refcount != 1 || o-&gt;encoding != OBJ_ENCODING_RAW) &#123;robj *decoded = getDecodedObject(o);o = createRawStringObject(decoded-&gt;ptr, sdslen(decoded-&gt;ptr));decrRefCount(decoded);dbOverwrite(db,key,o);&#125;return o;&#125; robj的引用计数操作将robj的引用计数加1和减1的操作，定义在object.c中： 1234567891011121314151617181920void incrRefCount(robj *o) &#123;o-&gt;refcount++;&#125;void decrRefCount(robj *o) &#123;if (o-&gt;refcount &lt;= 0) serverPanic(&quot;decrRefCount against refcount &lt;= 0&quot;);if (o-&gt;refcount == 1) &#123;switch(o-&gt;type) &#123;case OBJ_STRING: freeStringObject(o); break;case OBJ_LIST: freeListObject(o); break;case OBJ_SET: freeSetObject(o); break;case OBJ_ZSET: freeZsetObject(o); break;case OBJ_HASH: freeHashObject(o); break;default: serverPanic(&quot;Unknown object type&quot;); break;&#125;zfree(o);&#125; else &#123;o-&gt;refcount--;&#125;&#125; 我们特别关注一下将引用计数减1的操作decrRefCount。如果只剩下最后一个引用了（refcount已经是1了），那么在decrRefCount被调用后，整个robj将被释放。 注意：Redis的del命令就依赖decrRefCount操作将value释放掉。 经过了本文的讨论，我们很容易看出，robj所表示的就是Redis对外暴露的第一层面的数据结构：string, list, hash, set, sorted set，而每一种数据结构的底层实现所对应的是哪个（或哪些）第二层面的数据结构（dict, sds, ziplist, quicklist, skiplist, 等），则通过不同的encoding来区分。可以说，robj是联结两个层面的数据结构的桥梁。 本文详细介绍了OBJ_STRING类型的字符串对象的底层实现，其编码和解码过程在Redis里非常重要，应用广泛，我们在后面的讨论中可能还会遇到。现在有了robj的概念基础，我们下一篇会讨论ziplist，以及它与hash的关系。","categories":[{"name":"5.缓存","slug":"5-缓存","permalink":"https://zhangxin66666.github.io/categories/5-%E7%BC%93%E5%AD%98/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://zhangxin66666.github.io/tags/redis/"}]},{"title":"Redis内部数据结构详解(2)——sds","slug":"5.缓存/Redis内部数据结构详解(2)——sds","date":"2024-12-29T16:00:00.000Z","updated":"2024-12-30T06:11:41.977Z","comments":true,"path":"2024/12/30/5.缓存/Redis内部数据结构详解(2)——sds/","link":"","permalink":"https://zhangxin66666.github.io/2024/12/30/5.%E7%BC%93%E5%AD%98/Redis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3(2)%E2%80%94%E2%80%94sds/","excerpt":"","text":"不管在哪门编程语言当中，字符串都几乎是使用最多的数据结构。sds正是在Redis中被广泛使用的字符串结构，它的全称是Simple Dynamic String。与其它语言环境中出现的字符串相比，它具有如下显著的特点： 可动态扩展内存。sds表示的字符串其内容可以修改，也可以追加。在很多语言中字符串会分为mutable和immutable两种，显然sds属于mutable类型的。二进制安全（Binary Safe）。sds能存储任意二进制数据，而不仅仅是可打印字符。与传统的C语言字符串类型兼容。这个的含义接下来马上会讨论。看到这里，很多对Redis有所了解的同学可能已经产生了一个疑问：Redis已经对外暴露了一个字符串结构，叫做string，那这里所说的sds到底和string是什么关系呢？可能有人会猜：string是基于sds实现的。这个猜想已经非常接近事实，但在描述上还不太准确。有关string和sds之间关系的详细分析，我们放在后面再讲。现在为了方便讨论，让我们先暂时简单地认为，string的底层实现就是sds。 在讨论sds的具体实现之前，我们先站在Redis使用者的角度，来观察一下string所支持的一些主要操作。下面是一个操作示例： 以上这些操作都比较简单，我们简单解释一下： 初始的字符串的值设为”tielei”。第3步通过append命令对字符串进行了追加，变成了”tielei zhang”。然后通过setbit命令将第53个bit设置成了1。bit的偏移量从左边开始算，从0开始。其中第48～55bit是中间的空格那个字符，它的ASCII码是0x20。将第53个bit设置成1之后，它的ASCII码变成了0x24，打印出来就是’$’。因此，现在字符串的值变成了”tielei$zhang”。最后通过getrange取从倒数第5个字节到倒数第1个字节的内容，得到”zhang”。这些命令的实现，有一部分是和sds的实现有关的。下面我们开始详细讨论。 sds的数据结构定义我们知道，在C语言中，字符串是以’\\0’字符结尾（NULL结束符）的字符数组来存储的，通常表达为字符指针的形式（char *）。它不允许字节0出现在字符串中间，因此，它不能用来存储任意的二进制数据。 我们可以在sds.h中找到sds的类型定义： typedef char *sds;肯定有人感到困惑了，竟然sds就等同于char *？我们前面提到过，sds和传统的C语言字符串保持类型兼容，因此它们的类型定义是一样的，都是char *。在有些情况下，需要传入一个C语言字符串的地方，也确实可以传入一个sds。但是，sds和char *并不等同。sds是Binary Safe的，它可以存储任意二进制数据，不能像C语言字符串那样以字符’\\0’来标识字符串的结束，因此它必然有个长度字段。但这个长度字段在哪里呢？实际上sds还包含一个header结构： 12345678910111213141516171819202122232425262728struct __attribute__ ((__packed__)) sdshdr5 &#123;unsigned char flags; /* 3 lsb of type, and 5 msb of string length */char buf[];&#125;;struct __attribute__ ((__packed__)) sdshdr8 &#123;uint8_t len; /* used */uint8_t alloc; /* excluding the header and null terminator */unsigned char flags; /* 3 lsb of type, 5 unused bits */char buf[];&#125;;struct __attribute__ ((__packed__)) sdshdr16 &#123;uint16_t len; /* used */uint16_t alloc; /* excluding the header and null terminator */unsigned char flags; /* 3 lsb of type, 5 unused bits */char buf[];&#125;;struct __attribute__ ((__packed__)) sdshdr32 &#123;uint32_t len; /* used */uint32_t alloc; /* excluding the header and null terminator */unsigned char flags; /* 3 lsb of type, 5 unused bits */char buf[];&#125;;struct __attribute__ ((__packed__)) sdshdr64 &#123;uint64_t len; /* used */uint64_t alloc; /* excluding the header and null terminator */unsigned char flags; /* 3 lsb of type, 5 unused bits */char buf[];&#125;; sds一共有5种类型的header。之所以有5种，是为了能让不同长度的字符串可以使用不同大小的header。这样，短字符串就能使用较小的header，从而节省内存。 一个sds字符串的完整结构，由在内存地址上前后相邻的两部分组成： 一个header。通常包含字符串的长度(len)、最大容量(alloc)和flags。sdshdr5有所不同。一个字符数组。这个字符数组的长度等于最大容量+1。真正有效的字符串数据，其长度通常小于最大容量。在真正的字符串数据之后，是空余未用的字节（一般以字节0填充），允许在不重新分配内存的前提下让字符串数据向后做有限的扩展。在真正的字符串数据之后，还有一个NULL结束符，即ASCII码为0的’\\0’字符。这是为了和传统C字符串兼容。之所以字符数组的长度比最大容量多1个字节，就是为了在字符串长度达到最大容量时仍然有1个字节存放NULL结束符。除了sdshdr5之外，其它4个header的结构都包含3个字段： len: 表示字符串的真正长度（不包含NULL结束符在内）。alloc: 表示字符串的最大容量（不包含最后多余的那个字节）。flags: 总是占用一个字节。其中的最低3个bit用来表示header的类型。header的类型共有5种，在sds.h中有常量定义。#define SDS_TYPE_5 0#define SDS_TYPE_8 1#define SDS_TYPE_16 2#define SDS_TYPE_32 3#define SDS_TYPE_64 4sds的数据结构，我们有必要非常仔细地去解析它。 上图是sds的一个内部结构的例子。图中展示了两个sds字符串s1和s2的内存结构，一个使用sdshdr8类型的header，另一个使用sdshdr16类型的header。但它们都表达了同样的一个长度为6的字符串的值：”tielei”。下面我们结合代码，来解释每一部分的组成。 sds的字符指针（s1和s2）就是指向真正的数据（字符数组）开始的位置，而header位于内存地址较低的方向。在sds.h中有一些跟解析header有关的宏定义： 12345#define SDS_TYPE_MASK 7#define SDS_TYPE_BITS 3#define SDS_HDR_VAR(T,s) struct sdshdr##T *sh = (void*)((s)-(sizeof(struct sdshdr##T)));#define SDS_HDR(T,s) ((struct sdshdr##T *)((s)-(sizeof(struct sdshdr##T))))#define SDS_TYPE_5_LEN(f) ((f)&gt;&gt;SDS_TYPE_BITS) 其中SDS_HDR用来从sds字符串获得header起始位置的指针，比如SDS_HDR(8, s1)表示s1的header指针，SDS_HDR(16, s2)表示s2的header指针。 当然，使用SDS_HDR之前我们必须先知道到底是哪一种header，这样我们才知道SDS_HDR第1个参数应该传什么。由sds字符指针获得header类型的方法是，先向低地址方向偏移1个字节的位置，得到flags字段。比如，s1[-1]和s2[-1]分别获得了s1和s2的flags的值。然后取flags的最低3个bit得到header的类型。 由于s1[-1] == 0x01 == SDS_TYPE_8，因此s1的header类型是sdshdr8。由于s2[-1] == 0x02 == SDS_TYPE_16，因此s2的header类型是sdshdr16。有了header指针，就能很快定位到它的len和alloc字段： s1的header中，len的值为0x06，表示字符串数据长度为6；alloc的值为0x80，表示字符数组最大容量为128。s2的header中，len的值为0x0006，表示字符串数据长度为6；alloc的值为0x03E8，表示字符数组最大容量为1000。（注意：图中是按小端地址构成）在各个header的类型定义中，还有几个需要我们注意的地方： 在各个header的定义中使用了__attribute__ ((packed))，是为了让编译器以紧凑模式来分配内存。如果没有这个属性，编译器可能会为struct的字段做优化对齐，在其中填充空字节。那样的话，就不能保证header和sds的数据部分紧紧前后相邻，也不能按照固定向低地址方向偏移1个字节的方式来获取flags字段了。在各个header的定义中最后有一个char buf[]。我们注意到这是一个没有指明长度的字符数组，这是C语言中定义字符数组的一种特殊写法，称为柔性数组（flexible array member），只能定义在一个结构体的最后一个字段上。它在这里只是起到一个标记的作用，表示在flags字段后面就是一个字符数组，或者说，它指明了紧跟在flags字段后面的这个字符数组在结构体中的偏移位置。而程序在为header分配的内存的时候，它并不占用内存空间。如果计算sizeof(struct sdshdr16)的值，那么结果是5个字节，其中没有buf字段。sdshdr5与其它几个header结构不同，它不包含alloc字段，而长度使用flags的高5位来存储。因此，它不能为字符串分配空余空间。如果字符串需要动态增长，那么它就必然要重新分配内存才行。所以说，这种类型的sds字符串更适合存储静态的短字符串（长度小于32）。至此，我们非常清楚地看到了：sds字符串的header，其实隐藏在真正的字符串数据的前面（低地址方向）。这样的一个定义，有如下几个好处： header和数据相邻，而不用分成两块内存空间来单独分配。这有利于减少内存碎片，提高存储效率（memory efficiency）。虽然header有多个类型，但sds可以用统一的char *来表达。且它与传统的C语言字符串保持类型兼容。如果一个sds里面存储的是可打印字符串，那么我们可以直接把它传给C函数，比如使用strcmp比较字符串大小，或者使用printf进行打印。弄清了sds的数据结构，它的具体操作函数就比较好理解了。 sds的一些基础函数sdslen(const sds s): 获取sds字符串长度。sdssetlen(sds s, size_t newlen): 设置sds字符串长度。sdsinclen(sds s, size_t inc): 增加sds字符串长度。sdsalloc(const sds s): 获取sds字符串容量。sdssetalloc(sds s, size_t newlen): 设置sds字符串容量。sdsavail(const sds s): 获取sds字符串空余空间（即alloc - len）。sdsHdrSize(char type): 根据header类型得到header大小。sdsReqType(size_t string_size): 根据字符串数据长度计算所需要的header类型。这里我们挑选sdslen和sdsReqType的代码，察看一下。 12345678910111213141516171819202122232425262728static inline size_t sdslen(const sds s) &#123;unsigned char flags = s[-1];switch(flags&amp;SDS_TYPE_MASK) &#123;case SDS_TYPE_5:return SDS_TYPE_5_LEN(flags);case SDS_TYPE_8:return SDS_HDR(8,s)-&gt;len;case SDS_TYPE_16:return SDS_HDR(16,s)-&gt;len;case SDS_TYPE_32:return SDS_HDR(32,s)-&gt;len;case SDS_TYPE_64:return SDS_HDR(64,s)-&gt;len;&#125;return 0;&#125;static inline char sdsReqType(size_t string_size) &#123;if (string_size &lt; 1&lt;&lt;5)return SDS_TYPE_5;if (string_size &lt; 1&lt;&lt;8)return SDS_TYPE_8;if (string_size &lt; 1&lt;&lt;16)return SDS_TYPE_16;if (string_size &lt; 1ll&lt;&lt;32)return SDS_TYPE_32;return SDS_TYPE_64;&#125; 跟前面的分析类似，sdslen先用s[-1]向低地址方向偏移1个字节，得到flags；然后与SDS_TYPE_MASK进行按位与，得到header类型；然后根据不同的header类型，调用SDS_HDR得到header起始指针，进而获得len字段。 通过sdsReqType的代码，很容易看到： 长度在0和2^5-1之间，选用SDS_TYPE_5类型的header。长度在2^5和2^8-1之间，选用SDS_TYPE_8类型的header。长度在2^8和2^16-1之间，选用SDS_TYPE_16类型的header。长度在2^16和2^32-1之间，选用SDS_TYPE_32类型的header。长度大于2^32的，选用SDS_TYPE_64类型的header。能表示的最大长度为2^64-1。注：sdsReqType的实现代码，直到3.2.0，它在长度边界值上都一直存在问题，直到最近3.2 branch上的commit 6032340才修复。 sds的创建和销毁 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869sds sdsnewlen(const void *init, size_t initlen) &#123;void *sh;sds s;char type = sdsReqType(initlen);/* Empty strings are usually created in order to append. Use type 8* since type 5 is not good at this. */if (type == SDS_TYPE_5 &amp;&amp; initlen == 0) type = SDS_TYPE_8;int hdrlen = sdsHdrSize(type);unsigned char *fp; /* flags pointer. */ sh = s_malloc(hdrlen+initlen+1); if (!init) memset(sh, 0, hdrlen+initlen+1); if (sh == NULL) return NULL; s = (char*)sh+hdrlen; fp = ((unsigned char*)s)-1; switch(type) &#123; case SDS_TYPE_5: &#123; *fp = type | (initlen &lt;&lt; SDS_TYPE_BITS); break; &#125; case SDS_TYPE_8: &#123; SDS_HDR_VAR(8,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; &#125; case SDS_TYPE_16: &#123; SDS_HDR_VAR(16,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; &#125; case SDS_TYPE_32: &#123; SDS_HDR_VAR(32,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; &#125; case SDS_TYPE_64: &#123; SDS_HDR_VAR(64,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; &#125; &#125; if (initlen &amp;&amp; init) memcpy(s, init, initlen); s[initlen] = &#x27;\\0&#x27;; return s;&#125;sds sdsempty(void) &#123;return sdsnewlen(&quot;&quot;,0);&#125;sds sdsnew(const char *init) &#123;size_t initlen = (init == NULL) ? 0 : strlen(init);return sdsnewlen(init, initlen);&#125;void sdsfree(sds s) &#123;if (s == NULL) return;s_free((char*)s-sdsHdrSize(s[-1]));&#125; sdsnewlen创建一个长度为initlen的sds字符串，并使用init指向的字符数组（任意二进制数据）来初始化数据。如果init为NULL，那么使用全0来初始化数据。它的实现中，我们需要注意的是： 如果要创建一个长度为0的空字符串，那么不使用SDS_TYPE_5类型的header，而是转而使用SDS_TYPE_8类型的header。这是因为创建的空字符串一般接下来的操作很可能是追加数据，但SDS_TYPE_5类型的sds字符串不适合追加数据（会引发内存重新分配）。需要的内存空间一次性进行分配，其中包含三部分：header、数据、最后的多余字节（hdrlen+initlen+1）。初始化的sds字符串数据最后会追加一个NULL结束符（s[initlen] = ‘\\0’）。关于sdsfree，需要注意的是：内存要整体释放，所以要先计算出header起始指针，把它传给s_free函数。这个指针也正是在sdsnewlen中调用s_malloc返回的那个地址。 sds的连接（追加）操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263sds sdscatlen(sds s, const void *t, size_t len) &#123;size_t curlen = sdslen(s); s = sdsMakeRoomFor(s,len); if (s == NULL) return NULL; memcpy(s+curlen, t, len); sdssetlen(s, curlen+len); s[curlen+len] = &#x27;\\0&#x27;; return s;&#125;sds sdscat(sds s, const char *t) &#123;return sdscatlen(s, t, strlen(t));&#125;sds sdscatsds(sds s, const sds t) &#123;return sdscatlen(s, t, sdslen(t));&#125;sds sdsMakeRoomFor(sds s, size_t addlen) &#123;void *sh, *newsh;size_t avail = sdsavail(s);size_t len, newlen;char type, oldtype = s[-1] &amp; SDS_TYPE_MASK;int hdrlen; /* Return ASAP if there is enough space left. */ if (avail &gt;= addlen) return s; len = sdslen(s); sh = (char*)s-sdsHdrSize(oldtype); newlen = (len+addlen); if (newlen &lt; SDS_MAX_PREALLOC) newlen *= 2; else newlen += SDS_MAX_PREALLOC; type = sdsReqType(newlen); /* Don&#x27;t use type 5: the user is appending to the string and type 5 is * not able to remember empty space, so sdsMakeRoomFor() must be called * at every appending operation. */ if (type == SDS_TYPE_5) type = SDS_TYPE_8; hdrlen = sdsHdrSize(type); if (oldtype==type) &#123; newsh = s_realloc(sh, hdrlen+newlen+1); if (newsh == NULL) return NULL; s = (char*)newsh+hdrlen; &#125; else &#123; /* Since the header size changes, need to move the string forward, * and can&#x27;t use realloc */ newsh = s_malloc(hdrlen+newlen+1); if (newsh == NULL) return NULL; memcpy((char*)newsh+hdrlen, s, len+1); s_free(sh); s = (char*)newsh+hdrlen; s[-1] = type; sdssetlen(s, len); &#125; sdssetalloc(s, newlen); return s;&#125; sdscatlen将t指向的长度为len的任意二进制数据追加到sds字符串s的后面。本文开头演示的string的append命令，内部就是调用sdscatlen来实现的。 在sdscatlen的实现中，先调用sdsMakeRoomFor来保证字符串s有足够的空间来追加长度为len的数据。sdsMakeRoomFor可能会分配新的内存，也可能不会。 sdsMakeRoomFor是sds实现中很重要的一个函数。关于它的实现代码，我们需要注意的是： 如果原来字符串中的空余空间够用（avail &gt;= addlen），那么它什么也不做，直接返回。如果需要分配空间，它会比实际请求的要多分配一些，以防备接下来继续追加。它在字符串已经比较长的情况下要至少多分配SDS_MAX_PREALLOC个字节，这个常量在sds.h中定义为(1024*1024)=1MB。按分配后的空间大小，可能需要更换header类型（原来header的alloc字段太短，表达不了增加后的容量）。如果需要更换header，那么整个字符串空间（包括header）都需要重新分配（s_malloc），并拷贝原来的数据到新的位置。如果不需要更换header（原来的header够用），那么调用一个比较特殊的s_realloc，试图在原来的地址上重新分配空间。s_realloc的具体实现得看Redis编译的时候选用了哪个allocator（在Linux上默认使用jemalloc）。但不管是哪个realloc的实现，它所表达的含义基本是相同的：它尽量在原来分配好的地址位置重新分配，如果原来的地址位置有足够的空余空间完成重新分配，那么它返回的新地址与传入的旧地址相同；否则，它分配新的地址块，并进行数据搬迁。参见http://man.cx/realloc。从sdscatlen的函数接口，我们可以看到一种使用模式：调用它的时候，传入一个旧的sds变量，然后它返回一个新的sds变量。由于它的内部实现可能会造成地址变化，因此调用者在调用完之后，原来旧的变量就失效了，而都应该用新返回的变量来替换。不仅仅是sdscatlen函数，sds中的其它函数（比如sdscpy、sdstrim、sdsjoin等），还有Redis中其它一些能自动扩展内存的数据结构（如ziplist），也都是同样的使用模式。 浅谈sds与string的关系现在我们回过头来看看本文开头给出的string操作的例子。 append操作使用sds的sdscatlen来实现。前面已经提到。setbit和getrange都是先根据key取到整个sds字符串，然后再从字符串选取或修改指定的部分。由于sds就是一个字符数组，所以对它的某一部分进行操作似乎都比较简单。但是，string除了支持这些操作之外，当它存储的值是个数字的时候，它还支持incr、decr等操作。那么，当string存储数字值的时候，它的内部存储还是sds吗？实际上，不是了。而且，这种情况下，setbit和getrange的实现也会有所不同。这些细节，我们放在下一篇介绍robj的时候再进行系统地讨论。","categories":[{"name":"5.缓存","slug":"5-缓存","permalink":"https://zhangxin66666.github.io/categories/5-%E7%BC%93%E5%AD%98/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://zhangxin66666.github.io/tags/redis/"}]},{"title":"Redis内部数据结构详解(4)——ziplist","slug":"5.缓存/Redis内部数据结构详解(4)——ziplist","date":"2024-12-29T16:00:00.000Z","updated":"2024-12-30T06:24:59.653Z","comments":true,"path":"2024/12/30/5.缓存/Redis内部数据结构详解(4)——ziplist/","link":"","permalink":"https://zhangxin66666.github.io/2024/12/30/5.%E7%BC%93%E5%AD%98/Redis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3(4)%E2%80%94%E2%80%94ziplist/","excerpt":"","text":"在本文中，我们首先介绍一个新的Redis内部数据结构——ziplist，然后在文章后半部分我们会讨论一下在robj, dict和ziplist的基础上，Redis对外暴露的hash结构是怎样构建起来的。 我们在讨论中还会涉及到两个Redis配置（在redis.conf中的ADVANCED CONFIG部分）： 12hash-max-ziplist-entries 512hash-max-ziplist-value 64 本文的后半部分会对这两个配置做详细的解释。 什么是ziplistRedis官方对于ziplist的定义是（出自ziplist.c的文件头部注释）： The ziplist is a specially encoded dually linked list that is designed to be very memory efficient. It stores both strings and integer values, where integers are encoded as actual integers instead of a series of characters. It allows push and pop operations on either side of the list in O(1) time. 翻译一下就是说：ziplist是一个经过特殊编码的双向链表，它的设计目标就是为了提高存储效率。ziplist可以用于存储字符串或整数，其中整数是按真正的二进制表示进行编码的，而不是编码成字符串序列。它能以O(1)的时间复杂度在表的两端提供push和pop操作。 实际上，ziplist充分体现了Redis对于存储效率的追求。一个普通的双向链表，链表中每一项都占用独立的一块内存，各项之间用地址指针（或引用）连接起来。这种方式会带来大量的内存碎片，而且地址指针也会占用额外的内存。而ziplist却是将表中每一项存放在前后连续的地址空间内，一个ziplist整体占用一大块内存。它是一个表（list），但其实不是一个链表（linked list）。 另外，ziplist为了在细节上节省内存，对于值的存储采用了变长的编码方式，大概意思是说，对于大的整数，就多用一些字节来存储，而对于小的整数，就少用一些字节来存储。我们接下来很快就会讨论到这些实现细节。 ziplist的数据结构定义ziplist的数据结构组成是本文要讨论的重点。实际上，ziplist还是稍微有点复杂的，它复杂的地方就在于它的数据结构定义。一旦理解了数据结构，它的一些操作也就比较容易理解了。 我们接下来先从总体上介绍一下ziplist的数据结构定义，然后举一个实际的例子，通过例子来解释ziplist的构成。如果你看懂了这一部分，本文的任务就算完成了一大半了。 从宏观上看，ziplist的内存结构如下： … 各个部分在内存上是前后相邻的，它们分别的含义如下： : 32bit，表示ziplist占用的字节总数（也包括本身占用的4个字节）。: 32bit，表示ziplist表中最后一项（entry）在ziplist中的偏移字节数。的存在，使得我们可以很方便地找到最后一项（不用遍历整个ziplist），从而可以在ziplist尾端快速地执行push或pop操作。: 16bit， 表示ziplist中数据项（entry）的个数。zllen字段因为只有16bit，所以可以表达的最大值为2^16-1。这里需要特别注意的是，如果ziplist中数据项个数超过了16bit能表达的最大值，ziplist仍然可以来表示。那怎么表示呢？这里做了这样的规定：如果小于等于2^16-2（也就是不等于2^16-1），那么就表示ziplist中数据项的个数；否则，也就是等于16bit全为1的情况，那么就不表示数据项个数了，这时候要想知道ziplist中数据项总数，那么必须对ziplist从头到尾遍历各个数据项，才能计数出来。: 表示真正存放数据的数据项，长度不定。一个数据项（entry）也有它自己的内部结构，这个稍后再解释。: ziplist最后1个字节，是一个结束标记，值固定等于255。上面的定义中还值得注意的一点是：, , 既然占据多个字节，那么在存储的时候就有大端（big endian）和小端（little endian）的区别。ziplist采取的是小端模式来存储，这在下面我们介绍具体例子的时候还会再详细解释。 我们再来看一下每一个数据项的构成： 我们看到在真正的数据（）前面，还有两个字段： : 表示前一个数据项占用的总字节数。这个字段的用处是为了让ziplist能够从后向前遍历（从后一项的位置，只需向前偏移prevrawlen个字节，就找到了前一项）。这个字段采用变长编码。: 表示当前数据项的数据长度（即部分的长度）。也采用变长编码。那么和是怎么进行变长编码的呢？各位读者打起精神了，我们终于讲到了ziplist的定义中最繁琐的地方了。 先说。它有两种可能，或者是1个字节，或者是5个字节： 如果前一个数据项占用字节数小于254，那么就只用一个字节来表示，这个字节的值就是前一个数据项的占用字节数。如果前一个数据项占用字节数大于等于254，那么就用5个字节来表示，其中第1个字节的值是254（作为这种情况的一个标记），而后面4个字节组成一个整型值，来真正存储前一个数据项的占用字节数。有人会问了，为什么没有255的情况呢？ 这是因为：255已经定义为ziplist结束标记的值了。在ziplist的很多操作的实现中，都会根据数据项的第1个字节是不是255来判断当前是不是到达ziplist的结尾了，因此一个正常的数据的第1个字节（也就是的第1个字节）是不能够取255这个值的，否则就冲突了。 而字段就更加复杂了，它根据第1个字节的不同，总共分为9种情况（下面的表示法是按二进制表示）： |00pppppp| - 1 byte。第1个字节最高两个bit是00，那么字段只有1个字节，剩余的6个bit用来表示长度值，最高可以表示63 (2^6-1)。|01pppppp|qqqqqqqq| - 2 bytes。第1个字节最高两个bit是01，那么字段占2个字节，总共有14个bit用来表示长度值，最高可以表示16383 (2^14-1)。|10__|qqqqqqqq|rrrrrrrr|ssssssss|tttttttt| - 5 bytes。第1个字节最高两个bit是10，那么len字段占5个字节，总共使用32个bit来表示长度值（6个bit舍弃不用），最高可以表示2^32-1。需要注意的是：在前三种情况下，都是按字符串来存储的；从下面第4种情况开始，开始变为按整数来存储了。|11000000| - 1 byte。字段占用1个字节，值为0xC0，后面的数据存储为2个字节的int16_t类型。|11010000| - 1 byte。字段占用1个字节，值为0xD0，后面的数据存储为4个字节的int32_t类型。|11100000| - 1 byte。字段占用1个字节，值为0xE0，后面的数据存储为8个字节的int64_t类型。|11110000| - 1 byte。字段占用1个字节，值为0xF0，后面的数据存储为3个字节长的整数。|11111110| - 1 byte。字段占用1个字节，值为0xFE，后面的数据存储为1个字节的整数。|1111xxxx| - - (xxxx的值在0001和1101之间)。这是一种特殊情况，xxxx从1到13一共13个值，这时就用这13个值来表示真正的数据。注意，这里是表示真正的数据，而不是数据长度了。也就是说，在这种情况下，后面不再需要一个单独的字段来表示真正的数据了，而是和合二为一了。另外，由于xxxx只能取0001和1101这13个值了（其它可能的值和其它情况冲突了，比如0000和1110分别同前面第7种第8种情况冲突，1111跟结束标记冲突），而小数值应该从0开始，因此这13个值分别表示0到12，即xxxx的值减去1才是它所要表示的那个整数数据的值。好了，ziplist的数据结构定义，我们介绍完了，现在我们看一个具体的例子。 上图是一份真实的ziplist数据。我们逐项解读一下： 这个ziplist一共包含33个字节。字节编号从byte[0]到byte[32]。图中每个字节的值使用16进制表示。头4个字节（0x21000000）是按小端（little endian）模式存储的字段。什么是小端呢？就是指数据的低字节保存在内存的低地址中（参见维基百科词条Endianness）。因此，这里的值应该解析成0x00000021，用十进制表示正好就是33。接下来4个字节（byte[4..7]）是，用小端存储模式来解释，它的值是0x0000001D（值为29），表示最后一个数据项在byte[29]的位置（那个数据项为0x05FE14）。再接下来2个字节（byte[8..9]），值为0x0004，表示这个ziplist里一共存有4项数据。接下来6个字节（byte[10..15]）是第1个数据项。其中，prevrawlen=0，因为它前面没有数据项；len=4，相当于前面定义的9种情况中的第1种，表示后面4个字节按字符串存储数据，数据的值为”name”。接下来8个字节（byte[16..23]）是第2个数据项，与前面数据项存储格式类似，存储1个字符串”tielei”。接下来5个字节（byte[24..28]）是第3个数据项，与前面数据项存储格式类似，存储1个字符串”age”。接下来3个字节（byte[29..31]）是最后一个数据项，它的格式与前面的数据项存储格式不太一样。其中，第1个字节prevrawlen=5，表示前一个数据项占用5个字节；第2个字节=FE，相当于前面定义的9种情况中的第8种，所以后面还有1个字节用来表示真正的数据，并且以整数表示。它的值是20（0x14）。最后1个字节（byte[32]）表示，是固定的值255（0xFF）。总结一下，这个ziplist里存了4个数据项，分别为： 字符串: “name”字符串: “tielei”字符串: “age”整数: 20（好吧，被你发现了~~tielei实际上当然不是20岁，他哪有那么年轻啊……） 实际上，这个ziplist是通过两个hset命令创建出来的。这个我们后半部分会再提到。 好了，既然你已经阅读到这里了，说明你还是很有耐心的（其实我写到这里也已经累得不行了）。可以先把本文收藏，休息一下，回头再看后半部分。 接下来我要贴一些代码了。 ziplist的接口我们先不着急看实现，先来挑几个ziplist的重要的接口，看看它们长什么样子： 12345678910unsigned char *ziplistNew(void);unsigned char *ziplistMerge(unsigned char **first, unsigned char **second);unsigned char *ziplistPush(unsigned char *zl, unsigned char *s, unsigned int slen, int where);unsigned char *ziplistIndex(unsigned char *zl, int index);unsigned char *ziplistNext(unsigned char *zl, unsigned char *p);unsigned char *ziplistPrev(unsigned char *zl, unsigned char *p);unsigned char *ziplistInsert(unsigned char *zl, unsigned char *p, unsigned char *s, unsigned int slen);unsigned char *ziplistDelete(unsigned char *zl, unsigned char **p);unsigned char *ziplistFind(unsigned char *p, unsigned char *vstr, unsigned int vlen, unsigned int skip);unsigned int ziplistLen(unsigned char *zl); 我们从这些接口的名字就可以粗略猜出它们的功能，下面简单解释一下： ziplist的数据类型，没有用自定义的struct之类的来表达，而就是简单的unsigned char *。这是因为ziplist本质上就是一块连续内存，内部组成结构又是一个高度动态的设计（变长编码），也没法用一个固定的数据结构来表达。ziplistNew: 创建一个空的ziplist（只包含）。ziplistMerge: 将两个ziplist合并成一个新的ziplist。ziplistPush: 在ziplist的头部或尾端插入一段数据（产生一个新的数据项）。注意一下这个接口的返回值，是一个新的ziplist。调用方必须用这里返回的新的ziplist，替换之前传进来的旧的ziplist变量，而经过这个函数处理之后，原来旧的ziplist变量就失效了。为什么一个简单的插入操作会导致产生一个新的ziplist呢？这是因为ziplist是一块连续空间，对它的追加操作，会引发内存的realloc，因此ziplist的内存位置可能会发生变化。实际上，我们在之前介绍sds的文章中提到过类似这种接口使用模式（参见sdscatlen函数的说明）。ziplistIndex: 返回index参数指定的数据项的内存位置。index可以是负数，表示从尾端向前进行索引。ziplistNext和ziplistPrev分别返回一个ziplist中指定数据项p的后一项和前一项。ziplistInsert: 在ziplist的任意数据项前面插入一个新的数据项。ziplistDelete: 删除指定的数据项。ziplistFind: 查找给定的数据（由vstr和vlen指定）。注意它有一个skip参数，表示查找的时候每次比较之间要跳过几个数据项。为什么会有这么一个参数呢？其实这个参数的主要用途是当用ziplist表示hash结构的时候，是按照一个field，一个value来依次存入ziplist的。也就是说，偶数索引的数据项存field，奇数索引的数据项存value。当按照field的值进行查找的时候，就需要把奇数项跳过去。ziplistLen: 计算ziplist的长度（即包含数据项的个数）。ziplist的插入逻辑解析ziplist的相关接口的具体实现，还是有些复杂的，限于篇幅的原因，我们这里只结合代码来讲解插入的逻辑。插入是很有代表性的操作，通过这部分来一窥ziplist内部的实现，其它部分的实现我们也就会很容易理解了。 ziplistPush和ziplistInsert都是插入，只是对于插入位置的限定不同。它们在内部实现都依赖一个名为__ziplistInsert的内部函数，其代码如下（出自ziplist.c）: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889static unsigned char *__ziplistInsert(unsigned char *zl, unsigned char *p, unsigned char *s, unsigned int slen) &#123;size_t curlen = intrev32ifbe(ZIPLIST_BYTES(zl)), reqlen;unsigned int prevlensize, prevlen = 0;size_t offset;int nextdiff = 0;unsigned char encoding = 0;long long value = 123456789; /* initialized to avoid warning. Using a valuethat is easy to see if for some reasonwe use it uninitialized. */zlentry tail; /* Find out prevlen for the entry that is inserted. */ if (p[0] != ZIP_END) &#123; ZIP_DECODE_PREVLEN(p, prevlensize, prevlen); &#125; else &#123; unsigned char *ptail = ZIPLIST_ENTRY_TAIL(zl); if (ptail[0] != ZIP_END) &#123; prevlen = zipRawEntryLength(ptail); &#125; &#125; /* See if the entry can be encoded */ if (zipTryEncoding(s,slen,&amp;value,&amp;encoding)) &#123; /* &#x27;encoding&#x27; is set to the appropriate integer encoding */ reqlen = zipIntSize(encoding); &#125; else &#123; /* &#x27;encoding&#x27; is untouched, however zipEncodeLength will use the * string length to figure out how to encode it. */ reqlen = slen; &#125; /* We need space for both the length of the previous entry and * the length of the payload. */ reqlen += zipPrevEncodeLength(NULL,prevlen); reqlen += zipEncodeLength(NULL,encoding,slen); /* When the insert position is not equal to the tail, we need to * make sure that the next entry can hold this entry&#x27;s length in * its prevlen field. */ nextdiff = (p[0] != ZIP_END) ? zipPrevLenByteDiff(p,reqlen) : 0; /* Store offset because a realloc may change the address of zl. */ offset = p-zl; zl = ziplistResize(zl,curlen+reqlen+nextdiff); p = zl+offset; /* Apply memory move when necessary and update tail offset. */ if (p[0] != ZIP_END) &#123; /* Subtract one because of the ZIP_END bytes */ memmove(p+reqlen,p-nextdiff,curlen-offset-1+nextdiff); /* Encode this entry&#x27;s raw length in the next entry. */ zipPrevEncodeLength(p+reqlen,reqlen); /* Update offset for tail */ ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+reqlen); /* When the tail contains more than one entry, we need to take * &quot;nextdiff&quot; in account as well. Otherwise, a change in the * size of prevlen doesn&#x27;t have an effect on the *tail* offset. */ zipEntry(p+reqlen, &amp;tail); if (p[reqlen+tail.headersize+tail.len] != ZIP_END) &#123; ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+nextdiff); &#125; &#125; else &#123; /* This element will be the new tail. */ ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(p-zl); &#125; /* When nextdiff != 0, the raw length of the next entry has changed, so * we need to cascade the update throughout the ziplist */ if (nextdiff != 0) &#123; offset = p-zl; zl = __ziplistCascadeUpdate(zl,p+reqlen); p = zl+offset; &#125; /* Write the entry */ p += zipPrevEncodeLength(p,prevlen); p += zipEncodeLength(p,encoding,slen); if (ZIP_IS_STR(encoding)) &#123; memcpy(p,s,slen); &#125; else &#123; zipSaveInteger(p,value,encoding); &#125; ZIPLIST_INCR_LENGTH(zl,1); return zl;&#125; 我们来简单解析一下这段代码： 这个函数是在指定的位置p插入一段新的数据，待插入数据的地址指针是s，长度为slen。插入后形成一个新的数据项，占据原来p的配置，原来位于p位置的数据项以及后面的所有数据项，需要统一向后移动，给新插入的数据项留出空间。参数p指向的是ziplist中某一个数据项的起始位置，或者在向尾端插入的时候，它指向ziplist的结束标记。函数开始先计算出待插入位置前一个数据项的长度prevlen。这个长度要存入新插入的数据项的字段。然后计算当前数据项占用的总字节数reqlen，它包含三部分：, 和真正的数据。其中的数据部分会通过调用zipTryEncoding先来尝试转成整数。由于插入导致的ziplist对于内存的新增需求，除了待插入数据项占用的reqlen之外，还要考虑原来p位置的数据项（现在要排在待插入数据项之后）的字段的变化。本来它保存的是前一项的总长度，现在变成了保存当前插入的数据项的总长度。这样它的字段本身需要的存储空间也可能发生变化，这个变化可能是变大也可能是变小。这个变化了多少的值nextdiff，是调用zipPrevLenByteDiff计算出来的。如果变大了，nextdiff是正值，否则是负值。现在很容易算出来插入后新的ziplist需要多少字节了，然后调用ziplistResize来重新调整大小。ziplistResize的实现里会调用allocator的zrealloc，它有可能会造成数据拷贝。现在额外的空间有了，接下来就是将原来p位置的数据项以及后面的所有数据都向后挪动，并为它设置新的字段。此外，还可能需要调整ziplist的字段。最后，组装新的待插入数据项，放在位置p。hash与ziplisthash是Redis中可以用来存储一个对象结构的比较理想的数据类型。一个对象的各个属性，正好对应一个hash结构的各个field。 我们在网上很容易找到这样一些技术文章，它们会说存储一个对象，使用hash比string要节省内存。实际上这么说是有前提的，具体取决于对象怎么来存储。如果你把对象的多个属性存储到多个key上（各个属性值存成string），当然占的内存要多。但如果你采用一些序列化方法，比如Protocol Buffers，或者Apache Thrift，先把对象序列化为字节数组，然后再存入到Redis的string中，那么跟hash相比，哪一种更省内存，就不一定了。 当然，hash比序列化后再存入string的方式，在支持的操作命令上，还是有优势的：它既支持多个field同时存取（hmset/hmget），也支持按照某个特定的field单独存取（hset/hget）。 实际上，hash随着数据的增大，其底层数据结构的实现是会发生变化的，当然存储效率也就不同。在field比较少，各个value值也比较小的时候，hash采用ziplist来实现；而随着field增多和value值增大，hash可能会变成dict来实现。当hash底层变成dict来实现的时候，它的存储效率就没法跟那些序列化方式相比了。 当我们为某个key第一次执行 hset key field value 命令的时候，Redis会创建一个hash结构，这个新创建的hash底层就是一个ziplist。 123456robj *createHashObject(void) &#123;unsigned char *zl = ziplistNew();robj *o = createObject(OBJ_HASH, zl);o-&gt;encoding = OBJ_ENCODING_ZIPLIST;return o;&#125; 上面的createHashObject函数，出自object.c，它负责的任务就是创建一个新的hash结构。可以看出，它创建了一个type = OBJ_HASH但encoding = OBJ_ENCODING_ZIPLIST的robj对象。 实际上，本文前面给出的那个ziplist实例，就是由如下两个命令构建出来的。 12hset user:100 name tieleihset user:100 age 20 每执行一次hset命令，插入的field和value分别作为一个新的数据项插入到ziplist中（即每次hset产生两个数据项）。 当随着数据的插入，hash底层的这个ziplist就可能会转成dict。那么到底插入多少才会转呢？ 还记得本文开头提到的两个Redis配置吗？ 12hash-max-ziplist-entries 512hash-max-ziplist-value 64 这个配置的意思是说，在如下两个条件之一满足的时候，ziplist会转成dict： 当hash中的数据项（即field-value对）的数目超过512的时候，也就是ziplist数据项超过1024的时候（请参考t_hash.c中的hashTypeSet函数）。当hash中插入的任意一个value的长度超过了64的时候（请参考t_hash.c中的hashTypeTryConversion函数）。Redis的hash之所以这样设计，是因为当ziplist变得很大的时候，它有如下几个缺点： 每次插入或修改引发的realloc操作会有更大的概率造成内存拷贝，从而降低性能。一旦发生内存拷贝，内存拷贝的成本也相应增加，因为要拷贝更大的一块数据。当ziplist数据项过多的时候，在它上面查找指定的数据项就会性能变得很低，因为ziplist上的查找需要进行遍历。总之，ziplist本来就设计为各个数据项挨在一起组成连续的内存空间，这种结构并不擅长做修改操作。一旦数据发生改动，就会引发内存realloc，可能导致内存拷贝。","categories":[{"name":"5.缓存","slug":"5-缓存","permalink":"https://zhangxin66666.github.io/categories/5-%E7%BC%93%E5%AD%98/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://zhangxin66666.github.io/tags/redis/"}]},{"title":"Redis内部数据结构详解(5)——quicklist","slug":"5.缓存/Redis内部数据结构详解(5)——quicklist","date":"2024-12-29T16:00:00.000Z","updated":"2024-12-30T06:44:20.736Z","comments":true,"path":"2024/12/30/5.缓存/Redis内部数据结构详解(5)——quicklist/","link":"","permalink":"https://zhangxin66666.github.io/2024/12/30/5.%E7%BC%93%E5%AD%98/Redis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3(5)%E2%80%94%E2%80%94quicklist/","excerpt":"","text":"在本文中，我们介绍一个Redis内部数据结构——quicklist。Redis对外暴露的list数据类型，它底层实现所依赖的内部数据结构就是quicklist。 我们在讨论中还会涉及到两个Redis配置（在redis.conf中的ADVANCED CONFIG部分）： 12list-max-ziplist-size -2list-compress-depth 0 我们在讨论中会详细解释这两个配置的含义。 注：本文讨论的quicklist实现基于Redis源码的3.2分支。 quicklist概述Redis对外暴露的上层list数据类型，经常被用作队列使用。比如它支持的如下一些操作： lpush: 在左侧（即列表头部）插入数据。rpop: 在右侧（即列表尾部）删除数据。rpush: 在右侧（即列表尾部）插入数据。lpop: 在左侧（即列表头部）删除数据。这些操作都是O(1)时间复杂度的。 当然，list也支持在任意中间位置的存取操作，比如lindex和linsert，但它们都需要对list进行遍历，所以时间复杂度较高，为O(N)。 概况起来，list具有这样的一些特点：它是一个能维持数据项先后顺序的列表（各个数据项的先后顺序由插入位置决定），便于在表的两端追加和删除数据，而对于中间位置的存取具有O(N)的时间复杂度。这不正是一个双向链表所具有的特点吗？ list的内部实现quicklist正是一个双向链表。在quicklist.c的文件头部注释中，是这样描述quicklist的： A doubly linked list of ziplists 它确实是一个双向链表，而且是一个ziplist的双向链表。 这是什么意思呢？ 我们知道，双向链表是由多个节点（Node）组成的。这个描述的意思是：quicklist的每个节点都是一个ziplist。ziplist我们已经在上一篇介绍过。 ziplist本身也是一个能维持数据项先后顺序的列表（按插入位置），而且是一个内存紧缩的列表（各个数据项在内存上前后相邻）。比如，一个包含3个节点的quicklist，如果每个节点的ziplist又包含4个数据项，那么对外表现上，这个list就总共包含12个数据项。 quicklist的结构为什么这样设计呢？总结起来，大概又是一个空间和时间的折中： 双向链表便于在表的两端进行push和pop操作，但是它的内存开销比较大。首先，它在每个节点上除了要保存数据之外，还要额外保存两个指针；其次，双向链表的各个节点是单独的内存块，地址不连续，节点多了容易产生内存碎片。ziplist由于是一整块连续内存，所以存储效率很高。但是，它不利于修改操作，每次数据变动都会引发一次内存的realloc。特别是当ziplist长度很长的时候，一次realloc可能会导致大批量的数据拷贝，进一步降低性能。于是，结合了双向链表和ziplist的优点，quicklist就应运而生了。 不过，这也带来了一个新问题：到底一个quicklist节点包含多长的ziplist合适呢？比如，同样是存储12个数据项，既可以是一个quicklist包含3个节点，而每个节点的ziplist又包含4个数据项，也可以是一个quicklist包含6个节点，而每个节点的ziplist又包含2个数据项。 这又是一个需要找平衡点的难题。我们只从存储效率上分析一下： 每个quicklist节点上的ziplist越短，则内存碎片越多。内存碎片多了，有可能在内存中产生很多无法被利用的小碎片，从而降低存储效率。这种情况的极端是每个quicklist节点上的ziplist只包含一个数据项，这就蜕化成一个普通的双向链表了。每个quicklist节点上的ziplist越长，则为ziplist分配大块连续内存空间的难度就越大。有可能出现内存里有很多小块的空闲空间（它们加起来很多），但却找不到一块足够大的空闲空间分配给ziplist的情况。这同样会降低存储效率。这种情况的极端是整个quicklist只有一个节点，所有的数据项都分配在这仅有的一个节点的ziplist里面。这其实蜕化成一个ziplist了。可见，一个quicklist节点上的ziplist要保持一个合理的长度。那到底多长合理呢？这可能取决于具体应用场景。实际上，Redis提供了一个配置参数list-max-ziplist-size，就是为了让使用者可以来根据自己的情况进行调整。 1list-max-ziplist-size -2 我们来详细解释一下这个参数的含义。它可以取正值，也可以取负值。 当取正值的时候，表示按照数据项个数来限定每个quicklist节点上的ziplist长度。比如，当这个参数配置成5的时候，表示每个quicklist节点的ziplist最多包含5个数据项。 当取负值的时候，表示按照占用字节数来限定每个quicklist节点上的ziplist长度。这时，它只能取-1到-5这五个值，每个值含义如下： -5: 每个quicklist节点上的ziplist大小不能超过64 Kb。（注：1kb =&gt; 1024 bytes）-4: 每个quicklist节点上的ziplist大小不能超过32 Kb。-3: 每个quicklist节点上的ziplist大小不能超过16 Kb。-2: 每个quicklist节点上的ziplist大小不能超过8 Kb。（-2是Redis给出的默认值）-1: 每个quicklist节点上的ziplist大小不能超过4 Kb。另外，list的设计目标是能够用来存储很长的数据列表的。比如，Redis官网给出的这个教程：Writing a simple Twitter clone with PHP and Redis，就是使用list来存储类似Twitter的timeline数据。 当列表很长的时候，最容易被访问的很可能是两端的数据，中间的数据被访问的频率比较低（访问起来性能也很低）。如果应用场景符合这个特点，那么list还提供了一个选项，能够把中间的数据节点进行压缩，从而进一步节省内存空间。Redis的配置参数list-compress-depth就是用来完成这个设置的。 1list-compress-depth 0 这个参数表示一个quicklist两端不被压缩的节点个数。注：这里的节点个数是指quicklist双向链表的节点个数，而不是指ziplist里面的数据项个数。实际上，一个quicklist节点上的ziplist，如果被压缩，就是整体被压缩的。 参数list-compress-depth的取值含义如下： 0: 是个特殊值，表示都不压缩。这是Redis的默认值。1: 表示quicklist两端各有1个节点不压缩，中间的节点压缩。2: 表示quicklist两端各有2个节点不压缩，中间的节点压缩。3: 表示quicklist两端各有3个节点不压缩，中间的节点压缩。依此类推…由于0是个特殊值，很容易看出quicklist的头节点和尾节点总是不被压缩的，以便于在表的两端进行快速存取。 Redis对于quicklist内部节点的压缩算法，采用的LZF——一种无损压缩算法。 quicklist的数据结构定义quicklist相关的数据结构定义可以在quicklist.h中找到： 1234567891011121314151617181920212223242526typedef struct quicklistNode &#123;struct quicklistNode *prev;struct quicklistNode *next;unsigned char *zl;unsigned int sz; /* ziplist size in bytes */unsigned int count : 16; /* count of items in ziplist */unsigned int encoding : 2; /* RAW==1 or LZF==2 */unsigned int container : 2; /* NONE==1 or ZIPLIST==2 */unsigned int recompress : 1; /* was this node previous compressed? */unsigned int attempted_compress : 1; /* node can&#x27;t compress; too small */unsigned int extra : 10; /* more bits to steal for future usage */&#125; quicklistNode;typedef struct quicklistLZF &#123;unsigned int sz; /* LZF size in bytes*/char compressed[];&#125; quicklistLZF;typedef struct quicklist &#123;quicklistNode *head;quicklistNode *tail;unsigned long count; /* total count of all entries in all ziplists */unsigned int len; /* number of quicklistNodes */int fill : 16; /* fill factor for individual nodes */unsigned int compress : 16; /* depth of end nodes not to compress;0=off */&#125; quicklist; quicklistNode结构代表quicklist的一个节点，其中各个字段的含义如下： prev: 指向链表前一个节点的指针。next: 指向链表后一个节点的指针。zl: 数据指针。如果当前节点的数据没有压缩，那么它指向一个ziplist结构；否则，它指向一个quicklistLZF结构。sz: 表示zl指向的ziplist的总大小（包括zlbytes, zltail, zllen, zlend和各个数据项）。需要注意的是：如果ziplist被压缩了，那么这个sz的值仍然是压缩前的ziplist大小。count: 表示ziplist里面包含的数据项个数。这个字段只有16bit。稍后我们会一起计算一下这16bit是否够用。encoding: 表示ziplist是否压缩了（以及用了哪个压缩算法）。目前只有两种取值：2表示被压缩了（而且用的是LZF压缩算法），1表示没有压缩。container: 是一个预留字段。本来设计是用来表明一个quicklist节点下面是直接存数据，还是使用ziplist存数据，或者用其它的结构来存数据（用作一个数据容器，所以叫container）。但是，在目前的实现中，这个值是一个固定的值2，表示使用ziplist作为数据容器。recompress: 当我们使用类似lindex这样的命令查看了某一项本来压缩的数据时，需要把数据暂时解压，这时就设置recompress=1做一个标记，等有机会再把数据重新压缩。attempted_compress: 这个值只对Redis的自动化测试程序有用。我们不用管它。extra: 其它扩展字段。目前Redis的实现里也没用上。quicklistLZF结构表示一个被压缩过的ziplist。其中： sz: 表示压缩后的ziplist大小。compressed: 是个柔性数组（flexible array member），存放压缩后的ziplist字节数组。真正表示quicklist的数据结构是同名的quicklist这个struct： head: 指向头节点（左侧第一个节点）的指针。tail: 指向尾节点（右侧第一个节点）的指针。count: 所有ziplist数据项的个数总和。len: quicklist节点的个数。fill: 16bit，ziplist大小设置，存放list-max-ziplist-size参数的值。compress: 16bit，节点压缩深度设置，存放list-compress-depth参数的值。 上图是一个quicklist的结构图举例。图中例子对应的ziplist大小配置和节点压缩深度配置，如下： 12list-max-ziplist-size 3list-compress-depth 2 这个例子中我们需要注意的几点是： 两端各有2个橙黄色的节点，是没有被压缩的。它们的数据指针zl指向真正的ziplist。中间的其它节点是被压缩过的，它们的数据指针zl指向被压缩后的ziplist结构，即一个quicklistLZF结构。左侧头节点上的ziplist里有2项数据，右侧尾节点上的ziplist里有1项数据，中间其它节点上的ziplist里都有3项数据（包括压缩的节点内部）。这表示在表的两端执行过多次push和pop操作后的一个状态。现在我们来大概计算一下quicklistNode结构中的count字段这16bit是否够用。 我们已经知道，ziplist大小受到list-max-ziplist-size参数的限制。按照正值和负值有两种情况： 当这个参数取正值的时候，就是恰好表示一个quicklistNode结构中zl所指向的ziplist所包含的数据项的最大值。list-max-ziplist-size参数是由quicklist结构的fill字段来存储的，而fill字段是16bit，所以它所能表达的值能够用16bit来表示。当这个参数取负值的时候，能够表示的ziplist最大长度是64 Kb。而ziplist中每一个数据项，最少需要2个字节来表示：1个字节的prevrawlen，1个字节的data（len字段和data合二为一；详见上一篇）。所以，ziplist中数据项的个数不会超过32 K，用16bit来表达足够了。实际上，在目前的quicklist的实现中，ziplist的大小还会受到另外的限制，根本不会达到这里所分析的最大值。 下面进入代码分析阶段。 quicklist的创建当我们使用lpush或rpush命令第一次向一个不存在的list里面插入数据的时候，Redis会首先调用quicklistCreate接口创建一个空的quicklist。 1234567891011quicklist *quicklistCreate(void) &#123;struct quicklist *quicklist; quicklist = zmalloc(sizeof(*quicklist)); quicklist-&gt;head = quicklist-&gt;tail = NULL; quicklist-&gt;len = 0; quicklist-&gt;count = 0; quicklist-&gt;compress = 0; quicklist-&gt;fill = -2; return quicklist;&#125; 在很多介绍数据结构的书上，实现双向链表的时候经常会多增加一个空余的头节点，主要是为了插入和删除操作的方便。从上面quicklistCreate的代码可以看出，quicklist是一个不包含空余头节点的双向链表（head和tail都初始化为NULL）。 quicklist的push操作quicklist的push操作是调用quicklistPush来实现的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354void quicklistPush(quicklist *quicklist, void *value, const size_t sz,int where) &#123;if (where == QUICKLIST_HEAD) &#123;quicklistPushHead(quicklist, value, sz);&#125; else if (where == QUICKLIST_TAIL) &#123;quicklistPushTail(quicklist, value, sz);&#125;&#125;/* Add new entry to head node of quicklist.** Returns 0 if used existing head.* Returns 1 if new head created. */ int quicklistPushHead(quicklist *quicklist, void *value, size_t sz) &#123; quicklistNode *orig_head = quicklist-&gt;head; if (likely( _quicklistNodeAllowInsert(quicklist-&gt;head, quicklist-&gt;fill, sz))) &#123; quicklist-&gt;head-&gt;zl = ziplistPush(quicklist-&gt;head-&gt;zl, value, sz, ZIPLIST_HEAD); quicklistNodeUpdateSz(quicklist-&gt;head); &#125; else &#123; quicklistNode *node = quicklistCreateNode(); node-&gt;zl = ziplistPush(ziplistNew(), value, sz, ZIPLIST_HEAD); quicklistNodeUpdateSz(node); _quicklistInsertNodeBefore(quicklist, quicklist-&gt;head, node); &#125; quicklist-&gt;count++; quicklist-&gt;head-&gt;count++; return (orig_head != quicklist-&gt;head); &#125;/* Add new entry to tail node of quicklist.** Returns 0 if used existing tail.* Returns 1 if new tail created. */ int quicklistPushTail(quicklist *quicklist, void *value, size_t sz) &#123; quicklistNode *orig_tail = quicklist-&gt;tail; if (likely( _quicklistNodeAllowInsert(quicklist-&gt;tail, quicklist-&gt;fill, sz))) &#123; quicklist-&gt;tail-&gt;zl = ziplistPush(quicklist-&gt;tail-&gt;zl, value, sz, ZIPLIST_TAIL); quicklistNodeUpdateSz(quicklist-&gt;tail); &#125; else &#123; quicklistNode *node = quicklistCreateNode(); node-&gt;zl = ziplistPush(ziplistNew(), value, sz, ZIPLIST_TAIL); quicklistNodeUpdateSz(node); _quicklistInsertNodeAfter(quicklist, quicklist-&gt;tail, node); &#125; quicklist-&gt;count++; quicklist-&gt;tail-&gt;count++; return (orig_tail != quicklist-&gt;tail); &#125; 不管是在头部还是尾部插入数据，都包含两种情况： 如果头节点（或尾节点）上ziplist大小没有超过限制（即_quicklistNodeAllowInsert返回1），那么新数据被直接插入到ziplist中（调用ziplistPush）。如果头节点（或尾节点）上ziplist太大了，那么新创建一个quicklistNode节点（对应地也会新创建一个ziplist），然后把这个新创建的节点插入到quicklist双向链表中（调用_quicklistInsertNodeAfter）。在_quicklistInsertNodeAfter的实现中，还会根据list-compress-depth的配置将里面的节点进行压缩。它的实现比较繁琐，我们这里就不展开讨论了。 quicklist的其它操作quicklist的操作较多，且实现细节都比较繁杂，这里就不一一分析源码了，我们简单介绍一些比较重要的操作。 quicklist的pop操作是调用quicklistPopCustom来实现的。quicklistPopCustom的实现过程基本上跟quicklistPush相反，先从头部或尾部节点的ziplist中把对应的数据项删除，如果在删除后ziplist为空了，那么对应的头部或尾部节点也要删除。删除后还可能涉及到里面节点的解压缩问题。 quicklist不仅实现了从头部或尾部插入，也实现了从任意指定的位置插入。quicklistInsertAfter和quicklistInsertBefore就是分别在指定位置后面和前面插入数据项。这种在任意指定位置插入数据的操作，情况比较复杂，有众多的逻辑分支。 当插入位置所在的ziplist大小没有超过限制时，直接插入到ziplist中就好了；当插入位置所在的ziplist大小超过了限制，但插入的位置位于ziplist两端，并且相邻的quicklist链表节点的ziplist大小没有超过限制，那么就转而插入到相邻的那个quicklist链表节点的ziplist中；当插入位置所在的ziplist大小超过了限制，但插入的位置位于ziplist两端，并且相邻的quicklist链表节点的ziplist大小也超过限制，这时需要新创建一个quicklist链表节点插入。对于插入位置所在的ziplist大小超过了限制的其它情况（主要对应于在ziplist中间插入数据的情况），则需要把当前ziplist分裂为两个节点，然后再其中一个节点上插入数据。quicklistSetOptions用于设置ziplist大小配置参数（list-max-ziplist-size）和节点压缩深度配置参数（list-compress-depth）。代码比较简单，就是将相应的值分别设置给quicklist结构的fill字段和compress字段。","categories":[{"name":"5.缓存","slug":"5-缓存","permalink":"https://zhangxin66666.github.io/categories/5-%E7%BC%93%E5%AD%98/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://zhangxin66666.github.io/tags/redis/"}]},{"title":"Redis内部数据结构详解(1)——dict","slug":"5.缓存/Redis内部数据结构详解(1)——dict","date":"2024-12-25T16:00:00.000Z","updated":"2024-12-30T06:06:47.619Z","comments":true,"path":"2024/12/26/5.缓存/Redis内部数据结构详解(1)——dict/","link":"","permalink":"https://zhangxin66666.github.io/2024/12/26/5.%E7%BC%93%E5%AD%98/Redis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3(1)%E2%80%94%E2%80%94dict/","excerpt":"","text":"如果你使用过Redis，一定会像我一样对它的内部实现产生兴趣。《Redis内部数据结构详解》是我准备写的一个系列，也是我个人对于之前研究Redis的一个阶段性总结，着重讲解Redis在内存中的数据结构实现（暂不涉及持久化的话题）。Redis本质上是一个数据结构服务器（data structures server），以高效的方式实现了多种现成的数据结构，研究它的数据结构和基于其上的算法，对于我们自己提升局部算法的编程水平有很重要的参考意义。 当我们在本文中提到Redis的“数据结构”，可能是在两个不同的层面来讨论它。 第一个层面，是从使用者的角度。比如： stringlisthashsetsorted set这一层面也是Redis暴露给外部的调用接口。 第二个层面，是从内部实现的角度，属于更底层的实现。比如： dictsdsziplistquicklistskiplist第一个层面的“数据结构”，Redis的官方文档(http://redis.io/topics/data-types-intro)有详细的介绍。本文的重点在于讨论第二个层面，Redis数据结构的内部实现，以及这两个层面的数据结构之间的关系：Redis如何通过组合第二个层面的各种基础数据结构来实现第一个层面的更高层的数据结构。 在讨论任何一个系统的内部实现的时候，我们都要先明确它的设计原则，这样我们才能更深刻地理解它为什么会进行如此设计的真正意图。在本文接下来的讨论中，我们主要关注以下几点： 存储效率（memory efficiency）。Redis是专用于存储数据的，它对于计算机资源的主要消耗就在于内存，因此节省内存是它非常非常重要的一个方面。这意味着Redis一定是非常精细地考虑了压缩数据、减少内存碎片等问题。快速响应时间（fast response time）。与快速响应时间相对的，是高吞吐量（high throughput）。Redis是用于提供在线访问的，对于单个请求的响应时间要求很高，因此，快速响应时间是比高吞吐量更重要的目标。有时候，这两个目标是矛盾的。单线程（single-threaded）。Redis的性能瓶颈不在于CPU资源，而在于内存访问和网络IO。而采用单线程的设计带来的好处是，极大简化了数据结构和算法的实现。相反，Redis通过异步IO和pipelining等机制来实现高速的并发访问。显然，单线程的设计，对于单个请求的快速响应时间也提出了更高的要求。本文是《Redis内部数据结构详解》系列的第一篇，讲述Redis一个重要的基础数据结构：dict。 dict是一个用于维护key和value映射关系的数据结构，与很多语言中的Map或dictionary类似。Redis的一个database中所有key到value的映射，就是使用一个dict来维护的。不过，这只是它在Redis中的一个用途而已，它在Redis中被使用的地方还有很多。比如，一个Redis hash结构，当它的field较多时，便会采用dict来存储。再比如，Redis配合使用dict和skiplist来共同维护一个sorted set。这些细节我们后面再讨论，在本文中，我们集中精力讨论dict本身的实现。 dict本质上是为了解决算法中的查找问题（Searching），一般查找问题的解法分为两个大类：一个是基于各种平衡树，一个是基于哈希表。我们平常使用的各种Map或dictionary，大都是基于哈希表实现的。在不要求数据有序存储，且能保持较低的哈希值冲突概率的前提下，基于哈希表的查找性能能做到非常高效，接近O(1)，而且实现简单。 在Redis中，dict也是一个基于哈希表的算法。和传统的哈希算法类似，它采用某个哈希函数从key计算得到在哈希表中的位置，采用拉链法解决冲突，并在装载因子（load factor）超过预定值时自动扩展内存，引发重哈希（rehashing）。Redis的dict实现最显著的一个特点，就在于它的重哈希。它采用了一种称为增量式重哈希（incremental rehashing）的方法，在需要扩展内存时避免一次性对所有key进行重哈希，而是将重哈希操作分散到对于dict的各个增删改查的操作中去。这种方法能做到每次只对一小部分key进行重哈希，而每次重哈希之间不影响dict的操作。dict之所以这样设计，是为了避免重哈希期间单个请求的响应时间剧烈增加，这与前面提到的“快速响应时间”的设计原则是相符的。 下面进行详细介绍。 dict的数据结构定义为了实现增量式重哈希（incremental rehashing），dict的数据结构里包含两个哈希表。在重哈希期间，数据从第一个哈希表向第二个哈希表迁移。 dict的C代码定义如下（出自Redis源码dict.h）： 123456789101112131415161718192021222324252627282930313233343536ypedef struct dictEntry &#123; void *key; union &#123; void *val; uint64_t u64; int64_t s64; double d; &#125; v; struct dictEntry *next;&#125; dictEntry;typedef struct dictType &#123; unsigned int (*hashFunction)(const void *key); void *(*keyDup)(void *privdata, const void *key); void *(*valDup)(void *privdata, const void *obj); int (*keyCompare)(void *privdata, const void *key1, const void *key2); void (*keyDestructor)(void *privdata, void *key); void (*valDestructor)(void *privdata, void *obj);&#125; dictType;/* This is our hash table structure. Every dictionary has two of this as we * implement incremental rehashing, for the old to the new table. */typedef struct dictht &#123; dictEntry **table; unsigned long size; unsigned long sizemask; unsigned long used;&#125; dictht;typedef struct dict &#123; dictType *type; void *privdata; dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ int iterators; /* number of iterators currently running */&#125; dict; 为了能更清楚地展示dict的数据结构定义，我们用一张结构图来表示它。如下。 结合上面的代码和结构图，可以很清楚地看出dict的结构。一个dict由如下若干项组成： 一个指向dictType结构的指针（type）。它通过自定义的方式使得dict的key和value能够存储任何类型的数据。一个私有数据指针（privdata）。由调用者在创建dict的时候传进来。两个哈希表（ht[2]）。只有在重哈希的过程中，ht[0]和ht[1]才都有效。而在平常情况下，只有ht[0]有效，ht[1]里面没有任何数据。上图表示的就是重哈希进行到中间某一步时的情况。当前重哈希索引（rehashidx）。如果rehashidx = -1，表示当前没有在重哈希过程中；否则，表示当前正在进行重哈希，且它的值记录了当前重哈希进行到哪一步了。当前正在进行遍历的iterator的个数。这不是我们现在讨论的重点，暂时忽略。dictType结构包含若干函数指针，用于dict的调用者对涉及key和value的各种操作进行自定义。这些操作包含： hashFunction，对key进行哈希值计算的哈希算法。keyDup和valDup，分别定义key和value的拷贝函数，用于在需要的时候对key和value进行深拷贝，而不仅仅是传递对象指针。keyCompare，定义两个key的比较操作，在根据key进行查找时会用到。keyDestructor和valDestructor，分别定义对key和value的析构函数。私有数据指针（privdata）就是在dictType的某些操作被调用时会传回给调用者。 需要详细察看的是dictht结构。它定义一个哈希表的结构，由如下若干项组成： 一个dictEntry指针数组（table）。key的哈希值最终映射到这个数组的某个位置上（对应一个bucket）。如果多个key映射到同一个位置，就发生了冲突，那么就拉出一个dictEntry链表。size：标识dictEntry指针数组的长度。它总是2的指数。sizemask：用于将哈希值映射到table的位置索引。它的值等于(size-1)，比如7, 15, 31, 63，等等，也就是用二进制表示的各个bit全1的数字。每个key先经过hashFunction计算得到一个哈希值，然后计算(哈希值 &amp; sizemask)得到在table上的位置。相当于计算取余(哈希值 % size)。used：记录dict中现有的数据个数。它与size的比值就是装载因子（load factor）。这个比值越大，哈希值冲突概率越高。dictEntry结构中包含k, v和指向链表下一项的next指针。k是void指针，这意味着它可以指向任何类型。v是个union，当它的值是uint64_t、int64_t或double类型时，就不再需要额外的存储，这有利于减少内存碎片。当然，v也可以是void指针，以便能存储任何类型的数据。 dict的创建（dictCreate） 12345678910111213141516171819202122232425262728dict *dictCreate(dictType *type,void *privDataPtr)&#123;dict *d = zmalloc(sizeof(*d)); _dictInit(d,type,privDataPtr); return d;&#125;int _dictInit(dict *d, dictType *type,void *privDataPtr)&#123;_dictReset(&amp;d-&gt;ht[0]);_dictReset(&amp;d-&gt;ht[1]);d-&gt;type = type;d-&gt;privdata = privDataPtr;d-&gt;rehashidx = -1;d-&gt;iterators = 0;return DICT_OK;&#125;static void _dictReset(dictht *ht)&#123;ht-&gt;table = NULL;ht-&gt;size = 0;ht-&gt;sizemask = 0;ht-&gt;used = 0;&#125; dictCreate为dict的数据结构分配空间并为各个变量赋初值。其中两个哈希表ht[0]和ht[1]起始都没有分配空间，table指针都赋为NULL。这意味着要等第一个数据插入时才会真正分配空间。 dict的查找（dictFind） 12345678910111213141516171819202122#define dictIsRehashing(d) ((d)-&gt;rehashidx != -1)dictEntry *dictFind(dict *d, const void *key)&#123;dictEntry *he;unsigned int h, idx, table; if (d-&gt;ht[0].used + d-&gt;ht[1].used == 0) return NULL; /* dict is empty */ if (dictIsRehashing(d)) _dictRehashStep(d); h = dictHashKey(d, key); for (table = 0; table &lt;= 1; table++) &#123; idx = h &amp; d-&gt;ht[table].sizemask; he = d-&gt;ht[table].table[idx]; while(he) &#123; if (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) return he; he = he-&gt;next; &#125; if (!dictIsRehashing(d)) return NULL; &#125; return NULL;&#125; 上述dictFind的源码，根据dict当前是否正在重哈希，依次做了这么几件事： 如果当前正在进行重哈希，那么将重哈希过程向前推进一步（即调用_dictRehashStep）。实际上，除了查找，插入和删除也都会触发这一动作。这就将重哈希过程分散到各个查找、插入和删除操作中去了，而不是集中在某一个操作中一次性做完。计算key的哈希值（调用dictHashKey，里面的实现会调用前面提到的hashFunction）。先在第一个哈希表ht[0]上进行查找。在table数组上定位到哈希值对应的位置（如前所述，通过哈希值与sizemask进行按位与），然后在对应的dictEntry链表上进行查找。查找的时候需要对key进行比较，这时候调用dictCompareKeys，它里面的实现会调用到前面提到的keyCompare。如果找到就返回该项。否则，进行下一步。判断当前是否在重哈希，如果没有，那么在ht[0]上的查找结果就是最终结果（没找到，返回NULL）。否则，在ht[1]上进行查找（过程与上一步相同）。下面我们有必要看一下增量式重哈希的_dictRehashStep的实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748static void _dictRehashStep(dict *d) &#123;if (d-&gt;iterators == 0) dictRehash(d,1);&#125;int dictRehash(dict *d, int n) &#123;int empty_visits = n*10; /* Max number of empty buckets to visit. */if (!dictIsRehashing(d)) return 0; while(n-- &amp;&amp; d-&gt;ht[0].used != 0) &#123; dictEntry *de, *nextde; /* Note that rehashidx can&#x27;t overflow as we are sure there are more * elements because ht[0].used != 0 */ assert(d-&gt;ht[0].size &gt; (unsigned long)d-&gt;rehashidx); while(d-&gt;ht[0].table[d-&gt;rehashidx] == NULL) &#123; d-&gt;rehashidx++; if (--empty_visits == 0) return 1; &#125; de = d-&gt;ht[0].table[d-&gt;rehashidx]; /* Move all the keys in this bucket from the old to the new hash HT */ while(de) &#123; unsigned int h; nextde = de-&gt;next; /* Get the index in the new hash table */ h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[1].sizemask; de-&gt;next = d-&gt;ht[1].table[h]; d-&gt;ht[1].table[h] = de; d-&gt;ht[0].used--; d-&gt;ht[1].used++; de = nextde; &#125; d-&gt;ht[0].table[d-&gt;rehashidx] = NULL; d-&gt;rehashidx++; &#125; /* Check if we already rehashed the whole table... */ if (d-&gt;ht[0].used == 0) &#123; zfree(d-&gt;ht[0].table); d-&gt;ht[0] = d-&gt;ht[1]; _dictReset(&amp;d-&gt;ht[1]); d-&gt;rehashidx = -1; return 0; &#125; /* More to rehash... */ return 1;&#125; dictRehash每次将重哈希至少向前推进n步（除非不到n步整个重哈希就结束了），每一步都将ht[0]上某一个bucket（即一个dictEntry链表）上的每一个dictEntry移动到ht[1]上，它在ht[1]上的新位置根据ht[1]的sizemask进行重新计算。rehashidx记录了当前尚未迁移（有待迁移）的ht[0]的bucket位置。 如果dictRehash被调用的时候，rehashidx指向的bucket里一个dictEntry也没有，那么它就没有可迁移的数据。这时它尝试在ht[0].table数组中不断向后遍历，直到找到下一个存有数据的bucket位置。如果一直找不到，则最多走n*10步，本次重哈希暂告结束。 最后，如果ht[0]上的数据都迁移到ht[1]上了（即d-&gt;ht[0].used == 0），那么整个重哈希结束，ht[0]变成ht[1]的内容，而ht[1]重置为空。 根据以上对于重哈希过程的分析，我们容易看出，本文前面的dict结构图中所展示的正是rehashidx=2时的情况，前面两个bucket（ht[0].table[0]和ht[0].table[1]）都已经迁移到ht[1]上去了。 dict的插入（dictAdd和dictReplace）dictAdd插入新的一对key和value，如果key已经存在，则插入失败。 dictReplace也是插入一对key和value，不过在key存在的时候，它会更新value。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960int dictAdd(dict *d, void *key, void *val)&#123;dictEntry *entry = dictAddRaw(d,key); if (!entry) return DICT_ERR; dictSetVal(d, entry, val); return DICT_OK;&#125;dictEntry *dictAddRaw(dict *d, void *key)&#123;int index;dictEntry *entry;dictht *ht; if (dictIsRehashing(d)) _dictRehashStep(d); /* Get the index of the new element, or -1 if * the element already exists. */ if ((index = _dictKeyIndex(d, key)) == -1) return NULL; /* Allocate the memory and store the new entry. * Insert the element in top, with the assumption that in a database * system it is more likely that recently added entries are accessed * more frequently. */ ht = dictIsRehashing(d) ? &amp;d-&gt;ht[1] : &amp;d-&gt;ht[0]; entry = zmalloc(sizeof(*entry)); entry-&gt;next = ht-&gt;table[index]; ht-&gt;table[index] = entry; ht-&gt;used++; /* Set the hash entry fields. */ dictSetKey(d, entry, key); return entry;&#125;static int _dictKeyIndex(dict *d, const void *key)&#123;unsigned int h, idx, table;dictEntry *he; /* Expand the hash table if needed */ if (_dictExpandIfNeeded(d) == DICT_ERR) return -1; /* Compute the key hash value */ h = dictHashKey(d, key); for (table = 0; table &lt;= 1; table++) &#123; idx = h &amp; d-&gt;ht[table].sizemask; /* Search if this slot does not already contain the given key */ he = d-&gt;ht[table].table[idx]; while(he) &#123; if (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) return -1; he = he-&gt;next; &#125; if (!dictIsRehashing(d)) break; &#125; return idx;&#125; 以上是dictAdd的关键实现代码。我们主要需要注意以下几点： 它也会触发推进一步重哈希（_dictRehashStep）。如果正在重哈希中，它会把数据插入到ht[1]；否则插入到ht[0]。在对应的bucket中插入数据的时候，总是插入到dictEntry的头部。因为新数据接下来被访问的概率可能比较高，这样再次查找它时就比较次数较少。_dictKeyIndex在dict中寻找插入位置。如果不在重哈希过程中，它只查找ht[0]；否则查找ht[0]和ht[1]。_dictKeyIndex可能触发dict内存扩展（_dictExpandIfNeeded，它将哈希表长度扩展为原来两倍，具体请参考dict.c中源码）。dictReplace在dictAdd基础上实现，如下： 1234567891011121314151617181920int dictReplace(dict *d, void *key, void *val)&#123;dictEntry *entry, auxentry; /* Try to add the element. If the key * does not exists dictAdd will suceed. */ if (dictAdd(d, key, val) == DICT_OK) return 1; /* It already exists, get the entry */ entry = dictFind(d, key); /* Set the new value and free the old one. Note that it is important * to do that in this order, as the value may just be exactly the same * as the previous one. In this context, think to reference counting, * you want to increment (set), and then decrement (free), and not the * reverse. */ auxentry = *entry; dictSetVal(d, entry, val); dictFreeVal(d, &amp;auxentry); return 0;&#125; 在key已经存在的情况下，dictReplace会同时调用dictAdd和dictFind，这其实相当于两次查找过程。这里Redis的代码不够优化。 dict的删除（dictDelete）dictDelete的源码这里忽略，具体请参考dict.c。需要稍加注意的是： dictDelete也会触发推进一步重哈希（_dictRehashStep）如果当前不在重哈希过程中，它只在ht[0]中查找要删除的key；否则ht[0]和ht[1]它都要查找。删除成功后会调用key和value的析构函数（keyDestructor和valDestructor）。","categories":[{"name":"5.缓存","slug":"5-缓存","permalink":"https://zhangxin66666.github.io/categories/5-%E7%BC%93%E5%AD%98/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://zhangxin66666.github.io/tags/redis/"}]},{"title":"redis面试","slug":"5.缓存/面试-redis","date":"2024-12-24T16:00:00.000Z","updated":"2025-01-03T11:56:20.959Z","comments":true,"path":"2024/12/25/5.缓存/面试-redis/","link":"","permalink":"https://zhangxin66666.github.io/2024/12/25/5.%E7%BC%93%E5%AD%98/%E9%9D%A2%E8%AF%95-redis/","excerpt":"","text":"1.底层数据结构地址：Redis内部数据结构详解(1)——dict地址：Redis内部数据结构详解(2)——sds地址：Redis内部数据结构详解(3)——robj地址：Redis内部数据结构详解(4)——ziplist地址：Redis内部数据结构详解(5)——quicklist地址：Redis内部数据结构详解(6)——skiplist地址：Redis内部数据结构详解(7)——intset 2.数据结构-对象头RedisObject16字节的存储空间 3.数据结构-string 相比较于C语言中的字符串，头部信息里面包含了字符串的实际长度len，可以通过O（1）的时间复杂度得到 有点类似于Java中的ArrayList 数组长度和数组容量使用范型来定义是为了追求内存的极致优化，对于不同长度的字符串，底层采用不同类型的数据来保存，字符串比较短的时候，可以使用byte和short来表示。 4.数据结构-string 三种底层编码方式地址：Redis内部数据结构详解(2)——sds 1).当存储的字符串全是数字的时候，此时使用int方式来存储 2).当存储的字符串长度小于等于44字符的时候，使用embstr方式来存储 对象头和SDS对象本身在内存中地址连续 3).长度大于44字符时，采用raw方式存储 对象头和SDS对象本身在内存中地址不连续 5.数据结构-string 为什么上面的阈值是44呢？一个字符串包含RedisObject和SDS的数据结构，至少会占用19（16 + 3）个字节的空间大小。 C语言中的内存分配器分配内存大小的单位都是2的n次方，为了容纳一个完整的embstr对象，最少要分配32字节的空间。稍微长一些就是64字节了。所以定义大于64字节就属于大字符串。 64-19 = 45.剩余可防存放字符串的空间45字节，而字符串又是以NULL结尾，占据了1个字节，所以阈值为44 6.数据结构-List1).Redis3.2版本之前，使用两种数据结构作为底层实现。 1.压缩列表 zipList地址：Redis内部数据结构详解(4)——ziplist 设计初衷是为了节约内存。使用一块连续的内存空间存储。每个元素长度不同，采用变长编码 如果采用双向链表，那么就需要额外的内存空间来维护头尾两个指针，在zipList在结构上可以得到上一个结点的长度和当前结点的长度。通过上一个结点的长度可以定位到上一个元素起始的位置，而通过当前结点的长度，可以将指针定位到下一个元素的起始位置。 变长编码体现在prevrawlensize属性，它记录的是prerawlen的大小，分为两种。 a.若前一个结点的长度小于254字节，那么则使用1字节来存储prerawlen b.若前一个结点的长度大于等于254字节，那么将第一个字节设置为254，然后接下来的4个字节保存实际的长度。也就是用5个字节来表示prerawlen的长度。 存在连锁更新问题 2.双向链表 LinkedList 2).3.2之后，将压缩列表和双向链表结合，称之为quickList地址：Redis内部数据结构详解(5)——quicklist 7.数据结构-hash1).当数据量较小的时候，采用zipList作为hash的底层实现地址：Redis内部数据结构详解(4)——ziplist 2).另一种方式是字典dict地址：Redis内部数据结构详解(1)——dict rehashidx用于标记rehash的进度，为0 表示rehash开始 hash表dictht结构 table 2维数组，第一维度数组表示hash表的槽位，第二个维度是每一个槽对应的链表 真正的存储数据的结构dictEnrty 完整结构图 8.dict的扩容和缩容1).扩容 是为了减少hash冲突的概率 扩容的时机： a.BGSAVE和BGREWRITEAOF指令的情况下，hash表的负载因子大于等于1的时候进行扩容。 b.正在执行BGSAVE和BGREWRITEAOF指令的情况下，hash表的负载因子大于等于5的时候进行扩容。 扩容的大小: 扩容后的dictEntry数组长度为第一个大于等于 ht[0].used * 2 的 2^n 也就是第一个大于等于已使用数量的两倍的2的幂次方。 渐进式rehash: rehash进行期间，每次对字典执行添加、删除、查找或者更新操作的时候，除了执行指定的操作之外，还会顺带将dictht[0] hash表当中在rehashidx索引上的所有键值对进行rehash到dictht[1]当中，当一次rehash工作完成之后，会将rehashidx的值+1。 同时在循环时间事件serverCron当中，会调用rehash相关函数，在1ms的时间内，进行rehash处理，每次仅处理少量的转移任务（100个元素） 随着字典操作的不断执行，最终在某个时间点上，dictht[0]当中所有的键值对都会被rehash到dictht[1]当中，此时将rehashidx属性值设置为-1，表示rehash操作已经完成，将dictht[0]重新赋值dictht[1],接着清空dictht[1]。 2).缩容 减少空间的消耗。 缩容的时机： 负载因子小于0.1的时候，Redis自动开始对Hash表进行缩容操作。 缩容的大小： 缩容后的dictEntry数组长度为第一个大于等于 ht[0].used 的 2^n 也就是第一个大于等于已使用数量的2的幂次方。 9.数据结构-set1).当存储的元素都是整数值，且数据量不大时使用inset地址：Redis内部数据结构详解(7)——intset 结构定义 编码格式有3种，为了节省内存，会根据插入数据的大小选择不一样的类型来存储。 length记录保存数据的数组contents中共有多少个元素。 contents数组，真正存储数据的地方，数组时按照从小到大有序排列，且不包含任何重复项 优点：节省内存 缺点\u0010：升级过程中消耗资源；不支持降级 2).其他时候使用dict，结构等同于hash中的dict地址：Redis内部数据结构详解(1)——dict 10.inset整数升级过程假设一开始4个元素，1、2、3、4，依次存储在contents当中。每个元素占据了16位 11.数据结构-zset1).ziplist，元素数量小于128个 所有元素的长度都小于64字节的时候 使用该数据结构。地址：Redis内部数据结构详解(4)——ziplist 以上两个条件可以通过Redis配置文件 zset-max-ziplist-entries 选项和 zset-max-ziplist-value 进行修改 2).其他时候，由跳表和字典组成的数据结构地址：Redis内部数据结构详解(1)——dict地址：Redis内部数据结构详解(6)——skiplist 字典结构来存储value和score的对应关系，跳表提供按照score排序的功能 zskiplist是skiplist的数据结构 跳表结构的更新： 当插入一个数据的时候，根据score找到插入的位置，然后随机计算出结点的层数，插入，再修改结点间的引用关系。 随机层数的伪代码： p为0.25，MaxLevel为32 12.Redis持久化1).RDB快照(snapshot) Redis DataBase缩写快找。Redis中的默认的持久化机制 按照一定的时间将内存的数据以快找的形式保存到磁盘中，对应产生的数据文件为dump.rdb。通过配置文件中的save参数来定义快照的周期。 N 秒内数据集至少有 M 个改动 例如 save 60 1000 表示 60秒至少有1000个键被改动 则自动保存一次 优点: a.只有一个文件dump.rdb，方便持久化 b.容灾性好，一个文件可以保存到安全的磁盘 c.性能最大化，fork子进程来完成写操作，让主进程继续处理命令，所以是IO最大化。使用单独子进程来进行持久化，主进程不会进行任何IO操作，保证了redis的高性能 d.相对于数据集大时，比AOF的启动效率更高。 缺点: a.数据安全性低。RDB是间隔一段时间进行持久化，如果持久化之间redis发生故障，会发生数据丢失。 2).AOF(append-only file) 将Redis执行的每次写命令记录到单独的日志文件中，当重启Redis会重新将持久化的日志中文件恢复数据。 优点: a.数据安全 b.AOF rewrite模式。AOF文件没有被rewrite之前（文件过大时会对命令进行合并重写），可以删除其中的某些命令。 c.aof文件是明文的，可阅读性较好 缺点: a.AOF文件比RDB文件大，且恢复速度慢 b.数据集大的时候，比RDB启动效率低 3).当两种方式同时开启的时候，Redis会优先选择AOF恢复 4).混合持久化 Redis4.0新的持久化选项 将rdb文件的内容和增量的AOF日志文件存在一起。这里的AOF日志不再是全量的日志，而是自持久化开始到持久化结束的这段时间发生的增量AOF日志 13.Redis的过期策略有哪些Redis是key-value数据库，我们可以设置Redis中缓存的key的过期时间。Redis的过期策略就是指当Redis中缓存的key过期了 1).定时过期 每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即删除。 该策略可以立即清除过期的数据，对内存很友好，但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。 2).惰性过期 只有当访问一个key的时候，才会判断key是否过期，过期则删除。 该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。 3).定期过期 每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。 通过调整定时扫描器的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。 redis中同时使用了惰性过期和定期过期两种策略。 14.Redis的内存淘汰策略有哪些主动清理策略在Redis 4.0 之前一共实现了 6 种内存淘汰策略，在 4.0 之后，又增加了 2 种策略，总共8种： a) 针对设置了过期时间的key做处理： volatile-ttl：在筛选时，会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除。 volatile-random：就像它的名称一样，在设置了过期时间的键值对中，进行随机删除。 volatile-lru：会使用 LRU 算法筛选设置了过期时间的键值对删除。 volatile-lfu：会使用 LFU 算法筛选设置了过期时间的键值对删除。 b) 针对所有的key做处理： allkeys-random：从所有键值对中随机选择并删除数据。 allkeys-lru：使用 LRU 算法在所有数据中进行筛选删除。 allkeys-lfu：使用 LFU 算法在所有数据中进行筛选删除。 c) 不处理： noeviction：不会剔除任何数据，拒绝所有写入操作并返回客户端错误信息”(error) OOM command not allowed when used memory”，此时Redis只响应读操作。 LRU 算法（Least Recently Used，最近最少使用） 淘汰很久没被访问过的数据，以最近一次访问时间作为参考。 LFU 算法（Least Frequently Used，最不经常使用） 淘汰最近一段时间被访问次数最少的数据，以次数作为参考。 当存在热点数据时，LRU的效率很好，但偶发性的、周期性的批量操作会导致LRU命中率急剧下降，缓存污染情况比较严重。这时使用LFU可能更好点。根据自身业务类型，配置好maxmemory-policy(默认是noeviction)，推荐使用volatile-lru。如果不设置最大内存，当 Redis 内存超出物理内存限制时，内存的数据会开始和磁盘产生频繁的交换 (swap)，会让 Redis 的性能急剧下降。当Redis运行在主从模式时，只有主结点才会执行过期删除策略，然后把删除操作”del key”同步到从结点删除数据。 15.缓存雪崩、缓存击穿、缓存穿透及解决方案 缓存雪崩：缓存同一时间大面积失效，后面请求都落到数据库上，造成大量请求直接打到数据库。 过期时间随机，防止同一时间大量数据过期 缓存预热：项目启动加载缓存到redis 互斥锁：修改的时候只允许一个线程修改数据库，其他读写线程等待 加相应缓存标记，记录缓存是否失效，如果失效，更新缓存(内存消耗大，一般不用) 缓存穿透：指数据库没有数据，导致请求落到数据库上 接口层增加校验，对id进行规则拦截 缓存取不到数据，设置null值到缓存，设置短时间超时，防止网络攻击 布隆过滤器，所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据就会被这个bitmap拦截 缓存击穿：缓存没有，数据库中有数据（一般是缓存时间到期），并发用户特别多，同时大量请求落到数据库，缓存击穿指并发查询同一条数据，缓存雪崩是不同数据都过期 key不过期 加互斥锁 16.大key按照一定规则拆分大key，比如一个key中放入了全量类型的配置，可以拆成若干个不同类型的key，大key变小。 17.热key集群模式下，把key复制若干份，使得不同的key分配在不同的槽中，用户访问的时候，可以根据用户id进行hash操作，使不同用户访问不同的key，但是这些key里的信息都是完全一致的，同时更新缓存的时候也需要注意更新所有的缓存key 18.缓存与数据库数据一致性 解决方案： 对于并发几率很小的数据(如个人维度的订单数据、用户数据等)，这种几乎不用考虑这个问题，很少会发生缓存不一致，可以给缓存数据加上过期时间，每隔一段时间触发读的主动更新即可。 就算并发很高，如果业务上能容忍短时间的缓存数据不一致(如商品名称，商品分类菜单等)，缓存加上过期时间依然可以解决大部分业务对于缓存的要求。 如果不能容忍缓存数据不一致，可以通过加读写锁保证并发读写或写写的时候按顺序排好队，读读的时候相当于无锁。 也可以用阿里开源的canal通过监听数据库的binlog日志及时的去修改缓存，但是引入了新的中间件，增加了系统的复杂度。 19.redis主从模式 单机的 redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。 20.redis主从同步完整的复制流程 21.master节点 run id的作用 22.主从模式全量复制 23.主从模式增量同步 24.redis哨兵模式（sentinal） 为什么redis哨兵集群只有2个节点无法正常工作 经典的3节点哨兵集群 异步复制导致的数据丢失问题 集群脑裂导致的数据丢失问题 解决异步复制和脑裂导致的数据丢失 redis哨兵的选举算法 25.redis集群模式cluster 26.一致性hash3.redis单线程为什么快redis的速度非常的快，单机的redis就可以支撑每秒10几万的并发，相对于mysql来说，性能是mysql的几十倍。速度快的原因主要有几点： 完全基于内存操作 C语言实现，优化过的数据结构，基于几种基础的数据结构，redis做了大量的优化，性能极高 使用单线程，无上下文的切换成本 基于非阻塞的IO多路复用机制 4.redis单线程模型5.那为什么Redis6.0之后又改用多线程呢?redis使用多线程并非是完全摒弃单线程，redis还是使用单线程模型来处理客户端的请求，只是使用多线程来处理数据的读写和协议解析，执行命令还是使用单线程。 这样做的目的是因为redis的性能瓶颈在于网络IO而非CPU，使用多线程能提升IO读写的效率，从而整体提高redis的性能。 10.redis淘汰策略？（同问题定期+惰性都没有删除过期的key怎么办？）11.redis分布式锁、zk分布式锁（普通、有序两种）、redission分布式锁13.redis主从复制原理（全量同步&amp;增量同步）14.redis哨兵模式15.redis集群模式17.redis和数据库数据一致性问题18.BloomFilter过滤器原理19.场景题排行榜（多维度） 签到 redis实现消息队列 一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。 如果对方追问可不可以不用sleep呢？ list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。 如果对方接着追问能不能生产一次消费多次呢？ 使用pub/sub主题订阅者模式，可以实现 1:N 的消息队列。 如果对方继续追问 pub/su b有什么缺点？ 在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如RocketMQ等。 如果对方究极TM追问Redis如何实现延时队列？ 使用sortedset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。 Redis单线程模型Redis基于Reactor模式开发的网络事件处理器，这个文件事件处理器是单线程的，采用IO多路复用机制监听多个Socker，根据Socker上的事件类型选择对应的事件处理器处理这个事件，可以实现高性能的网络通信。文件事件处理器包含4个部分，多个Socker，IO多路复用程序，文件事件分派器和事件处理器(命令请求处理器，命令回复处理器，链接应答处理器)，多个Socket可能并发产生不同操作，每个操作对应不同的文件事件，但是IO多路复用会监听多个Socket，会将Socket放入一个队列，每次从队列中取出一个Socket给时间分派器，时间分派器把Socket给对应的事件处理器。 单线程快的原因：1.纯内存操作；2.核心是基于非阻塞的IO多路复用机制；3.单线程反而避免多线程的频繁上下文切换 分布式锁怎么实现的？ 基于数据库实现分布式锁：唯一索引，状态机唯一联合索引 基于缓存（Redis等）实现分布式锁：SET key value NX PX 30000 基于Zookeeper实现分布式锁； setnx用到的参数SET key value NX PX 30000 第三个参数：把key、value set到redis中的策略 nx ： not exists, 只有key 不存在时才把key value set 到redis xx ： is exists ，只有 key 存在是，才把key value set 到redis 第四个参数：过期时间单位 ex ：seconds 秒 px : milliseconds 毫秒 使用其他值，抛出 异常 ： redis.clients.jedis.exceptions.JedisDataException : ERR syntax error 第五个参数：有两种可选的值， int 和long 的time，都是过期时间 ，expx 参数是px的时候，使用long类型的参数，可以表示更多时间 - Redis过期键删除策略 1.","categories":[{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://zhangxin66666.github.io/tags/redis/"}]},{"title":"JVM面试","slug":"3.jvm/面试-JVM","date":"2024-12-22T16:00:00.000Z","updated":"2024-12-24T08:20:42.398Z","comments":true,"path":"2024/12/23/3.jvm/面试-JVM/","link":"","permalink":"https://zhangxin66666.github.io/2024/12/23/3.jvm/%E9%9D%A2%E8%AF%95-JVM/","excerpt":"","text":"1.jvm内存模型 堆：堆Java虚拟机中最大的一块内存，是线程共享的内存区域，基本上所有的对象实例数组都是在堆上分配空间。堆区细分为Yound区年轻代和Old区老年代，其中年轻代又分为Eden、S0、S1 3个部分，他们默认的比例是8:1:1的大小。 栈：栈是线程私有的内存区域，每个方法执行的时候都会在栈创建一个栈帧，方法的调用过程就对应着栈的入栈和出栈的过程。每个栈帧的结构又包含局部变量表、操作数栈、动态连接、方法返回地址。 局部变量表用于存储方法参数和局部变量。当第一个方法被调用的时候，他的参数会被传递至从0开始的连续的局部变量表中。 操作数栈用于一些字节码指令从局部变量表中传递至操作数栈，也用来准备方法调用的参数以及接收方法返回结果。 动态连接用于将符号引用表示的方法转换为实际方法的直接引用。 元数据：在Java1.7之前，包含方法区的概念，常量池就存在于方法区（永久代）中，而方法区本身是一个逻辑上的概念，在1.7之后则是把常量池移到了堆内，1.8之后移出了永久代的概念(方法区的概念仍然保留)，实现方式则是现在的元数据。它包含类的元信息和运行时常量池。 Class文件就是类和接口的定义信息。 运行时常量池就是类和接口的常量池运行时的表现形式。 本地方法栈：主要用于执行本地native方法的区域 程序计数器：也是线程私有的区域，用于记录当前线程下虚拟机正在执行的字节码的指令地址 PC 寄存器用来存储指向下一条指令的地址，即将要执行的指令代码。由执行引擎读取下一条指令。 2.PC寄存器为什么会被设定为线程私有的？多线程在一个特定的时间段内只会执行其中某一个线程方法，CPU会不停的做任务切换，这样必然会导致经常中断或恢复。为了能够准确的记录各个线程正在执行的当前字节码指令地址，所以为每个线程都分配了一个PC寄存器，每个线程都独立计算，不会互相影响 3.什么是虚拟机栈（线程私有）？主管 Java 程序的运行，它保存方法的局部变量、部分结果，并参与方法的调用和返回。每个线程在创建的时候都会创建一个虚拟机栈，其内部保存一个个的栈帧(Stack Frame），对应着一次次 Java 方法调用，是线程私有的，生命周期和线程一致。 特点？ 栈是一种快速有效的分配存储方式，访问速度仅次于程序计数器 JVM 直接对虚拟机栈的操作只有两个：每个方法执行，伴随着入栈（进栈/压栈），方法执行结束出栈 栈不存在垃圾回收问题 可以通过参数-Xss来设置线程的最大栈空间，栈的大小直接决定了函数调用的最大可达深度 该区域有哪些异常？ 如果采用固定大小的 Java 虚拟机栈，那每个线程的 Java 虚拟机栈容量可以在线程创建的时候独立选定。如果线程请求分配的栈容量超过 Java 虚拟机栈允许的最大容量，Java 虚拟机将会抛出一个 StackOverflowError 异常 如果 Java 虚拟机栈可以动态扩展，并且在尝试扩展的时候无法申请到足够的内存，或者在创建新的线程时没有足够的内存去创建对应的虚拟机栈，那 Java 虚拟机将会抛出一个OutOfMemoryError异常 栈帧的内部结构？ 局部变量表（Local Variables） 操作数栈（Operand Stack）(或称为表达式栈) 动态链接（Dynamic Linking）：指向运行时常量池的方法引用 方法返回地址（Return Address）：方法正常退出或异常退出的地址 一些附加信息 4.什么是本地方法栈（线程私有）？ 本地方法接口 一个 Native Method 就是一个 Java 调用非 Java 代码的接口。我们知道的 Unsafe 类就有很多本地方法。 本地方法栈(Native Method Stack) Java 虚拟机栈用于管理 Java 方法的调用，而本地方法栈用于管理本地方法的调用 5.什么是方法区（线程共享）？方法区（method area）只是 JVM 规范中定义的一个概念，用于存储类信息、常量池、静态变量、JIT编译后的代码等数据，并没有规定如何去实现它，不同的厂商有不同的实现。而**永久代（PermGen）**是 **Hotspot** 虚拟机特有的概念， Java8 的时候又被**元空间**取代了，永久代和元空间都可以理解为方法区的落地实现。 JDK1.8之前调节方法区大小： 12-XX:PermSize=N //方法区（永久代）初始大小-XX:MaxPermSize=N //方法区（永久代）最大大小，超出这个值将会抛出OutOfMemoryError JDK1.8开始方法区（HotSpot的永久代）被彻底删除了，取而代之的是元空间，元空间直接使用的是本机内存。参数设置： 12-XX:MetaspaceSize=N //设置Metaspace的初始（和最小大小）-XX:MaxMetaspaceSize=N //设置Metaspace的最大大小 6.栈、堆、方法区的交互关系 7.堆区内存是怎么细分的？ 对于大多数应用，Java 堆是 Java 虚拟机管理的内存中最大的一块，被所有线程共享。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数据都在这里分配内存。 为了进行高效的垃圾回收，虚拟机把堆内存逻辑上划分成三块区域（分代的唯一理由就是优化 GC 性能）： 新生带（年轻代）：新对象和没达到一定年龄的对象都在新生代 老年代（养老区）：被长时间使用的对象，老年代的内存空间应该要比年轻代更大 Java 虚拟机规范规定，Java 堆可以是处于物理上不连续的内存空间中，只要逻辑上是连续的即可，像磁盘空间一样。实现时，既可以是固定大小，也可以是可扩展的，主流虚拟机都是可扩展的（通过 -Xmx 和 -Xms 控制），如果堆中没有完成实例分配，并且堆无法再扩展时，就会抛出 OutOfMemoryError 异常。 年轻代 (Young Generation) 年轻代是所有新对象创建的地方。当填充年轻代时，执行垃圾收集。这种垃圾收集称为 Minor GC。年轻一代被分为三个部分——伊甸园（Eden Memory）和两个幸存区（Survivor Memory，被称为from/to或s0/s1），默认比例是8:1:1 大多数新创建的对象都位于 Eden 内存空间中 当 Eden 空间被对象填充时，执行Minor GC，并将所有幸存者对象移动到一个幸存者空间中 Minor GC 检查幸存者对象，并将它们移动到另一个幸存者空间。所以每次，一个幸存者空间总是空的 经过多次 GC 循环后存活下来的对象被移动到老年代。通常，这是通过设置年轻一代对象的年龄阈值来实现的，然后他们才有资格提升到老一代 老年代(Old Generation) 旧的一代内存包含那些经过许多轮小型 GC 后仍然存活的对象。通常，垃圾收集是在老年代内存满时执行的。老年代垃圾收集称为 主GC（Major GC），通常需要更长的时间。 大对象直接进入老年代（大对象是指需要大量连续内存空间的对象）。这样做的目的是避免在 Eden 区和两个Survivor 区之间发生大量的内存拷贝 8.JVM中对象在堆中的生命周期? 在 JVM 内存模型的堆中，堆被划分为新生代和老年代 新生代又被进一步划分为 Eden区 和 Survivor区，Survivor 区由 From Survivor 和 To Survivor 组成 当创建一个对象时，对象会被优先分配到新生代的 Eden 区 此时 JVM 会给对象定义一个对象年轻计数器（-XX:MaxTenuringThreshold） 当 Eden 空间不足时，JVM 将执行新生代的垃圾回收（Minor GC） JVM 会把存活的对象转移到 Survivor 中，并且对象年龄 +1 对象在 Survivor 中同样也会经历 Minor GC，每经历一次 Minor GC，对象年龄都会+1 如果分配的对象超过了-XX:PetenureSizeThreshold，对象会直接被分配到老年代 9.JVM中对象的分配过程?为对象分配内存是一件非常严谨和复杂的任务，JVM 的设计者们不仅需要考虑内存如何分配、在哪里分配等问题，并且由于内存分配算法和内存回收算法密切相关，所以还需要考虑 GC 执行完内存回收后是否会在内存空间中产生内存碎片。 new 的对象先放在伊甸园区，此区有大小限制 当伊甸园的空间填满时，程序又需要创建对象，JVM 的垃圾回收器将对伊甸园区进行垃圾回收（Minor GC），将伊甸园区中的不再被其他对象所引用的对象进行销毁。再加载新的对象放到伊甸园区 然后将伊甸园中的剩余对象移动到幸存者 0 区 如果再次触发垃圾回收，此时上次幸存下来的放到幸存者 0 区，如果没有回收，就会放到幸存者 1 区 如果再次经历垃圾回收，此时会重新放回幸存者 0 区，接着再去幸存者 1 区 什么时候才会去养老区呢？ 默认是 15 次回收标记 在养老区，相对悠闲。当养老区内存不足时，再次触发 Major GC，进行养老区的内存清理 若养老区执行了 Major GC 之后发现依然无法进行对象的保存，就会产生 OOM 异常 10.静态变量与局部变量的对比变量按照数据类型：基本数据类型和引用数据类型 变量按照类中声明位置：成员变量（类变量 ｓｔａｔｉｃ ，局部变量） 局部变量 成员变量 在使用前都经过默认赋值，ｉｎｔ显式赋值。 实例变量 随着对象创建会在堆空间中分配实例变量空间，并进行默认赋值。 局部变量显式赋值，否则没法使用。 在栈帧中，与性能调优最为密切的就是局部变量表，在方法执行时，虚拟机使用局部变量表完成方法的传递。 在局部变量表中的变量也是重要的垃圾回收根节点，只要被局部变量表中直接引用或间接引用的对象都不会被回收。 11.方法重写的本质找到操作数栈顶的第一个元素所执行的对象的实际类型，记做Ｃ。 如果在类型Ｃ中找到与常量中的描述符和简单名称都相符的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用，查找过程结束；如果不通过则返回java.lang.IllegalAccessError。 否则，按照继承关系从下往上依次对C的各个父类进行第二步的搜索和验证过程。 如果始终没有找到合适的方法，则抛出java.lang.AbstractMethodError异常。 java.lang.IllegalAccessError 程序试图访问或者修改一个属性或调用一个方法，这个属性或者方法，你没有权限访问，一般的，这个会引起编译期异常，这个错误 如果发生在运行时，就说明一个类发生了不兼容的改变。 补充：多态的本质就是指向同一个虚方法表的引用。 12.对象创建过程 类加载检查：虚拟机遇到一条new指令时，首先将去检查这个类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程：类加载过程中会通过jvm自带的三个类加载器按照双亲委派机制进行类加载过程，我们常见的ClassNotFoundException就是在这里发生的，我们代码中的静态代码块里面的内容也是在这个过程中执行的。 分配内存：在类加载检查通过后，虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务等同于把 一块确定大小的内存从Java堆中划分出来。 初始化零值：内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 设置对象头：初始化零值之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息(即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例)、对象的哈希码（HashCode）、对象的GC分代年龄、锁状态标志等信息。这些信息存放在对象的对象头Object Header之中。 执行方法：执行方法，即对象按照程序员的意愿进行初始化。对应到语言层面上讲，就是为属性赋值（注意，这与上面的赋零值不同，这是由程序员赋的值），和执行构造方法。 13.对象分配过程1.new的对象先放在伊甸园区，此区有大小限制。 2.当伊甸园的空间填满时，程序有需要创建对象，JVM的垃圾回收器将对伊甸园区进行垃圾回收，将伊甸园区的不在被其他对象所引用的对象销毁，在加载新的对象放在伊甸园区。 3.然后将伊甸园区剩余的对象移动到幸存者0. 4.如果再次触发垃圾回收，此时上次幸存下来的放到幸存者0区的，接着再去幸存者1区。 5.啥时候去养老区呢？可以设置次数，默认是15次。-XX:MaxTenuringThreshold= (伊甸园区满了会触发ygc，将幸存者区域和伊甸园区都回收一下，但是幸存者区满了不会触发垃圾回收) 总结：针对幸存者S0，S1区的总结：复制之后有交换，谁空谁是to。关于垃圾回收，频繁在新生区收集，很少在养老区收集，几乎不在永久区或者元空间收集。 14.类的加载过程 加载-&gt;链接（验证，准备，解析）-&gt;初始化 1）加载 通过全限定类名加载一个类的二进制字节流，将静态结构转化为方法区的运行时数据结构，在内存中生成一个代表这个类的Class对象 加载.class文件的方式： 从本地系统中直接加载 通过网络获取，典型场景：Web Applet 从zip压缩包中读取，成为日后jar ，war格式的基础 运行时计算生成，使用最多的是：动态代理技术 其他文件生成：典型场景JSP 从专有数据库提取.class文件，比较少见 从加密文件中获取，典型的防Class文件被反编译的保护措施 2）链接 1.验证：确保class文件内容不会危害到当前虚拟机 2.准备：为类变量分配内存并设置初始值，不会为实例变量分配空间初始化，类变量分配在方法区，实例变量分配在堆空间。 3.解析：将常量池的符号引用转换为直接引用 3）初始化 执行类构造器方法（完成静态属性和静态代码块变量的赋值操作）的过程，此方法不需要定义，是javac完成的。 clinit()不同于类的构造器,他只会加载一次。若该类具有父类，JVM会保证子类的clinit()执行前，父类的clinit()已经执行完毕。 虚拟机必须保证一个类的clinit()方法在多线程下被同步加锁。 任何一个类声明以后，内部至少存在一个类的构造器 15.类加载器 JDK自带有三个类加载器：bootstrapClassLoader(引导类加载器)、ExtClassLoader(扩展类加载器)、AppClassLoader(系统类加载器)，bootstrapClassLoader是ExtClassLoader的父类加载器(并不是集成关系，是属性关系)默认加载%JAVA_HOME%/lib下的jar包和class文件ExtClassLoader是AppClassLoader的父类加载器，默认加载%JAVA_HOME%/lib/ext文件夹下的jar包和class文件AppClassLoader是自定义类加载器，负责加载classpath下的类文件 1）启动类加载器 启动类加载器 BootStrap ClassLoader 加载java核心类库，只加载java javax sun开头的类 2）系统类加载器 extends ClassLoader java ClassLoader.getSystemClassLoader(); 加载用户自定义类，父类加载器为扩展类加载器 3）拓展类加载器 extends ClassLoader java SystemClassLoader.getParent(); 4）用户自定义类加载器 extends ClassLoader 1.为什么要用户自定义类加载器？ 1隔离加载类` `修改类加载的方式` `扩展加载源` `防止源码泄露 2.用户自定义类加载器步骤？ jdk1.2之前，继承ClassLoader重写loadClass().jdk1.2之后建议重写findClass() 也可以继承URLClassLoader,避免了自己编写findClass()以及获取自己码流的方式。 3.ClassLoader 它是一个抽象类，其后所有的类加载器都继承自ClassLoader(不包括启动类加载器) 1234getParent() 返回父类加载器loadClass(String name) 返回Class实例findClass(String name) 返回Class实例defineClass(String name,byte[] b ,int off,int len) 把字节数组b中的内容转换为一个java类，返回结果为java.lang.Class类的实例 16.JVM类加载机制有哪些？ 全盘负责，当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入 父类委托，先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类 缓存机制，缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效 双亲委派机制, 如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。 17.双亲委派机制分为向上委派和向下查找，每一个类加载器都有各自的缓存，都会记录自己加载过的类，当一个类需要加载，AppClassLoader先查找自己的缓存有没有加载过这个类，没有加载过就会调用ExtClassLoader去查找，ExtClassLoader没有加载过，就会调用bootstrapClassLoader类去加载，如果bootstrapClassLoader没有加载过，就会去他的类加载路径查找，如果没有找到ExtClassLoader就会查找他的类加载路径，向上委派就是查找缓存，查找到bootstrapClassLoader位置，向下查找就是查找类加载路径，查找到发起的类加载器为止。 java虚拟机对class文件采用的是按需加载，而且加载某个类的class文件时，java虚拟机采用的是双亲委派机制，就是把请求交由父类加载器处理，它是一种任务委派模式 1）工作原理 如果一个类加载器收到了类加载请求，他并不会自己先去加载，而是把这个请求委托给父类的加载器去执行。 如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将达到顶层的启动类加载器。 如果父加载器可以完成类加载任务，就成功返回，倘若父加载器无法完成此类加载任务，子加载器才会尝试自己去加载。 2）举例 1.调用JDBC接口，接口是引导类加载器加载的，但是实现类是系统类加载器加载的。 2.假设当前加载的是java.lang.Object这个类，很显然，该类属于JDK中核心得不能再核心的一个类，因此一定只能由引导类加载器进行加载。当JVM准备加载java.lang.Object时，JVM默认会使用系统类加载器去加载，按照上面4步加载的逻辑，在第1步从系统类的缓存中肯定查找不到该类，于是进入第2步。由于从系统类加载器的父加载器是扩展类加载器，于是扩展类加载器继续从第1步开始重复。由于扩展类加载器的缓存中也一定查找不到该类，因此进入第2步。扩展类的父加载器是null,因此系统调用findClass(String)，最终通过引导类加载器进行加载。 3）优点 1、安全性，避免自己写的类替换掉java核心类；保护程序安全，防止核心API被随意篡改 2、避免类重复加载，相同class文件不同的类加载器加载的也是两个类。 4）双亲委托模式的弊端 检查类是否加载的委托过程是单向的，这个方式虽然从结构上说比较清晰，使各个ClassLoader的职责非常明确，但是同时会带来一个问题，即顶层的ClassLoader无法访问底层的ClassLoader所加载的类。 通常情况下，启动类加载器中的类为系统核心类，包括一些重要的系统接口，而在应用类加载器中，为应用类。按照这种模式，应用类访问系统类自然是没有问题，但是系统类访问应用类就会出现问题。比如在系统类中提供了一个接口，该接口需要在应用类中得以实现，该接口还绑定一个工厂方法，用于创建该接口的实例，而接口和工厂方法都在启动类加载器中。这时，就会出现该工厂方法无法创建由应用类加载器加载的应用实例的问题。 5）思考 如果在自定义的类加载器中重写java.lang.ClassLoader.loadClass(String)或java.lang.ClassLoader.loadClass(String, boolean)方法,抹去其中的双亲委派机制,仅保留上面这4步中的第1步与第4步，那么是不是就能够加载核心类库了呢? 这也不行!因为JDK还为核心类库提供了一层保护机制。不管是自定义的类加载器，还是系统类加载器还是扩展类加载器，最终都必须调用java.lang.classLoader.defineClass(String,byte[], int, int, ProtectionDomain)方法，而该方法会执行preDefineClass()接口，该接口中提供了对JDK核心类库的保护。 6）特殊情况: 由于Java虚拟机规范并没有明确要求类加载器的加载机制一定要使用双亲委派模型，只是建议采用这种方式而己。 比如在Tomcat中，类加载器所采用的加载机制就和传统的双亲委派模型有一定区别，当缺省的类加载器接收到一个类的加载任务时，首先会由它自行加载，当它加载失败时，才会将类的加载任务委派给它的超类加载器去执行，这同时也是Servlet规范推荐的一种做法。 18.破坏双亲委派机制①破坏双亲委派机制1 双亲委派模型并不是一个具有强制性约束的模型，而是Java设计者推荐给开发者们的类加载器实现方式。 在Java的世界中大部分的类加载器都遵循这个模型，但也有例外的情况，直到Java模块化出现为止，双亲委派模型主要出现过3次较大规模“被破坏”的情况。 第一次破坏双亲委派机制: 双亲委派模型的第一次“被破坏”其实发生在双亲委派模型出现之前–即JDK1.2面世以前的“远古”时代。 由于双亲委派模型在JDK 1.2之后才被引入，但是类加载器的概念和抽象类java.lang.ClassLoader则在Java的第一个版本中就已经存在，面对已经存在的用户自定义类加载器的代码，Java设计者们引入双亲委派模型时不得不做出一些妥协，为了兼容这些已有代码，无法再以技术手段避免loadClass()被子类覆盖的可能性，只能在JDK1.2之后的java.lang.ClassLoader中添加一个新的protected方法findClass()，并引导用户编写的类加载逻辑时尽可能去重写这个方法，而不是在loadClass()中编写代码。上节我们已经分析过loadClass()方法，双亲委派的具体逻辑就实现在这里面，按照loadClass()方法的逻辑，如果父类加载失败，会自动调用自己的findClass()方法来完成加载，这样既不影响用户按照自己的意愿去加载类，又可以保证新写出来的类加载器是符合双亲委派规则的。 不仅要继承ClassLoader类，还要重写loadClass和findClass方法 ②破坏双亲委派机制2 第二次破坏双亲委派机制:线程上下文类加载器 双亲委派模型的第二次“被破坏”是由这个模型自身的缺陷导致的，双亲委派很好地解决了各个类加载器协作时基础类型的一致性问题〈越基础的类由越上层的加载器进行加载），基础类型之所以被称为“基础”，是因为它们总是作为被用户代码继承、调用的API存在，但程序设计往往没有绝对不变的完美规则，如果有基础类型又要调用回用户的代码,那该怎么办呢? 这并非是不可能出现的事情，一个典型的例子便是JNDI服务，JNDI现在已经是Java的标准服务，它的代码由启动类加载器来完成加载（在JDK 1.3时加入到rt.jar的)，肯定属于Java中很基础的类型了。但JNDI存在的目的就是对资源进行查找和集中管理，它需要调用由其他厂商实现并部署在应用程序的ClassPath下的NDI服务提供者接口（Service Provider Interface，SPI）的代码，现在问题来了，启动类加载器是绝不可能认识、加载这些代码的，那该怎么办?(SPI:在Java平台中，通常把核心类rt.jar中提供外部服务、可由应用层自行实现的接口称为SPI) 为了解决这个困境，Java的设计团队只好引入了一个不太优雅的设计:线程上下文类加载器（Thread ContextClassLoader)。这个类加载器可以通过java.lang.Thread类的setContextClassLoader()方法进行设置，如果创建线程时还未设置，它将会从父线程中继承一个，如果在应用程序的全局范围内都没有设置过的话，那这个类加载器默认就是应用程序类加载器。 有了线程上下文类加载器，程序就可以做一些“舞弊”的事情了。JNDI服务使用这个线程上下文类加载器去加载所需的SPI服务代码，这是一种父类加载器去请求子类加载器完成类加载的行为，这种行为实际上是打通了双亲委派模型的层次结构来逆向使用类加载器，已经违背了双亲委派模型的一般性原则，但也是无可奈何的事情。Java中涉及SPI的加载基本上都采用这种方式来完成，例如NDI、JDBC、JCE、JAXB和BT等。不过，当SPI的服务提供者多于一个的时候，代码就只能根据具体提供者的类型来硬编码判断，为了消除这种极不优雅的实现方式，在JDK 6时，JDK提供了java.util.ServiceLoader类，以META-INF/services中的配置信息，辅以责任链模式，这才算是给SPI的加载提供了一种相对合理的解决方案。 默认上下文加载器就是应用类加载器，这样以上下文加载器为中介，使得启动类加载器中的代码也可以访问应用类加载器中的类。 ③破坏双亲委派机制3 第三次破坏双亲委派机制: 双亲委派模型的第三次“被破坏”是由于用户对程序动态性的追求而导致的。如:代码热替换（Hot Swap)、模块热部署（Hot Deployment）等 IBM公司主导的JSR-291(即OSGi R4.2）实现模块化热部署的关键是它自定义的类加载器机制的实现，每一个程序模块（OSGi中称为Bundle)都有一个自己的类加载器，当需要更换一个Bundle时，就把Bundle连同类加载器一起换掉以实现代码的热替换。在OSGi环境下，类加载器不再双亲委派模型推荐的树状结构，而是进一步发展为更加复杂的网状结构。 当收到类加载请求时，OSGi将按照下面的顺序进行类搜索:（不细讲） 1）将以java.*开头的类，委派给父类加载器加载。 2）否则，将委派列表名单内的类，委派给父类加载器加载。 3）否则，将Import列表中的类，委派给Export这个类的Bundle的类加载器加载。 4）否则，查找当前Bundle的ClassPath，使用自己的类加载器加载。 5）否则，查找类是否在自己的Fragment Bundle中，如果在，则委派给Fragment Bundle的类加载器加载。 6）否则，查找Dynamic Import列表的Bundle，委派给对应Bundle的类加载器加载。 7）否则，类查找失败。 说明:只有开头两点仍然符合双亲委派模型的原则，其余的类查找都是在平级的类加载器中进行的 小结: 这里，我们使用了“被破坏”这个词来形容上述不符合双亲委派模型原则的行为，但这里“被破坏”并不一定是带有贬义的。只要有明确的目的和充分的理由，突破旧有原则无疑是一种创新。 正如:OSGi中的类加载器的设计不符合传统的双亲委派的类加载器架构，且业界对其为了实现热部署而带来的额外的高复杂度还存在不少争议，但对这方面有了解的技术人员基本还是能达成一个共识，认为OSGi中对类加载器的运用是值得学习的，完全弄懂了OSGi的实现，就算是掌握了类加载器的精粹。 19.热替换的实现热替换是指在程序的运行过程中，不停止服务，只通过替换程序文件来修改程序的行为。热替换的关键需求在于服务不能中断，修改必须立即表现正在运行的系统之中。基本上大部分脚本语言都是天生支持热替换的，比如: PHP，只要替换了PHP源文件，这种改动就会立即生效，而无需重启Web服务器。 但对Java来说，热替换并非天生就支持，如果一个类已经加载到系统中，通过修改类文件，并无法让系统再来加载并重新定义这个类。因此，在Java中实现这一功能的一个可行的方法就是灵活运用ClassLoader。 注意:由不同ClassLoader加载的同名类属于不同的类型，不能相互转换和兼容。即两个不同的ClassLoader加载同个类，在虚拟机内部，会认为这2个类是完全不同的。 根据这个特点，可以用来模拟热替换的实现，基本思路如下图所示: 20.Class.forName()和ClassLoader.loadClass()区别? Class.forName(): 将类的.class文件加载到jvm中之外，还会对类进行解释，执行类中的static块； ClassLoader.loadClass(): 只干一件事情，就是将.class文件加载到jvm中，不会执行static中的内容,只有在newInstance才会去执行static块。 Class.forName(name, initialize, loader)带参函数也可控制是否加载static块。并且只有调用了newInstance()方法采用调用构造函数，创建类的对象 。 21.什么是 TLAB （Thread Local Allocation Buffer）? 从内存模型而不是垃圾回收的角度，对 Eden 区域继续进行划分，JVM 为每个线程分配了一个私有缓存区域，它包含在 Eden 空间内 多线程同时分配内存时，使用 TLAB 可以避免一系列的非线程安全问题，同时还能提升内存分配的吞吐量，因此我们可以将这种内存分配方式称为快速分配策略 OpenJDK 衍生出来的 JVM 大都提供了 TLAB 设计 22.为什么要有 TLAB ? 堆区是线程共享的，任何线程都可以访问到堆区中的共享数据 由于对象实例的创建在 JVM 中非常频繁，因此在并发环境下从堆区中划分内存空间是线程不安全的 为避免多个线程操作同一地址，需要使用加锁等机制，进而影响分配速度 尽管不是所有的对象实例都能够在 TLAB 中成功分配内存，但 JVM 确实是将 TLAB 作为内存分配的首选。 在程序中，可以通过 -XX:UseTLAB 设置是否开启 TLAB 空间。 默认情况下，TLAB 空间的内存非常小，仅占有整个 Eden 空间的 1%，我们可以通过 -XX:TLABWasteTargetPercent 设置 TLAB 空间所占用 Eden 空间的百分比大小。 一旦对象在 TLAB 空间分配内存失败时，JVM 就会尝试着通过使用加锁机制确保数据操作的原子性，从而直接在 Eden 空间中分配内存。 23.ClassLoader源码解析ClassLoader 与现有类加载的关系： 除了以上虚拟机自带的加载器外，用户还可以定制自己的类加载器。Java 提供了抽象类 java.lang.ClassLoader，所有用户自定义的类加载器都应该继承 ClassLoader 类。 1）ClassLoader的主要方法public final ClassLoader getParent()返回该类加载器的超类加载器 public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException加载名称为name的类，返回结果为java.lang.Class类的实例。如果找不到类，则返回ClassNotFoundException异常。 该方法中的逻辑就是双亲委派模式的实现。 protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException查找二进制名称为name的类，返回结果为java.lang.Class类的实例。这是一个受保护的方法，JVM鼓励我们重写此方法，需要自定义加载器遵循双亲委托机制，该方法会在检查完父类加载器之后被loadClass()方法调用。 在JDK1.2之前，在自定义类加载时，总会去继承ClassLoader类并重写loadClass方法，从而实现自定义的类加载类。但是在JDK1.2之后已不再建议用户去覆盖loadClass()方法，而是建议把自定义的类加载逻辑写在findClass()方法中，从前面的分析可知， findClass()方法是在loadClass()方法中被调用的，当loadClass()方法中父加载器加载失败后，则会调用自己的findClass()方法来完成类加载，这样就可以保证自定义的类加载器也符合双亲委托模式。需要注意的是ClassLoader类中并没有实现findClass()方法的具体代码逻辑，取而代之的是抛出ClassNotFoundException异常，同时应该知道的是findClass方法通常是和defineClass方法一起使用的。一般情况下，在自定义类加载器时，会直接覆盖ClassLoader的findClass()方法并编写加载规则，取得要加载类的字节码后转换成流，然后调用defineClass()方法生成类的Class对象。 protected final Class&lt;?&gt; defineClass(byte[] b, int off, int len)根据给定的字节数组b转换为Class的实例，off和len参数表示实际Class信息在byte数组中的位置和长度，其中byte数组b是ClassLoader从外部获取的。这是受保护的方法，只有在自定义ClassLoader子类中可以使用。 defineClass()方法是用来将byte字节流解析成JVM能够识别的Class对象(ClassLoader中已实现该方法逻辑)，通过这个方法不仅能够通过class文件实例化class对象，也可以通过其他方式实例化class对象，如通过网络接收一个类的字节码，然后转换为byte字节流创建对应的Class对象。 defineClass()方法通常与findClass()方法一起使用，一般情况下，在自定义类加载器时，会直接覆盖ClassLoader的findClass()方法并编写加载规则，取得要加载类的字节码后转换成流，然后调用defineClass()方法生成类的Class对象。 123456789101112131415/** * 编写findClass方法的逻辑 */ @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; // 获取类的class文件字节数组 byte[] classData = getClassData(name); if (classData == null) &#123; throw new ClassNotFoundException(); &#125; else &#123; //直接生成class对象 return defineClass(name, classData, 0, classData.length); &#125; &#125; protected final void resolveClass(Class&lt;?&gt; c)链接指定的一个Java类。使用该方法可以使用类的Class对象创建完成的同时也被解析(即加载的同时也进行解析)。前面我们说链接阶段主要是对字节码进行验证，为类变量分配内存并设置初始值同时将字节码文件中的符号引用转换为直接引用。 protected final Class&lt;?&gt; findLoadedClass(String name)查找名称为name的已经被加载过的类，返回结果为java.lang.Class类的实例。这个方法是final方法，无法被修改。 private final ClassLoader parent它也是一个ClassLoader的实例，这个字段所表示的ClassLoader也称为这个ClassLoader的双亲。在类加载的过程中,ClassLoader可能会将某些请求交予自己的双亲处理。 ①loadClass()的剖析测试代码 1ClassLoader.getSystemClassLoader().loadClass(&quot;com.atguig.java.User&quot;); 涉及到对如下方法的调用 123456789101112131415161718192021222324252627282930313233343536373839404142protected Class&lt;?&gt; loadClass(String name, boolean resolve)//resolve:true-加载class的同时进行解析操作。 throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; //同步操作，保证只能加载一次。 // First, check if the class has already been loaded //首先，在缓存中判断是否已经加载同名的类 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; //获取当前类加载器的父类加载器。 if (parent != null) &#123; //如果存在父类加载器，则调用父类加载器进行类的加载（递归） c = parent.loadClass(name, false); &#125; else &#123; //parent为null:父类加载器是引导类加教器 c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123;//当前类的加载器的父类加载器未加载此类or此类的加载器未加载此类 // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); //调用当前ClassLoader的findClass() c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; //是否进行解析操作 resolveClass(c); &#125; return c; &#125;&#125; 2）SecureClassLoader与URLClassLoader接着SecureClassLoader扩展了ClassLoader，新增了几个与使用相关的代码源(对代码源的位置及其证书的验证)和权限定义类验证(主要指对class源码的访问权限)的方法，一般我们不会直接跟这个类打交道，更多是与它的子类URLClassLoader有所关联。 前面说过，ClassLoader是一个抽象类，很多方法是空的没有实现，比如 findClass()、findResource()等。而URLClassLoader这个实现类为这些方法提供了具体的实现。并新增了URLClassPath类协助取得Class字节码流等功能。在编写自定义类加载器时，如果没有太过于复杂的需求，可以直接继承URLClassLoader类，这样就可以避免自己去编写findClass()方法及其获取字节码流的方式，使自定义类加载器编写更加简洁。 3）ExtClassLoader与AppClassLoader了解完URLClassLoader后接着看看剩余的两个类加载器，即拓展类加载器ExtClassLoader和系统类加载AppClassLoader，这两个类都继承自URLClassLoader，是sun.misc.Launcher的静态内部类。 sun.misc.Launcher主要被系统用于启动主应用程序，ExtClassLoader和AppClassLoader都是由sun.misc.Launcher创建的，其类主要类结构如下: 我们发现ExtClassLoader并没有重写loadClass()方法，这足矣说明其遵循双亲委派模式，而AppClassLoader重载了loadclass()方法，但最终调用的还是父类loadClass()方法，因此依然遵守双亲委派模式。 4）Class.forName与ClassLoader.loadClass()Class.forName():是一个静态方法,最常用的是Class.forName(String className);根据传入的类的全限定名返回一个Class对象。该方法在将Class文件加载到内存的同时,会执行类的初始化。 如:Class.forName( &quot;com.atguigu.java.Helloworld&quot;) ; 12ClassLoader.loadClass()`:这是一个实例方法,需要一个`ClassLoader`对象来调用该方法。该方法将class文件加载到内存时,并不会执行类的初始化,直到这个类第一次使用时才进行初始化。该方法因为需要得到个`ClassLoader`对象,所以可以根据需要指定使用哪个类加载器.如: `ClassLoader cl=......;cl.loadClass (&quot;com.atguigu.java.Helloworld&quot; ); 24.类加载器Java9新特性为了保证兼容性，JDK 9没有从根本上改变三层类加载器架构和双亲委派模型，但为了模块化系统的顺利运行，仍然发生了一些值得被注意的变动。 1.扩展机制被移除，扩展类加载器由于向后兼容性的原因被保留，不过被重命名为平台类加载器（platform classloader)。可以通过ClassLoader的新方法getPlatformClassLoader()来获取。 JDK 9时基于模块化进行构建（原来的 rt.jar 和 tools.jar 被拆分成数十个 JMOD 文件)，其中的Java类库就已天然地满足了可扩展的需求，那自然无须再保留\\lib\\ext 目录，此前使用这个目录或者 java.ext.dirs系统变量来扩展JDK功能的机制已经没有继续存在的价值了。 2.平台类加载器和应用程序类加载器都不再继承自 java.net.URLClassLoader。 现在启动类加载器、平台类加载器、应用程序类加载器全都继承于jdk.internal.loader.BuiltinClassLoader。 如果有程序直接依赖了这种继承关系，或者依赖了URLClassLoader类的特定方法，那代码很可能会在 JDK9及更高版本的JDK中崩溃。 3.在Java 9中，类加载器有了名称。该名称在构造方法中指定，可以通过getName()方法来获取。平台类加载器的名称是platform，应用类加载器的名称是app。类加载器的名称在调试与类加载器相关的问题时会非常有用。 4.启动类加载器现在是在jvm内部和java类库共同协作实现的类加载器〈以前是C++实现)，但为了与之前代码兼容，在获取启动类加载器的场景中仍然会返回null，而不会得到BootClassLoader实例。 5.类加载的委派关系也发生了变动。 当平台及应用程序类加载器收到类加载请求，在委派给父加载器加载前，要先判断该类是否能够归属到某一个系统模块中，如果可以找到这样的归属关系，就要优先委派给负责那个模块的加载器完成加载。 在Java模块化系统明确规定了三个类加载器负责各自加载的模块。 25.两种类装载方式Java中的所有类，都需要由类加载器装载到JVM中才能运行。类加载器本身也是一个类，而它的工作就是把class文件从硬盘读取到内存中。在写程序的时候，我们几乎不需要关心类的加载，因为这些都是隐式装载的，除非我们有特殊的用法，像是反射，就需要显式的加载所需要的类。 类装载方式，有两种 ： 1.隐式装载， 程序在运行过程中当碰到通过new 等方式生成对象时，隐式调用类装载器加载对应的类到jvm中， 2.显式装载， 通过class.forname()等方法，显式加载需要的类 Java类的加载是动态的，它并不会一次性将所有类全部加载后再运行，而是保证程序运行的基础类(像是基类)完全加载到jvm中，至于其他类，则在需要的时候才加载。这当然就是为了节省内存开销。 26.深拷贝和浅拷贝浅拷贝（shallowCopy）只是增加了一个指针指向已存在的内存地址， 深拷贝（deepCopy）是增加了一个指针并且申请了一个新的内存，使这个增加的指针指向这个新的内存， 使用深拷贝的情况下，释放内存的时候不会因为出现浅拷贝时释放同一个内存的错误。 浅复制：仅仅是指向被复制的内存地址，如果原地址发生改变，那么浅复制出来的对象也会相应的改变。 深复制：在计算机中开辟一块新的内存地址用于存放复制的对象。 27.对象创建的几种方式 28.为对象分配内存两种方式类加载完成后，接着会在Java堆中划分一块内存分配给对象。内存分配根据Java堆是否规整，有两种方式： 指针碰撞：如果Java堆的内存是规整，即所有用过的内存放在一边，而空闲的的放在另一边。分配内存时将位于中间的指针指示器向空闲的内存移动一段与对象大小相等的距离，这样便完成分配内存工作。 空闲列表：如果Java堆的内存不是规整的，则需要由虚拟机维护一个列表来记录那些内存是可用的，这样在分配的时候可以从列表中查询到足够大的内存分配给对象，并在分配后更新列表记录。 选择哪种分配方式是由 Java 堆是否规整来决定的，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 29.对象的创建过程中处理并发安全问题对象的创建在虚拟机中是一个非常频繁的行为，哪怕只是修改一个指针所指向的位置，在并发情况下也是不安全的，可能出现正在给对象 A 分配内存，指针还没来得及修改，对象 B 又同时使用了原来的指针来分配内存的情况。解决这个问题有两种方案： 对分配内存空间的动作进行同步处理（采用 CAS + 失败重试来保障更新操作的原子性）； 把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在 Java 堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer, TLAB）。哪个线程要分配内存，就在哪个线程的 TLAB 上分配。只有 TLAB 用完并分配新的 TLAB 时，才需要同步锁。通过-XX:+/-UserTLAB参数来设定虚拟机是否使用TLAB。 30.介绍一下强引用、软引用、弱引用、虚引用的区别？1）强引用 我们平时new了一个对象就是强引用，例如 Object obj = new Object(); 即使在内存不足的情况下，JVM宁愿抛出OutOfMemory错误也不会回收这种对象。 2）软引用 如果一个对象只具有软引用，则内存空间足够，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。 1SoftReference&lt;String&gt; softRef=new SoftReference&lt;String&gt;(str); // 软引用 用处： 软引用在实际中有重要的应用，例如浏览器的后退按钮。按后退时，这个后退时显示的网页内容是重新进行请求还是从缓存中取出呢？这就要看具体的实现策略了。 （1）如果一个网页在浏览结束时就进行内容的回收，则按后退查看前面浏览过的页面时，需要重新构建 （2）如果将浏览过的网页存储到内存中会造成内存的大量浪费，甚至会造成内存溢出 如下代码： 12345678Browser prev = new Browser(); // 获取页面进行浏览SoftReference sr = new SoftReference(prev); // 浏览完毕后置为软引用 if(sr.get()!=null)&#123; rev = (Browser) sr.get(); // 还没有被回收器回收，直接获取&#125;else&#123; prev = new Browser(); // 由于内存吃紧，所以对软引用的对象回收了 sr = new SoftReference(prev); // 重新构建&#125; 3）弱引用 具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。 123456String str=new String(&quot;abc&quot;); WeakReference&lt;String&gt; abcWeakRef = new WeakReference&lt;String&gt;(str);str=null;等价于str = null;System.gc(); 4）虚引用 如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。虚引用主要用来跟踪对象被垃圾回收器回收的活动。 31.怎么判断对象是否可以被回收？垃圾收集器在做垃圾回收的时候，首先需要判定的就是哪些内存是需要被回收的，哪些对象是「存活」的，是不可以被回收的；哪些对象已经「死掉」了，需要被回收。 一般有两种方法来判断： 引用计数器法：为每个对象创建一个引用计数，有对象引用时计数器 +1，引用被释放时计数 -1，当计数器为 0 时就可以被回收。它有一个缺点不能解决循环引用的问题； 可达性分析算法：从 GC Roots 开始向下搜索，搜索所走过的路径称为引用链。当一个对象到 GC Roots 没有任何引用链相连时，则证明此对象是可以被回收的。 32.在Java中，对象什么时候可以被垃圾回收当对象对当前使用这个对象的应用程序变得不可触及的时候，这个对象就可以被回收了。垃圾回收不会发生在永久代，如果永久代满了或者是超过了临界值，会触发完全垃圾回收(Full GC)。如果你仔细查看垃圾收集器的输出信息，就会发现永久代也是被回收的。这就是为什么正确的永久代大小对避免Full GC是非常重要的原因。 33.JVM 运行时堆内存如何分代?Java 堆从 GC 的角度还可以细分为: 新生代(Eden 区、 From Survivor 区和 To Survivor 区)和老年代。 从图中可以看出： 堆大小 = 新生代 + 老年代。其中，堆的大小可以通过参数 –Xms、-Xmx 来指定。 默认的，新生代 ( Young ) 与老年代 ( Old ) 的比例的值为 1:2 ( 该值可以通过参数 –XX:NewRatio 来指定 )， 即：新生代 ( Young ) = 1/3 的堆空间大小。老年代 ( Old ) = 2/3 的堆空间大小。 其中，新生代 ( Young ) 被细分为 Eden 和 两个 Survivor 区域，这两个 Survivor 区域分别被命名为 from 和 to，以示区分 默认的，Eden: from : to = 8 :1 : 1 ( 可以通过参数–XX:SurvivorRatio 来设定 )，即： Eden = 8/10 的新生代空间大小，from = to = 1/10 的新生代空间大小。 JVM 每次只会使用 Eden 和其中的一块 Survivor 区域来为对象服务，所以无论什么时候，总是有一块Survivor区域是空闲着的。 因此，新生代实际可用的内存空间为 9/10 ( 即90% )的新生代空间。 新生代 是用来存放新生的对象。一般占据堆的 1/3 空间。由于频繁创建对象，所以新生代会频繁触发MinorGC 进行垃圾回收。新生代又分为 Eden区、 ServivorFrom、 ServivorTo 三个区。Eden 区Java 新对象的出生地（如果新创建的对象占用内存很大，则直接分配到老年代）。当 Eden 区内存不够的时候就会触发 MinorGC，对新生代区进行一次垃圾回收。Servivor from 区上一次 GC 的幸存者，作为这一次 GC 的被扫描者。Servivor to 区保留了一次 MinorGC 过程中的幸存者。MinorGC 的过程（复制-&gt;清空-&gt;互换）MinorGC 采用复制算法。 eden、 servicorFrom 复制到 ServicorTo，年龄+1首先，把 Eden 和 ServivorFrom 区域中存活的对象复制到 ServicorTo 区域（如果有对象的年龄以及达到了老年的标准，则赋值到老年代区），同时把这些对象的年龄+1（如果 ServicorTo 不够位置了就放到老年区）； 清空 eden、 servicorFrom然后，清空 Eden 和 ServicorFrom 中的对象； ServicorTo 和 ServicorFrom 互换最后， ServicorTo 和 ServicorFrom 互换，原 ServicorTo 成为下一次 GC 时的 ServicorFrom区。 老年代 主要存放应用程序中生命周期长的内存对象。老年代的对象比较稳定，所以 MajorGC （常常称之为 FULL GC）不会频繁执行。在进行 FULL GC前一般都先进行了一次 MinorGC，使得有新生代的对象晋身入老年代，导致空间不够用时才触发。当无法找到足够大的连续空间分配给新创建的较大对象时也会提前触发一次 MajorGC 进行垃圾回收腾出空间。FULL GC 采用标记清除算法：首先扫描一次所有老年代，标记出存活的对象，然后回收没有标记的对象。 ajorGC 的耗时比较长，因为要扫描再回收。 FULL GC 会产生内存碎片，为了减少内存损耗，我们一般需要进行合并或者标记出来方便下次直接分配。当老年代也满了装不下的时候，就会抛出 OOM（Out of Memory）异常。 永久代 指内存的永久保存区域，主要存放 Class 和 Meta（元数据）的信息,Class 在被加载的时候被放入永久区域， 它和和存放实例的区域不同,GC 不会在主程序运行期对永久区域进行清理。所以这也导致了永久代的区域会随着加载的 Class 的增多而胀满，最终抛出 OOM 异常。 34.JVM内存为什么要分成新生代，老年代，持久代。新生代中为什么要分为Eden和Survivor这样划分的目的是为了使 JVM 能够更好的管理堆内存中的对象，包括内存的分配以及回收。 1）共享内存区划分 共享内存区 = 持久带 + 堆 持久带 = 方法区 + 其他 Java堆 = 老年代 + 新生代 新生代 = Eden + S0 + S1 2）一些参数的配置 默认的，新生代 ( Young ) 与老年代 ( Old ) 的比例的值为 1:2 ，可以通过参数 –XX:NewRatio 配置。 默认的，Eden : from : to = 8 : 1 : 1 ( 可以通过参数 –XX:SurvivorRatio 来设定) Survivor区中的对象被复制次数为15(对应虚拟机参数 -XX:+MaxTenuringThreshold) 35.为什么要分为Eden和Survivor?为什么要设置两个Survivor区？ 如果没有Survivor，Eden区每进行一次Minor GC，存活的对象就会被送到老年代。老年代很快被填满，触发Major GC.老年代的内存空间远大于新生代，进行一次Full GC消耗的时间比Minor GC长得多,所以需要分为Eden和Survivor。 Survivor的存在意义，就是减少被送到老年代的对象，进而减少Full GC的发生，Survivor的预筛选保证，只有经历16次Minor GC还能在新生代中存活的对象，才会被送到老年代。 设置两个Survivor区最大的好处就是解决了碎片化，刚刚新建的对象在Eden中，经历一次Minor GC，Eden中的存活对象就会被移动到第一块survivor space S0，Eden被清空；等Eden区再满了，就再触发一次Minor GC，Eden和S0中的存活对象又会被复制送入第二块survivor space S1（这个过程非常重要，因为这种复制算法保证了S1中来自S0和Eden两部分的存活对象占用连续的内存空间，避免了碎片化的发生） 36.JVM中一次完整的GC流程是怎样的，对象如何晋升到老年代先描述一下Java堆内存划分，再解释Minor GC，Major GC，full GC，描述它们之间转化流程。 Java堆 = 老年代 + 新生代 新生代 = Eden + S0 + S1 当 Eden 区的空间满了， Java虚拟机会触发一次 Minor GC，以收集新生代的垃圾，存活下来的对象，则会转移到 Survivor区。 大对象（需要大量连续内存空间的Java对象，如那种很长的字符串）直接进入老年态； 如果对象在Eden出生，并经过第一次Minor GC后仍然存活，并且被Survivor容纳的话，年龄设为1，每熬过一次Minor GC，年龄+1，若年龄超过一定限制（15），则被晋升到老年态。即长期存活的对象进入老年态。 老年代满了而无法容纳更多的对象，Minor GC 之后通常就会进行Full GC，Full GC 清理整个内存堆 – 包括年轻代和年老代。 Major GC 发生在老年代的GC，清理老年区，经常会伴随至少一次Minor GC，比Minor GC慢10倍以上。 37.JVM中的永久代中会发生垃圾回收吗垃圾回收不会发生在永久代，如果永久代满了或者是超过了临界值，会触发完全垃圾回收(Full GC)。如果你仔细查看垃圾收集器的输出信息，就会发现永久代也是被回收的。这就是为什么正确的永久代大小对避免Full GC是非常重要的原因。请参考下Java8：从永久代到元数据区(译者注：Java8中已经移除了永久代，新加了一个叫做元数据区的native内存区) 38.JAVA8 与元数据在 Java8 中， 永久代已经被移除，被一个称为“元数据区”（元空间）的区域所取代。元空间的本质和永久代类似，元空间与永久代之间最大的区别在于： 元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制。 类的元数据放入native memory, 字符串池和类的静态变量放入 java 堆中， 这样可以加载多少类的元数据就不再由MaxPermSize 控制, 而由系统的实际可用空间来控制 39.如何判断对象可以被回收？判断对象是否存活一般有两种方式： 引用计数： 每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。 可达性分析（Reachability Analysis）： 从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的，不可达对象。 40.引用计数法在 Java 中，引用和对象是有关联的。如果要操作对象则必须用引用进行。因此，很显然一个简单的办法是通过引用计数来判断一个对象是否可以回收。简单说，即一个对象如果没有任何与之关联的引用， 即他们的引用计数都不为 0， 则说明对象不太可能再被用到，那么这个对象就是可回收对象。 41.可达性分析为了解决引用计数法的循环引用问题， Java 使用了可达性分析的方法。通过一系列的“GC roots”对象作为起点搜索。如果在“GC roots”和一个对象之间没有可达路径，则称该对象是不可达的。要注意的是，不可达对象不等价于可回收对象， 不可达对象变为可回收对象至少要经过两次标记过程。两次标记后仍然是可回收对象，则将面临回收。 42.Minor GC与Full GC分别在什么时候发生？新生代内存不够用时候发生MGC也叫YGC，JVM内存不够的时候发生FGC 43.标记清除算法当成功区分出内存中存活对象和死亡对象后，GC接下来的任务是执行垃圾回收，释放掉无用对象所占用的内存空间，以便有足够的可用内存空间为新对象分配内存。目前在JVM中比较常见的三种垃圾收集算法是标记-清除算法，复制算法，标记-压缩算法。 当堆中的有效内存空间被耗尽时，就会停止整个程序，然后进行两项工作，第一项是标记，第二项则是清除。 标记：从引用跟节点开始遍历，标记所有被引用的对象。一般是在对象的Header中记录为可达对象。 清除：对堆内存从头到尾进行线性遍历，如果发现某个对象在其header中没有标记为可达对象，则将其回收。 优点：不需要进行对象的移动，并且仅对不存活的对象进行处理，存活对象比较多的情况下极为高效。 缺点：标记和清除过程的效率都不高，这种方法需要使用一个空闲列表来记录所有的空闲区域以及大小，对空闲列表的管理会增加分配对象时的工作量；标记清除后会产生大量不连续的内存碎片，虽然空闲区域的大小是足够的，但却可能没有一个单一的区域能满足这次分配所需大小，分配还会失败，不得不触发再一次的垃圾回收。 何为清除：所谓清除，并不是并不是真的置空，而是把需要清除的对象地址保存在空闲的地址列表里，下次有新的对象需要加载时，判断垃圾的位置空间是够够，够就存放。 44.标记整理算法算法标记的过程与标记清除算法中的标记过程一样，但是对标记后出的垃圾对象的处理情况有所不同，他不是直接对可回收对象进行清理，而是让所有的对象都像一端移动，然后直接清理掉端边界以外的内存。在基于标记整理算法的收集齐实现中，一般增加句柄和句柄表。 优点：经过整理之后，新对象的分配只需要通过指针碰撞便能完成，比较简单；使用这种方法，空闲区域的位置是始终可知的，也不会再有碎片的问题了。 缺点：ＧＣ暂停的时间会增长，因为你需要将所有的对象都拷贝到一个新的地方，还得更新他们的引用地址。 45.复制算法复制算法主要是为了克服句柄的开销和解决堆碎片的垃圾回收。它将内存按照容量分为大小相等的两块，每次只使用其中的一块，当这一块的内存用完了，就将还活着的对象复制到另一块内存上面（空闲面），然后再把已使用过的内存空间一次清理掉。 复制算法比较适合于新生代（短生存期的对象），在老年代（长生存期的对象）中，对象存活率比较高，如果执行较多的复制操作，效率将会变低。所以老年代一般会选用其他算法，如标记整理算法。一种典型的基于复制算法的垃圾回收是stop-and-copy算法，他将堆分为对象区和空闲区，在对象区与空闲区的切换过程中，程序暂停执行。 优点：标记阶段和复制阶段可以同时进行，每次只对一块内存进行回收，运行高效，只需要移动栈顶指针，按顺序分配内存即可，实现简单，内存回收时，不用考虑内存碎片的出现。 缺点：需要一块能容纳下所有存活对象的额外的内存空间。因此，可一次性分配的最大内存缩小了一半。 46.分代收集算法将堆内存划分为新生代，老年代和永久代。新生代又被进一步划分为伊甸园区和幸存者0，幸存者1区。所有通过new创建的对象的内存都在堆中分配，其大小可以通过-Xms和-Xmx来控制。分代收集，是基于这样一个事实：不同的对象的生命周期是不一样的。因此可以将不同生命周期的对象分代，不同的代采取不同的回收算法进行垃圾回收，以便提高回收效率。 新生代：几乎所有新生成的对象首先都是放在年轻代的。新生代内存按照 8:1:1 的比例分为一个 Eden 区和两个 Survivor（Survivor0，Survivor1）区。大部分对象在 Eden 区中生成。当新对象生成，Eden 空间申请失败（因为空间不足等），则会发起一次 GC（Scavenge GC）。回收时先将 Eden 区存活对象复制到一个 Survivor0 区，然后清空 Eden 区，当这个 Survivor0 区也存放满了时，则将 Eden 区和 Survivor0 区存活对象复制到另一个 Survivor1 区，然后清空 Eden 和这个 Survivor0 区，此时 Survivor0 区是空的，然后将 Survivor0 区和 Survivor1 区交换，即保持 Survivor1 区为空， 如此往复。当 Survivor1 区不足以存放 Eden 和 Survivor0 的存活对象时，就将存活对象直接存放到老年代。当对象在 Survivor 区躲过一次 GC 的话，其对象年龄便会加 1，默认情况下，如果对象年龄达到 15 岁，就会移动到老年代中。若是老年代也满了就会触发一次 Full GC，也就是新生代、老年代都进行回收。新生代大小可以由-Xmn来控制，也可以用-XX:SurvivorRatio来控制 Eden 和 Survivor 的比例。 老年代：在新生代中经历了 N 次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。内存比新生代也大很多（大概比例是 1:2），当老年代内存满时触发 Major GC 即 Full GC，Full GC 发生频率比较低，老年代对象存活时间比较长，存活率高。一般来说，大对象会被直接分配到老年代。所谓的大对象是指需要大量连续存储空间的对象，最常见的一种大对象就是大数组。当然分配的规则并不是百分之百固定的，这要取决于当前使用的是哪种垃圾收集器组合和 JVM 的相关参数。 永久代：用于存放静态文件（class类、方法）和常量等。永久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如 Hibernate 等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。对永久代的回收主要回收两部分内容：废弃常量和无用的类。永久代在 Java SE8 特性中已经被移除了，取而代之的是元空间（MetaSpace），因此也不会再出现java.lang.OutOfMemoryError: PermGen error的错误了。 特别的，在分代收集算法中，对象的存储具有以下特点： 1.对象优先在伊甸园区分配 2.大对象直接进入老年代 3.长期存活的对象将进入老年代，默认为15岁 对于晋升老年代的年龄阈值，为什么是15岁？ 实际上，HotSpot虚拟机的对象头其中一部分用于存储对象自身的运行时数据，如哈希码，GC分代年龄，锁状态标志，线程持有的锁，偏向线程ID，偏向时间戳等，这部分数据的长度在32位和64位的虚拟机中分别为32bit和64bit，官方称为mark word。在 32 位的 HotSpot 虚拟机中，如果对象处于未被锁定的状态下，那么Mark Word的 32bit 空间中 25bit 用于存储对象哈希码，4bit 用于存储对象分代年龄，2bit 用于存储锁标志位，1bit 固定为 0，其中对象的分代年龄占 4 位，也就是从0000到1111，而其值最大为 15，所以分代年龄也就不可能超过 15 这个数值了。 GC的分类 新生代GC Minor GC ：发生在新生代的垃圾收集动作，因为java对象大多具有朝生夕灭的特性，因此MinorGC非常频繁，一般回收速度也比较快。在新生代中，每次垃圾收集时都会发现有大量对象死去，只有少量存活，因此可以选用复制算法。 老年代GC Major GC：发生在老年代的垃圾回收动作。Major GC 经常会伴随至少一次Minor GC。由于老年代中的对象的生命周期比较长，因此Major GC并不频繁，一般都是等待老年代满了之后才进行Full GC，而且其速度一般会比Minor GC慢10倍以上。另外，如果分配了Direct Memory，在老年代中进行 Full GC 时，会顺便清理掉 Direct Memory 中的废弃对象。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清除”算法或“标记-整理”算法来进行回收。新生代采用空闲指针的方式来控制 GC 触发，指针保持最后一个分配的对象在新生代区间的位置，当有新的对象要分配内存时，用于检查空间是否足够，不够就触发 GC。当连续分配对象时，对象会逐渐从 Eden 到 Survivor，最后到老年代。 47.增量收集算法上述现有的算法，在垃圾回收过程中，应用软件将处于一种STW的状态，在该状态下，应用程序所有的线程都会挂起，暂停一切正常的工作，等待垃圾回收完成。如果垃圾回收时间过长，应用程序会被挂起很久，将严重影响用户体验或者系统稳定性。为了解决这个问题，即对实时垃圾收集算法的亚久直接导致增量收集算法的产生。 如果一次性将所有的垃圾进行处理，需要造成系统长时间的停顿，那么就可以让垃圾收集线程 和应用程序线程交替执行。每次，垃圾收集线程值收集一小片区域的内存空间，接着切换到应用程序线程，依次反复，一直到垃圾收集完成。 总的来说，增量收集算法的基础仍然是传统的标记-清除和复制算法。增量收集算法通过对线程间冲突的妥善处理，允许垃圾收集线程以分阶段的方式完成标记，清理或复制工作。 缺点：线程切换和上下文的转换消耗性能，会使得垃圾回收的总体成本上升，造成系统吞吐量的下降。 48.分区算法分代算法将按照对象的生命周期长短划分成两个部分，分区算法将整个堆空间划分成连续的不同的小空间。每一个小空间都独立使用，独立回收。这种算法的好处是可以控制一次回收多少个小区间。 55.垃圾判断算法１）引用计数法 在这种算法中，假设堆中每个对象都有一个引用计数器。当一个对象被创建并且初始化赋值以后，对象的计数器就会设置为１，每当有一个地方引用他，计数器的值就会＋１，例如将对象Ｂ赋值给对象Ａ，那么Ｂ被引用，Ｂ的引用计数器就会＋１. 反之，当引用失效的时候，比如一个对象的某个引用被设置了新的值，则之前被引用的对象的计数器就会－１.而那些引用计数为０的对象，就可以称之为垃圾，可以被收集。 特别的，当一个对象被当做垃圾收集时，他引用的任何对象的计数器的值都－１. 优点：实现简单，对程序不被长时间打断的实时环境比较有利 缺点：需要额外的空间来存储计数器，难以检测对象之间的循环依赖 java并没有选择引用计数，是因为其存在一个基本难题，也就是很难处理循环引用关系。 Python如何解决循环引用？手动解决（在合适的时机，接触引用关系）使用弱引用weakref（weakref是Python提供的标准库，为了解决循环依赖） ２）可达性分析算法 可达性是指，如果一个对象会被至少一个在程序中的变量通过直接或间接的方式被其他可达的对象引用，则称该对象就是可达的。 对象成为可达对象的两个条件： 对象属于跟集中的对象 对象被一个可达的对象引用 56.在java语言中，GC ROOTS 包括以下几类元素：虚拟机栈中引用的对象（各个线程被调用的方法中使用到的参数，局部变量等） 本地方法栈内JNI（通常说的本地方法）引用的对象 方法区中类静态属性引用的对象（java类的引用类型静态变量） 方法区中常量引用的对象（字符串常量池（String Table）里的引用） 所有被同步锁synchronized持有的对象 java虚拟机内部的引用（基本数据类型对应的class对象，一些常驻的异常对象，系统类加载器） 本地代码缓存 临时的（分代收集，局部回收） 如何判断一个root 由于root采用栈方式存放变量和指针，所以如果一个指针，他保存了堆内存里面的对象，但是自己又不存放在堆内存里面，那他就是一个root。 如果要使用可达性分析算法来判断内存是否可回收，那么分析工作必须在一个能保障一致性的快照中进行。这点不满足的话分析结果的准确性就无法保证。这点也是导致GC进行时必须Stop the World的一个重要原因。 优点：可以解决循环引用的问题，不需要占用额外的空间 缺点：多线程场景下，其他线程可能会更新已经访问过的对象的引用。 57.Minor GC,Major GC,fULL GCjvm在进行GC时，并非每次都对上面三个内存区域一起回收的，大部分回收都是指新生代。 针对hotSpotVM的实现，它里面的GC按照回收区域分为两大类型：一种是部分收集，一种是FullGC 1.部分收集：不是完整收集整个Java堆的垃圾回收，又分为： 1）新生代收集Minor GC：只是新生代的垃圾收集 2）老年代收集Major GC：只是老年代的垃圾收集 目前只有CMSGC拥有单独收集老年代的行为，很多时候Minor GC会和FullGC混淆使用，需要具体分辨老年代回收还是整堆回收。 3）混合收集：收集整个新生代以及部分老年代的垃圾 2.整堆收集：收集整个java堆和方法区的垃圾 58.分代式GC策略触发条件1）年轻代触发机制 1.当年轻代空间不足时，就会出发minorGC，这里的年轻代指的是Eden满，Survivor满不会触发GC 2.因为java对象大多数存活时间比较短暂，所以MinorGC非常频繁，一般回收速度也比较快。 3.MinorGC会引发STW，暂停其他用户的线程，等垃圾回收结束，用户线程才恢复运行。 2）老年代GC触发机制 1.指的是发生在老年代的GC，对象从老年代消失时，我们说的MajorGC和FullGC发生了。 2.出现了MajorGC，经常会伴随至少一次的MinorGC，也就是在老年代空间不足的时候，会先尝试触发minorGC，如果之后空间还是不足，则会触发MajorGC。 3.MajorGC的速度一般会比MinorGC慢10倍以上，STW时间更长。 4.如果MajorGC后，内存还是不足，就会OOM。 3）FullGC触发条件 1.System.gc() 不是一定执行的。 2.老年代空间不足 3.方法区空间不足 4.通过Minor GC后进入老年代的平均大小大于老年代的可用内存。 5.Eden区，survivor0 区向survivor1区复制时，对象大小大于To Space可用区域，则把该对象转存到老年代，且老年代的可用内存小于该对象的内存就会触发GC。 Full GC是开发或者调优中尽量要避免的，这样暂停时间会短一些。 59.从逃逸分析角度分析对象内存分配1）堆是分配对象存储的唯一选择嘛？ 在java虚拟机中，对象在java堆中分配内存的，这是一个普遍的常识。但是有一个特殊的情况，那就是如果经过逃逸分析后发现，一个对象并没有逃逸出方法的话，那么就可能优化成栈上分配。这样就无需再堆上分配内存，也无须进行垃圾回收了。这是最常见的堆外存储技术。 2）逃逸分析 1.如果将堆上的对象分配到栈，需要使用逃逸分析手段。 2.这是一种可以有效减少java程序中同步负载和内存堆分配压力的跨函数全局数据流分析算法。 3.通过逃逸分析，hotspot编译器能够分析出一个新的对象的引用的适用范围从而决定是否要将这个对象分配到堆上。 4.逃逸分析的基本行为就是分析对象动态作用域： 当一个对象在方法中被定义后，对象只在方法内部使用，则认为没有发生逃逸。 当一个对象在方法中被定义后，它被外部方法所引用，则认为发生逃逸。例如作为调用参数传递到其他地方中。 5.没有发生逃逸的对象，则可以分配到栈上，随着方法的执行结束，栈空间就被移除，栈空间指向堆空间对象的引用就没了，等到年轻代GC，对象就会被回收。 3）补充 其实主要就是解决了循环引用，没有逃逸分析，这个对象可能一直不被回收，但是有了逃逸分析，栈帧销毁，这个对象就是垃圾了。 60.使用逃逸分析堆代码进行优化1）栈上分配 将堆分配转换为栈分配，如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配。 JIT编译器在编译期间根据逃逸分析的结果，发现如果一个对象并没有逃逸出方法的话，就可能被优化成栈上分配。分配完成后，继续在调用栈内执行，最后线程结束后，栈空间被回收，局部变量对象也被回收。这样就无需进行垃圾回收了。 常见的栈上分配场景：在逃逸分析中，已经说明了。分别是给成员变量赋值，方法返回值，实例引用传递。 1-XX:+DoEscapeAnalysis //默认开启 2）同步省略 如果一个对象被发现只能从一个线程被访问到，那么这个对象的操作可以不考虑同步。线程同步的代价是相当高的，同步的后果是降低并发和性能。在动态编译同步代码块的时候，jit编译器可以借助逃逸分析来判断同步块所使用的锁的对象是否只能被一个线程访问而没有被发布到其他线程，如果没有，那么JIT编译器在编译这个同步块的时候就会取消对这部分代码的同步。这样就能大大提高并发性能，这个取消的性能就叫做同步省略，也叫锁消除。 3）分离对象或标量替换 有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分可以不存储在内存，而是存储在CPU寄存器中 1.标量是指一个无法在分解成更小的数据的数据，Java中的原始数据类型就是标量。相对的，那些还可以分解的数据叫做聚合量，java中的对象就是聚合量，因为他可以分解成其他聚合量和标量。在JIT阶段，如果经过逃逸分析，发现一个对象不会被外界访问的话，那么经过JIT优化，就会把这个对象拆解成若干个其中包含的若干个成员变量来代替。这个过程就是标量替换。 2.标量替换的参数设置 参数-XX：+EliminateAllocations开启了标量替换，允许将对象打散分配在栈上。 3.逃逸分析技术并不成熟，其根本原因就是无法保证逃逸分析的性能消耗一定高于他的消耗。虽然经过逃逸分析可以做标量替换，栈上分配和锁消除。但是逃逸分析自身也是需要进行一系列复杂的分析的，这其实也是一个相对耗时的过程，虽然并不成熟，但是他也是即时编译器优化技术中一个十分重要的手段。 通过逃逸分析，jvm会在栈上分配那些不会逃逸的对象，这在理论上是可行的，但是取决于jvm设计者的选择。HotSpot虚拟机中并未这么做，所以可以明确所有的对象实力都是创建在堆上。 intern字符串的缓存和静态变量曾经都被分配到永久带上，而永久代已经被元数据区取代。但是，intern字符串缓存和静态变量并不是被转移到元数据区，而是直接在堆上分配 ，所以这一点同样符合前面的一点结论：对象实例都是分配在堆上。 61.垃圾回收分类按照垃圾回收线程分：串行垃圾回收器和并行垃圾回收器 串行回收是指在同一时间段内只允许有一个cpu执行垃圾回收操作，此时工作线程被暂停，直至垃圾回收结束。 并行则是允许运用多个cpu同时执行垃圾回收操作。 按照工作模式分：并发式垃圾回收器和独占式垃圾回收器 并发式垃圾回收器与应用程序线程交替，尽可能减少应用程序暂停时间。 独占式垃圾回收器一旦运行，就禁止应用程序中的所有用户线程，直到垃圾回收过程完全结束。 按照碎片处理方式分类，压缩垃圾回收器和非压缩垃圾回收器 压缩式垃圾回收器会在回收完成后，对存活对象进行压缩整理，消除回收后的碎片。 非压缩式的垃圾回收器不进行这步操作。 按照工作的内存区间分：又可分为年轻代的垃圾回收器和老年代的垃圾回收器。 62.评估GC的性能指标吞吐量：运行用户代码的时间占总运行时间的比例。（总运行时间=程序运行时间+垃圾回收时间） 暂停时间：执行垃圾回收时，程序的工作线程被暂停的时间。 收集频率：相对于应用程序的执行，收集发生的频率。 内存占用：java堆区所占内存大小。 三者总体表现会随着技术进步越来越好，主要抓住两点：吞吐量和暂停时间。 如果以吞吐量优先，那么必然需要降低内存回收的执行效率，但是这样会导致GC需要更长的时间来执行内存回收。如果选择低延迟优先，为了降低每次内存回收时的暂停时间，也只能频繁的执行内存回收，但又引起了年轻代内存的缩减和导致程序吞吐量的下降。 标准：在最大吞吐量优先的情况下，降低停顿时间。 63.不同的垃圾回收器概述1）垃圾收集器发展历史 1JDK1.3发布Serial GC 他是第一款GC。ParNew是Serial 的多线程版本。 1JDK1.4发布Parallel GC 和 Concurrent Mark Sweep GC。 1JDK6之后Parallel GC称为HotSpot默认垃圾回收器。 1JDK1.7引入G1。 1JDK9把G1变为默认垃圾收集器，替代CMS。 1JDK10中G1垃圾收集器的并行完整垃圾回收，实现并行性来改善最坏情况下的延迟。 1JDK11引入Epsilon GC。同时引入ZGC。 1JDK12增强G1，自动返回未使用堆内存给操作系统，同时引入Shenandoah GC。 1JDK13增强ZGC。 1JDK14删除CMS，并拓展ZGC的平台兼容性。 2）垃圾收集器分类 串行回收器：Serial，Serial Old 并行回收器：ParNew，Parallel Scavenge，Parallel Old 并发回收器：CMS,Gl 新生代收集器：Serial，ParNew，Parallel Scavenge 老年代收集器：Serial Old，Parallel Old，CMS 整堆收集器：G1 为什么要有很多收集器？ 因为java使用场景很多，移动端，服务端等。所以就需要针对不同的场景，提供不同的垃圾回收器，提高垃圾收集的性能。 对垃圾收集器进行比较只是对具体应用场景选择最合适的收集器。 如何查看默认的垃圾收集器？ 12345-XX:+PrintCommandLineFlags 查看命令行相关参数（包含垃圾收集器）使用命令行指令：jinfo -flag 相关垃圾回收器参数 进程IDParallel Scavenge 和 Parallel Old 64.Serial 回收器（串行回收器）jdk1.3之前回收新生代的唯一选择（HotSpot在Client模式下的默认新生代垃圾收集器） Serial 收集器采用复制算法，串行回收和STW机制的方式执行内存回收。 除了年轻代，Serial收集器还提供用于执行老年代垃圾回收的Serial Old收集器。Serial Old 收集器同样也采用了串行回收和STW机制，只不过内存回收算法使用的是标记-压缩算法。 Serial Old是运行在Client模式下默认的老年代垃圾回收器。 Serial Old在Server模式下主要有两个用途：1.与新生代的Parallel Scavenge配合使用 2.作为老年代CMS收集器的后备垃圾收集方案。 这个收集器是一个单线程的收集器，但他的单线程的意义并不仅仅说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到他收集结束。 优势：简单而高效，对于限定单个CPU的环境来讲，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。（运行在Client模式下的虚拟机是个不错的选择） 在HotSpot虚拟机中，使用-XX:UseSerialGC 参数可以指定年轻代和老年代都是用串行收集器。（等价于新生代使用Serial GC，老年代使用Serial Old GC），一般在java web程序中是不会使用这种垃圾回收器的。 65.ParNew回收器（并行回收）如果说Serial GC是年轻代中的单线程垃圾收集器，那么ParNew收集器则是Serial收集器的多线程版本。（Par是Parallel的缩写 New只能处理新生代） ParNew收集器除了采用并行回收方式执行内存回收外，两款垃圾收集器之间几乎没有差别，也是STW，也是复制算法。 ParNew是很多JVM运行在Server模式下新生代的默认垃圾收集器。 对于新生代，回收次数频繁，使用并行高效。对于老年代，回收次数少，使用串行方式节省资源。（CPU并行需要切换线程，串行可以省去切换线程的资源） 由于ParNew收集器是并行回收，那么是否可以断定在任何场景下他的回收效率都比Serial收集器更高效？ 1.多核系统下，充分利用系统资源，可以快速完成垃圾收集，提升程序吞吐量。 2.但是单个CPU下，他不一定有Serial收集器更高效。避免了线程切换。 除了Serial 外，目前只有ParNew 能与CMS收集器配合工作。 在程序中，开发人员可以通过选项-XX:UseParNewGC手动指定使用ParNew收集器执行内存回收任务 。他表示年轻代使用并行收集器并不影响老年代。 -XX:ParallelGCThreads限制线程数量，默认开启和CPU数相同的线程数。 66.Parallel Scavenge 回收器（吞吐量优先）1）概述 HotSpot的年轻代除了拥有ParNew收集器是基于并行回收的以外，Parallel Scavenge收集器同样也采用了复制算法，并行回收和STW机制。 那么他的出现是否多此一举呢？ 1.和ParNew收集器不同，Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量，他也被称为吞吐量优先的垃圾收集器。 2.自适应调节策略也是Parallel Scavenge与ParNew一个重要区别。 高吞吐量可以高效率的利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。因此，常见在服务器环境中使用。例如：那些执行批量处理，订单处理，工资支付，科学计算的应用程序。 Parallel 收集器在jdk1.6的时候提供了用于执行老年代垃圾搜集的Parallel Old收集器，用来替代老年代的Serial Old收集器。Parallel Old收集器采用了标记-压缩算法，但是同样也是基于并行回收和STW机制。 在程序吞吐量优先的场景中，Parallel收集器和Parallel Old收集器的组合，在Server模式下的内存回收性能很不错。在Java8中，默认是此垃圾回收器。 2）参数配置 -XX:UseParallelGC 手动指定年轻代使用Parallel并行收集器执行内存回收任务。 -XX:+UseParallelOldGC 手动指定老年代都是使用并行回收器。（分别适用于新生代和老年代。默认是JDK8开启的） -XX:ParallelGCThreads 设置年轻代并行收集器的线程数。一般的最好与CPU数量相等，避免线程数影响收集器性能。（默认情况下，CPU数小于8，ParallelGCThreads的值等于CPU数量，当cpu数大于8个的时候，ParallelGCThreads值=3+[5*cpu数]/8 ） -XX:MaxGCPauseMillis 设置垃圾收集器最大停顿时间（STW的时间，单位是ms） 为了尽可能把停顿时间控制在MaxGCPauseMills以内，收集器在工作时会调整Java堆大小或者其他一些参数。对于用户来讲，停顿时间越短，体验越好。但是在服务器端，我们注重高并发，整体的吞吐量，所以服务端适合Parallel，进行控制。（该参数谨慎使用） -XX:GCTimeRatio 垃圾收集时间占总时间比例（=1/(N+1)）用于衡量吞吐量大小。 -XX:UseAdaptiveSizePolicy 设置Parallel Scavenge收集器具有自适应调节策略 在这种模式下，年轻代的大小，Eden和幸存者的比例，晋升老年代的对象年龄等参数会被自动调整，已经达到在堆大小，吞吐量和停顿时间之间的平衡点。 在手动调优比较困难的场合，可以直接使用这种自适应的方式，仅仅指定虚拟机的最大堆，目标吞吐量和停顿时间，让虚拟机自己完成调优工作。 67.CMS回收器（低延迟）1）概述 JDK1.5，这款收集器是HotSpot虚拟机第一款真正意义上的并发收集器，第一次实现了让垃圾收集线程与用户线程同时工作。 CMS的关注点是尽可能缩短垃圾收集时用户线程的停顿时间。停顿时间越短就越适合于用户交互的程序，良好的响应速度能提升用户体验。 目前很大一部分的java应用集中在互联网或者B/S架构系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停段时间最短，以给用户较好的体验感。 CMS的垃圾收集算法采用标记-清除算法，并且也会STW。 不幸的是，CMS作为老年代的收集器，却无法与JDK1.4中已经存在的新生代收集器Parallel Scavenge 配合工作，所以在JDK1.5中使用CMS来收集老年代的时候，新生代只能选择ParNew或者Parial收集器中的一个。 在G1出现之前，CMS还是非常广泛的，一直到今天，仍然有许多系统使用CMS GC。 2）CMS工作流程 初始标记：标记出GCROOTS能直接关联到的对象（速度快） 并发标记：从GCROOTS的直接关联对象开始遍历整个对象图的过程（耗时长，但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行） 重新标记：修正并发标记期间，因为用户程序继续运作而导致标记产生变动的那一部分对象的标记记录（时间比初始标记稍微长） 并发清除：清理删除掉标记阶段判断的已经死亡的对象，释放内存空间。（由于不需要移动存活的对象，所以这个阶段也是可以与用户线程并发执行的） 初始化标记和再次标记仍然要STW机制，目前所有的来收集器都做不到完全不需要STW，只是尽可能的缩短暂停时间。由于最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低停顿的。 在CMS回收过程中，还应该确保应用程序线程有足够的内存可用。因此，CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了在进行收集，而是当堆内存使用率达到某一阈值时，便开始进行回收，以确保应用程序在CMS工作过程中依然有足够的空间支持应用程序在运行。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次”Concurrent Mode Failure”失败，这时虚拟机将启动预备方案，临时启动Serial Old收集器来重新收集老年代的垃圾收集，这样停顿时间就很长了。 CMS的垃圾收集算法使用的是标记清除算法，不可避免的会产生内存碎片，无法使用指针碰撞，只能选择空闲列表。 标记清除算法会造成内存碎片，为什么不把算法换成标记整理算法呢？ 因为当并发清除时，用标记整理算法整理内存的话，原来的用户线程使用的内存无法继续使用，要保证用户线程还能继续执行，前提是他运行的资源不受影响。 优点：并发收集，低延迟 缺点：会产生内存碎片，对CPU资源敏感（并发阶段，占用了一部分线程导致应用程序变慢，总吞吐量降低），无法处理浮动垃圾（在并发阶段如果产生新的垃圾对象，CMS无法对这些垃圾对象进行标记，最终会导致这些新产生 的垃圾对象没有及时回收，只能在下一次GC的时候释放这些之前未被回收的内存空间） 3）参数设置 -XX:+UseConcMarkSweepGC 手动指定使用CMS收集器执行内存回收任务 -XX:CMSlnitiatingOccupanyFraction 设置堆使用率的阈值，一旦达到阈值，便开始垃圾回收 -XX:+UseCMSCompactAtFullCollection 用于指定在执行完Full GC后堆内存空间进行压缩整理，以此避免内存碎片的产生。不过由于内存压缩整理过程无法并发执行，所带来的的问题就是停顿时间变得更长了。 -XX:+CMSFullGCsBeforeCompaction 设置在执行了多少次Full GC后对内存进行压缩整理。 -XX:ParallelCMSThreads 设置CMS线程数。CMS默认启动线程数是 （ParallelGCThreads+3）/4，ParallelGCThreads 是年轻代并行收集器的线程数。当CPU资源紧张时，受到CMS收集器线程的影响，应用程序的性能在垃圾回收节点可能会非常糟糕。 4）小技巧 HotSpot有这么多的垃圾回收器，那么如果有人问，Serial GC，Parallel GC，Concurrent Mark Sweep GC这三个GC有什么不同呢？ 最小化的使用内存和并行开销 Serial GC 最大化应用程序的吞吐量 Parallel GC 最小化GC的中断或停顿时间 CMS GC 5）后续版本变化 JDK9 声明CMS为过时，JDK14直接删掉了，如果使用不会报错，只是会给出警告，并使用默认的垃圾收集器。 68.G1回收器（区域分代化）1）概述 G1是在java7引入的垃圾回收器，为了适应不断扩大的内存和不断增加的处理器数量，进一步降低暂停时间，同时兼顾良好的吞吐量。 为什么叫G1回收器？ 因为G1是一个并行回收器，他把堆内存分割成很多不相关的区域。G1有计划的避免在整个java堆中进行全区域的垃圾收集。G1跟踪各个区域里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的区域。 由于这种方式的侧重点在于回收垃圾最大量的区间，所以我们给G1一个名字，垃圾优先。 G1主要针对配备多核CPU以及大容量内存的机器，是JDK9以后的默认垃圾回收器，在JDK8还不是默认的垃圾回收器，需要使用-XX:UseGmentGC来启用。 与其他垃圾回收器相比，G1使用了全新的分区算法： 2）特点 ①并行与并发 并行性：G1在回收期间，可以有多个GC线程同时工作，有效利用多核计算能力。此时用户线程STW。 并发性：G1拥有与应用程序交替执行的能力，部分工作可以和应用程序同时执行，因此，一般来说，不会再整个回收阶段发生完全阻塞应用程序的情况。 ②分代收集 从分代上看，G1依然属于分代型垃圾回收器，他会区分年轻代和老年代，年轻代依然有Eden区和幸存者区。但从堆的结构上看，他不要求整个Eden区，年轻代或者老年代都是连续的，也不再坚持固定大小和固定数量。 将堆空间分为若干个区域，这些区域中包含了逻辑上的年轻代和老年代。 和之前的各类回收器不同，他同时兼顾年轻代和老年代。 ③空间整合 CMS：标记清除算法，内存碎片，若干次GC后进行一次碎片整理。 G1将内存划分为一个个小区域，内存的回收是以一个个小区域为单位的。区域之间是复制算法，但是整体上可以看成标记压缩算法，两种算法都可以避免内存碎片。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。尤其是当java堆非常大的时候，G1的优势更加明显。 ④可预测的停顿时间 每次根据允许的收集时间，优先回收价值最大的区域，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。 3）缺点 在用户程序运行过程中，G1无论是为了垃圾收集产生的内存占用还是程序运行时的额外执行负载都要比CMS高。 经验上来讲：在小内存应用上CMS的表现大概率会优先于G1，而G1在大内存应用上则发挥其优势。平衡点在6-8G之间。 4）参数设置 -XX:+UseG1GC 手动指定使用G1垃圾收集器执行内存回收任务 -XX:G1HeapRegionSize 设置每个区域的大小。值是2的幂，范围是1M-32M之间。 -XX:MaxGCPauseMillis 设置期望达到的最大GC停顿时间指标 -XX:ParallelGCThread 设置STW工作线程数，最多设置为8 -XX:ConcGCThreads 设置并发标记的线程数 -XX:InitiatingHeapOccupancyPercent 设置触发并发GC周期的java堆占用率阈值。超过此值，就会出发GC。 5）G1调优的步骤 开启垃圾收集器 设置堆的最大内存 设置最大停顿时间 G1提供了三种垃圾收集模式：YoungGC，Mixed GC和Full GC，在不同的条件下被触发。 6）适用场景 面向服务端应用，针对具有大内存，多处理器的机器 需要低GC延迟，并具有大堆的应用程序提供解决方案 用来替换掉jdk5的cms HotSpot垃圾收集器，除了G1以外，其他的垃圾收集器使用内置的JVM线程执行GC的多线程操作，而G1 GC可以采用应用线程承担后台运行的GC工作，即当JVM的GC线程处理速度慢时，系统会调用应用程序线程帮助加速垃圾收集过程。 所有的区域都大小相同，并且在JVM生命周期内不会被改变，虽然还保留新生代和老年代的概念，但是新生代和老年代不再是物理隔离了，他们都是一部分区域的集合。通过区域的动态分配的方式实现逻辑上的连续。 7）Humongous G1垃圾收集器还增加了一种新的内存区域，叫做Humongous，主要存储大对象，如果超过1.5个区域，就放到H。 设置H的原因：对于堆中的大对象，默认直接会被分配到老年代，但是如果他是一个短期的大对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1划分了一个H区，他专门用来存放大对象。如果一个H区装不下一个大对象，那么G1会寻找连续的H区来存储。为了能找到连续的H区，有时候不得不启动Full GC。G1的大多数行为都把H区作为老年代的一部分来看待。 8）回收过程 G1 GC的垃圾回收过程主要包括如下三个环节 年轻代GC 老年代并发标记过程 混合回收 如果需要，单线程，独占式，高强度的Full GC还是继续存在的。他针对GC的评估失败提供了一种失败保护机制，即强力回收。 应用程序分配内存，当年轻代的Eden区用尽时开始年轻代回收过程；G1的年轻代收集阶段是一个并行的独占式收集器。在年轻代回收期，G1 GC暂停所有应用程序线程，启动多线程执行年轻代回收。然后从年轻代区间移动存活对象到Survivor区间或者老年代区间，也有可能是两个区间都会涉及。 当堆内存使用达到一定值的时候，开始老年代并发标记过程。 标记完成马上开始混合回收过程。对于一个混合回收期，G1 GC从老年区间移动存活对象到空闲区间，这些空闲区间也就成为了老年代的一部分。和年轻代不同，老年代的G1回收器和其他GC不同，G1的老年代回收器不需要整个老年代被回收，一次只需要扫描/回收一小部分老年代的区域就可以了。同时，这个老年代的区域是和新生代一起被回收的。 ①记忆集与写屏障 一个对象被不同区域引用的问题 一个区域不可能是孤立的，一个区域中的对象可能被其他任意区域中的对象引用，判断对象存活时，是否需要扫描整个Java堆才能保证准确？ 在其他的分代收集器，也存在这样的问题 回收新生代也不得不同时扫描老年代？ 这样的话会降低Minor GC的效率 解决方法 无论G1还是其他垃圾收集器，Jvm都是使用Remembered Set 来避免全局扫描。 每个区域都有一个对应的Remembered Set ； 每次Reference类型数据写操作时，都会产生一个Write Barrier暂时中断操作； 然后检查将要写入的引用指向的对象是否和该Reference类型数据在不同的区域 如果不同，通过CardTable把相关引用信息记录到引用指向对象的所在区域对应的Remembered Set 中； 当进行垃圾收集时，在GC跟节点的枚举范围加入Remembered Set；就可以保证不进行全局扫描，也不会有遗漏。 ②年轻代GC JVM启动时，G1先准备好Eden区，程序在运行过程中不断创建对象到Eden区，当Eden空间耗尽时，G1会启动一次年轻代垃圾回收过程。 年轻代垃圾回收只会回收Eden区和幸存者区。 首先G1停止应用程序的执行，G1创建回收集，回收集是指需要被回收的内存分段的集合，年轻代回收过程的回收集包含年轻代Eden区和幸存者区所有的内存字分段。然后开始进行如下回收：扫描根 更新RSet 处理RSet 复制对象 处理引用 第一阶段，扫描根。 根是指static变量指向的对象，正在执行的方法调用链条上的局部变量等。根引用连同RSet记录的外部引用作为扫描存活对象的入口。 第二阶段，更新RSet。 处理dirty card queue( 见备注)中的card，更新RSet。 此阶段完成后，RSet可 以准确的反映老年代对所在的内存分段中对象的引用。 第三阶段，处理RSet. 识别被老年代对象指向的Eden中的对象，这些被指向的Eden中的对象被认为是存活的对象。第四阶段，复制对象。 此阶段，对象树被遍历，Eden区 内存段中存活的对象会被复制到Survivor区中空的内存分段，Survivor区内存段中存活的对象如果年龄未达阈值，年龄会加1，达到阀值会被会被复制到 01d区中空的内存分段。如果Survivor空间不够，Eden空 间的部分数据会直接晋升到老年代空间。 第五阶段，处理引用。 处理Soft，Weak，Phantom, Final, JNI Weak等引用。最终Eden空间的数据为空，GC停止工作，而目标内存中的对象都是连续存储的，没有碎片，所以复制过程可以达到内存整理的效果，减少碎片。 ②并发标记过程 初始标记阶段 根区域扫描 并发标记 再次标记 独占标记 独占清理 并发清理阶段 1.初始标记阶段:标记从根节点直接可达的对象。这个阶段是STW的，并且会触发- - 次年轻代GC。 2.根区域扫描(Root Region Scanning) : G1 GC扫描Survivor区直接可达的老年代， 区域对象，并标记被引用的对象。这一-过程必须在young GC之前完成。 3.并发标记(Concurrent Marking): 在整个堆中进行并发标记(和应用程序并发执行)， 此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾,那这个区域会被立即回收。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。 4.再次标记(Remark):由 于应用程序持续进行，需要修正上一- 次的标记结果。是STW 的。G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。 5.独占清理(cleanup,STW):计算各个区域的存活对象和GC回收比例，并进行排序，识别可以混合回收的区域。为下阶段做铺垫。是STW的。 ➢这个阶段并不会实际上去做垃圾的收集。 6.并发清理阶段:识别并清理完全空闲的区域。 ③混合回收 当越来越多的对象晋升到老年代，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即Mixed GC，该算法并不是一个Old GC，除了回收整个Young Region，还会回收一部分的Old Region。这里需要注意：是一部分老年代，而不是全部老年代。可以选择哪些Old Region进行收集。从而可以对垃圾回收的耗时时间进行控制。也要注意的是Mixed GC并不是Full GC。 并发标记结束以后，老年代中百分百为垃圾的内存分段被回收了，部分为垃圾的内存分段被计算了出来。默认情况下，这些老年代的内存分段会分8次(可以通过-XX: G1MixedGCCountTarget设置)被回收。 混合回收的回收集(Collection Set)包括八分之- -的老年代内存分段，Eden区 内存 分段，Survivor区 内存分段。混合回收的算法和年轻代回收的算法完全一样， 只是回收集多了老年代的内存分段。具体过程请参考上面的年轻代回收过程。 由于老年代中的内存分段默认分8次回收，G1会优先回收垃圾多的内存分段。垃圾占内存分段比例越高的，越会被先回收。并且有一个阈值会决定内存分段是否被回收，-XX:G1MixedGCLiveThresholdPercent，默认为65%，意思是垃圾占内存分段比例要达到65%才会被回收。如果垃圾占比太低，意味着存活的对象占比高，在复制的时候会花费更多的时间。 混合回收并不一定要进行8次。有一个阈值-XX:G1HeapWastePercent，默认值为10%，意思是允许整个堆内存中有10%的空间被浪费，意味着如果发现可以回收的垃圾占堆内存的比例低于10%，则不再进行混合回收。因为GC会花费很多的时间但是回收到的内存却很少。 ④Full GC G1的初衷就是要避免Full GC的出现，但是如果上述方式不能正常工作，G1会停止应用程序的执行，使用单线程的内存回收算法进行垃圾回收，性能会非常差，应用程序停顿时间会很长。 要避免Full GC的发生，一旦发生需要进行调整。什么时候会发生Full GC呢？比如堆内存太小，当G1在复制存活对象的时候没有空的内存分段可用，则会回退到Full GC，这种情况下可以通过增大内存解决。 导致G1Full GC的原因可能有两个 Evacuation的时候没有足够的to-space来存放晋升的对象 并发处理过程完成之前空间耗尽 ⑤G1回收器优化建议 年轻代大小 避免使用-Xmn或者-XX:NewRatio等相关选项显式设置年轻代的大小 固定年轻代的大小会覆盖暂停时间目标 暂停时间目标不要太过严苛 G1 GC的吞吐量目标是90%的应用程序时间和10%的垃圾回收时间 评估G1 GC的吞吐量时，暂停时间目标不要太严苛。目标太严苛表示你愿意承受更多的垃圾回收开销，而这些会直接影响到吞吐量。 69.垃圾回收器总结1）对比 垃圾收集器 分类 作用位置 使用算法 特点 适用场景 Serial 串行 新生代 复制算法 响应速度优先 单CPU的client模式 ParNew 并行 新生代 复制算法 响应速度优先 多CPU的server模式与CMS配合使用 Parallel 并行 新生代 复制算法 吞吐量优先 后台运算而不需要太多交互的场景 Serial Old 串行 老年代 标记压缩 响应速度优先 单CPU的client模式 Parallel Old 并行 老年代 标记压缩 吞吐量优先 后台运算而不需要太多交互的场景 CMS 并发 老年代 标记清除 响应速度优先 互联网/BS业务 G1 并发，并行 新生代，老年代 标记压缩，复制 响应速度优先 服务端应用 GC发展阶段 Serial =&gt; Parallel(并行) =&gt;CMS(并发)=&gt;G1=&gt;ZGC 2）垃圾回收器的组合 3）怎么选择垃圾回收器？ 1.优先调整堆的大小让JVM自适应完成 2.如果内存小于100M，使用串行收集器 3.如果是单核，单机程序，并且没有停顿时间的要求，串行收集器 4.如果是多CPU，需要高吞吐量，允许停顿时间超过1s，选择并行或者JVM自己选择 5.如果是多CPU，追求低停顿时间，需快速响应，使用并发收集器 官方推荐G1，性能高。现在互联网的项目，基本都是使用G1. 没有最好的收集器，调优是针对特定场景，特定需求。 70.CMS运行过程，缺点？整个过程分为四个步骤 初始标记(STW)： 暂停所有的其他线程(STW)，并记录下gc roots直接能引用的对象，速度很快 并发标记： 并发标记阶段就是从GC Roots的直接关联对象开始遍历整个对象图的过程， 这个过程耗时较长但是不需要停顿用户线程，因为用户程序继续运行，可能会有导致已经标记过的对象状态发生改变。 重新标记(STW)： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短。 并发清理： 开启用户线程，同时GC线程开始对未标记的区域做清扫。 并发重置：重置本次GC过程中的标记数据。 缺点： 对CPU资源敏感（会和服务抢资源） 无法处理浮动垃圾(在并发标记和并发清理阶段又产生垃圾，这种浮动垃圾只能等到下一次gc再清理了)； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生，当然通过参数-XX:+UseCMSCompactAtFullCollection可以让jvm在执行完标记清除后再做整理 并发模式失败(最大问题)，执行过程中的不确定性，会存在上一次垃圾回收还没执行完，然后垃圾回收又被触发的情况，特别是在并发标记和并发清理阶段会出现，一边回收，系统一边运行，也许没回收完就再次触发full gc，也就是”concurrent mode failure”，此时会进入stop the world，用serial old垃圾收集器来回收 71.G1运行过程G1收集器一次GC的运作过程大致分为以下几个步骤： 初始标记（initial mark，STW）：暂停所有的其他线程，并记录下gc roots直接能引用的对象，速度很快 ； 并发标记（Concurrent Marking）：同CMS的并发标记 最终标记（Remark，STW）：同CMS的重新标记 筛选回收（Cleanup，STW）：G1采用复制算法回收几乎不会有太多内存碎片 72.G1适合什么场景 50%以上的堆被存活对象占用 对象分配和晋升的速度变化非常大 垃圾回收时间特别长，超过1秒 8GB以上的堆内存(建议值) 停顿时间是500ms以内 73.什么情况下会触发Full GC？对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件: 调用 System.gc() 只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。 老年代空间不足 老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。 为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对象进入老年代的年龄，让对象在新生代多存活一段时间。 空间分配担保失败 使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。 JDK 1.7 及以前的永久代空间不足 在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静态变量等数据。 当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。 为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。 Concurrent Mode Failure 执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足(可能是 GC 过程中浮动垃圾过多导致暂时性的空间不足)，便会报 Concurrent Mode Failure 错误，并触发 Full GC。 74.内存溢出与内存泄漏1）内存溢出 内存溢出是相对于内存泄漏来说的，尽管更容易被理解，但是同样的，内存溢出也是引发程序崩溃的罪魁祸首之一。大多数情况下，GC会进行各种年龄段的垃圾回收实在不行了就放大招，来一次Full GC操作，这时候会回收大量内存，供应用程序继续使用。javadoc对OOM的解释是：没有空闲内存，并且垃圾收集器也无法提供更多内存。 堆内存不够的原因有两个：java虚拟机的堆内存设置不够。代码中创建了大量的大对象，并且长时间不能被垃圾收集器收集（存在被引用） OOM之前，通常垃圾收集器会先进行GC，当然也不是任何情况下垃圾收集器都会被触发：比如我们创建一个超过堆空间大小的对象。 2）内存泄漏 只有对象不在被程序用到了，但是GC又不能回收他们的情况，才叫内存泄漏。实际上一些导致对象生命周期变得很长甚至OOM的操作，也称为内存泄漏。 尽管内存泄漏并不会立刻引起程序崩溃，但是一旦发生内存泄漏，程序中的可用内存就会被逐步蚕食，直至耗尽所有内存，最终出现OOM，导致程序崩溃。 这里的存储空间并不是指物理内存，而是指虚拟内存大小，这个虚拟内存取决于磁盘交换区设定的大小。 Example： 1.单例模式，单例的生命周期默认和应用程序一样长，所以单例程序中，如果持有对外部对象的引用的话，那么这个外部对象是不能被回收的，否则会导致内存泄漏的产生。 2.一些提供close的资源未关闭导致内存泄漏（数据库连接，网络连接，io连接） 75.Stop The World指的是GC事件发生过程中，会产生应用程序的停顿。停顿产生时整个应用程序线程都会被停掉，没有任何响应。 STW事件和采用哪款垃圾收集器无关，所有GC都有这个事件。它是由JVM在后台自动发起和自动完成的。 76.安全点与安全区域1）安全点 程序执行过程中并不是在所有地方都能停顿下来开始GC，只有在特定的位置才能停顿下来开始GC，这些位置称为安全点。 安全点太少可能导致GC等待时间太长，太多可能导致程序运行时的性能问题。 方法调用，循环跳转，异常跳转。 如何在GC发生时，检查所有线程都跑到最近的安全点停下来呢？ 抢先式中断（目前没有虚拟机采用了）中断所有线程，哪个没到安全点就恢复线程，让线程跑到安全点。 主动式中断设置一个中断标志，各个线程运行到安全点的时候主动轮询这个标志，如果中断标志为真，则将自己进行中断挂起。 2）安全区域 安全点机制保证了程序执行时，在不太长的时间内就会遇到可进入的GC安全点，但是假如程序处于sleep状态，这时候线程无法响应jvm的中断请求，走到安全点去中断挂起，jvm也不太可能的等待线程被唤醒。对于这种情况，就需要安全区域来解决。 安全区域是指在一段代码片段中，对象的引用关系不会发生变化，在这个区域中的任何位置开始GC都是安全的 实际执行时 当线程运行到安全区域的代码时，首先标识已经进入了安全区域，如果这段时间内发生GC，JVM会忽略标识为安全区域状态的线程。 当线程即将离开安全区域时，会检查jvm是否已经完成GC，如果完成了，则继续运行，否则线程必须等待直到收到可以安全离开安全区域的信号为止。 77.再谈内存泄漏1）内存泄露的理解与分析 可达性分析算法来判断对象是否是不再使用的对象，本质都是判断一个对象是否还被引用。那么对于这种情况下，由于代码的实现不同就会出现很多种内存泄漏问题(让JVM误以为此对象还在引用中，无法回收，造成内存泄漏)。 是否还被使用? 是 是否还被需要? 否 内存泄漏(memory leak) 的理解 严格来说，只有对象不会再被程序用到了，但是GC又不能回收他们的情况，才叫内存泄漏。 但实际情况很多时候一些不太好的实践(或疏忽)会导致对象的生命周期变得很长甚至导致0OM，也可以叫做宽泛意义上的“内存泄漏”。 对象X引用对象Y,X的生命周期比Y的生命周期长; 那么当Y生命周期结束的时候，X依然引用着Y，这时候，垃圾回收期是不会回收对象Y的; 如果对象X还引用着生命周期比较短的A、B、C,对象A又引用着对象a、b、c，这样就可能造成大量无用的对象不能被回收，进而占据了内存资源，造成内存泄漏，直到内存溢出。 内存泄漏与内存溢出的关系: 11.内存泄漏(memory leak ) 申请了内存用完了不释放，比如一共有1024M的内存，分配了512M 的内存一-直不回收，那么可以 用的内存只有512M 了，仿佛泄露掉了一部分; 通俗一点讲的话，内存泄漏就是[占着茅坑不拉shi]。 12.内存溢出(out of memory ) 申请内存时，没有足够的内存可以使用; 通俗一点儿讲，-一个厕所就三个坑，有两个站着茅坑不走的(内存泄漏)，剩下最后一个坑，厕所表示接待压力很大，这时候一下子来了两个人，坑位(内存)就不够了，内存泄漏变成内存溢出了。 可见，内存泄漏和内存溢出的关系:内存泄漏的增多，最终会导致内存溢出。 1泄漏的分类 经常发生:发生内存泄露的代码会被多次执行，每次执行，泄露一块内存; 偶然发生:在某些特定情况下才会发生 一次性:发生内存泄露的方法只会执行一次; 隐式泄漏:一 直占着内存不释放，直到执行结束;严格的说这个不算内存泄漏，因为最终释放掉了，但是如果执行时间特别长，也可能会导致内存耗尽。 2）Java中内存泄露的8种情况 11-静态集合类 静态集合类，如HashMap、 LinkedList等等。 如果这些容器为静态的，那么它们的生命周期与JVM程序一致，则容器中的对象在程序结束之前将不能被释放，从而造成内存泄漏。简单而言，长生命周期的对象持有短生命周期对象的引用，尽管短生命周期的对象不再使用，但是因为长生命周期对象持有它的引用而导致不能被回收。 12-单例模式 单例模式，和静态集合导致内存泄露的原因类似，因为单例的静态特性，它的生命周期和JVM的生命周期一样长，所以如果单例对象如果持有外部对象的引用，那么这个外部对象也不会被回收，那么就会造成内存泄漏。 13-内部类持有外部类 内部类持有外部类，如果一个外部类的实例对象的方法返回了一个内部类的实例对象。 这个内部类对象被长期引用了，即使那个外部类实例对象不再被使用，但由于内部类持有外部类的实例对象，这个外部类对象将不会被垃圾回收，这也会造成内存泄漏。 14-各种连接，如数据库连接、网络连接和IO连接等 各种连接，如数据库连接、网络连接和IO连接等。 在对数据库进行操作的过程中，首先需要建立与数据库的连接，当不再使用时，需要调用close方法来释放与数据库的连接。只有连接被关闭后，垃圾回收器才会回收对应的对象。 否则，如果在访问数据库的过程中，对Connection、 Statement或ResultSet不显性地关闭，将会造成大量的对象无法被回收，从而引起内存泄漏。 15-变量不合理的作用域 变量不合理的作用域。一般而言，一个变量的定义的作用范围大于其使用范围，很有可能会造成内存泄漏。另一方面，如果没有及时地把对象设置为null,很有可能导致内存泄漏的发生。 16-改变哈希值 改变哈希值，当一个对象被存储进HashSet集合中以后，就不能修改这个对象中的那些参与计算哈希值的字段了。 否则，对象修改后的哈希值与最初存储进HashSet集合中时的哈希值就不同了，在这种情况下，即使在contains方法使用该对象的当前引用作为的参数去HashSet集合中检索对象，也将返回找不到对象的结果，这也会导致无法从HashSet集合中单独删除当前对象，造成内存泄漏。 这也是String为什么被设置成了不可变类型，我们可以放心地把String 存入HashSet,或者把 String当做HashMap 的key值; 当我们想把自己定义的类保存到散列表的时候，需要保证对象的hashCode不可变。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118public class ChangeHashCode &#123; public static void main(String[] args) &#123; HashSet set = new HashSet(); Person p1 = new Person(1001, &quot;AA&quot;); Person p2 = new Person(1002, &quot;BB&quot;); set.add(p1); set.add(p2); p1.name = &quot;CC&quot;;//导致了内存的泄漏 set.remove(p1); //删除失败 System.out.println(set); set.add(new Person(1001, &quot;CC&quot;)); System.out.println(set); set.add(new Person(1001, &quot;AA&quot;)); System.out.println(set); &#125;&#125;class Person &#123; int id; String name; public Person(int id, String name) &#123; this.id = id; this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (!(o instanceof Person)) return false; Person person = (Person) o; if (id != person.id) return false; return name != null ? name.equals(person.name) : person.name == null; &#125; @Override public int hashCode() &#123; int result = id; result = 31 * result + (name != null ? name.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125;例2：/** * 演示内存泄漏 * @author shkstart * @create 14:47 */public class ChangeHashCode1 &#123; public static void main(String[] args) &#123; HashSet&lt;Point&gt; hs = new HashSet&lt;Point&gt;(); Point cc = new Point(); cc.setX(10);//hashCode = 41 hs.add(cc); cc.setX(20);//hashCode = 51 此行为导致了内存的泄漏 System.out.println(&quot;hs.remove = &quot; + hs.remove(cc));//false hs.add(cc); System.out.println(&quot;hs.size = &quot; + hs.size());//size = 2 System.out.println(hs); &#125;&#125;class Point &#123; int x; public int getX() &#123; return x; &#125; public void setX(int x) &#123; this.x = x; &#125; @Override public int hashCode() &#123; final int prime = 31; int result = 1; result = prime * result + x; return result; &#125; @Override public boolean equals(Object obj) &#123; if (this == obj) return true; if (obj == null) return false; if (getClass() != obj.getClass()) return false; Point other = (Point) obj; if (x != other.x) return false; return true; &#125; @Override public String toString() &#123; return &quot;Point&#123;&quot; + &quot;x=&quot; + x + &#x27;&#125;&#x27;; &#125;&#125; 17-缓存泄露 内存泄漏的另一个常见来源是缓存，- -旦你把对象引用放入到缓存中，他就很容易遗忘。比如:之前项目在一次上线的时候，应用启动奇慢直到夯死，就是因为代码中会加载-个表中的数据到缓存(内存)中，测试环境只有几百条数据，但是生产环境有几百万的数据。 对于这个问题，可以使用WeakHashMap代表缓存，此种Map的特点是，当除了自身有对key的引用外，此key没有其他引用那么此map会自动丢弃此值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class MapTest &#123; static Map wMap = new WeakHashMap(); static Map map = new HashMap(); public static void main(String[] args) &#123; init(); testWeakHashMap(); testHashMap(); &#125; public static void init() &#123; String ref1 = new String(&quot;obejct1&quot;); String ref2 = new String(&quot;obejct2&quot;); String ref3 = new String(&quot;obejct3&quot;); String ref4 = new String(&quot;obejct4&quot;); wMap.put(ref1, &quot;cacheObject1&quot;); wMap.put(ref2, &quot;cacheObject2&quot;); map.put(ref3, &quot;cacheObject3&quot;); map.put(ref4, &quot;cacheObject4&quot;); System.out.println(&quot;String引用ref1，ref2，ref3，ref4 消失&quot;); &#125; public static void testWeakHashMap() &#123; System.out.println(&quot;WeakHashMap GC之前&quot;); for (Object o : wMap.entrySet()) &#123; System.out.println(o); &#125; try &#123; System.gc(); TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;WeakHashMap GC之后&quot;); for (Object o : wMap.entrySet()) &#123; System.out.println(o); &#125; &#125; public static void testHashMap() &#123; System.out.println(&quot;HashMap GC之前&quot;); for (Object o : map.entrySet()) &#123; System.out.println(o); &#125; try &#123; System.gc(); TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;HashMap GC之后&quot;); for (Object o : map.entrySet()) &#123; System.out.println(o); &#125; &#125;&#125; 上面代码和图示主演演示WeakHashMap如何自动释放缓存对象，当init函 数执行完成后，局部变量字 符串引用weakd1 ,weakd2,d1,d2都会消失，此时只有静态map中保存中对字符串对象的引用，可以 看到，调用gc之后，HashMap的没有被回收，而WeakHashMap 里面的缓存被回收了。 18-监听器和回调 内存泄漏第三个常见来源是监听器和其他回调，如果客户端在你实现的API中注册回调，却没有显示的取消，那么就会积聚。 需要确保回调立即被当作垃圾回收的最佳方法是只保存它的弱引用，例如将他们保存成为WeakHashMap中的键。 3）内存泄露案例分析 ①代码 123456789101112131415161718192021222324252627public class Stack &#123; private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() &#123; elements = new Object[DEFAULT_INITIAL_CAPACITY]; &#125; public void push(Object e) &#123; //入栈 ensureCapacity(); elements[size++] = e; &#125; public Object pop() &#123; if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; return result; &#125; private void ensureCapacity() &#123; if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); &#125;&#125; ②分析 上述程序并没有明显的错误，但是这段程序有一个内存泄漏，随着GC活动的增加，或者内存占用的不断增加，程序性能的降低就会表现出来，严重时可导致内存泄漏，但是这种失败情况相对较少。 代码的主要问题在pop函数，下面通过这张图示展现 假设这个栈- -直增长，增长后如下图所示 当进行大量的POP操作时，由于引用未进行置空，gc是不会释放的，如下图所示： 从上图可以看出，如果栈先增长，在收缩，那么从栈中弹出的对象将不会被当作垃圾回收，即使程序不再使用栈中的这些对象，他们也不会回收，因为栈中仍然保存这对象的引用，这个内存泄漏很隐蔽。 ③解决办法 将代码中的pop()方法变成如下方法： 12345public Object pop() &#123; //出栈 if (size == 0) throw new EmptyStackException(); return elements[--size];&#125; 一旦引用过期，清空这些引用，将引用置空。 78.判断元空间是无用的类法区（元空间）主要回收的是无用的类，那么如何判断一个类是无用的类呢？类需要同时满足下面3个条件才能算是 “无用的类” ： 该类所有的对象实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。（条件苛刻，自定义类加载器会被回收掉） 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 79.创建对象的几种方式1）new new（直接new ，工厂模式，构建者模式） 2）Class.forName().newInstance() 反射的方式，只能调用空参构造器，权限是public 3）Constructor.newInstance(xxx) 反射的方式，可以调用空参，带参的构造器，权限没要求 4）使用clone（） 不调用任何构造器，当前需要实现Cloneable接口，实现clone（）；分为深克隆和浅克隆 对象嵌套 5）使用反序列化 从文件，网络中获取一个对象的二进制流 6）第三方库Objenesis 80.对象访问定位JVM是如何通过栈帧中的对象引用访问到其内部的对象实例呢？ 定位，通过栈上reference访问。创建对象的目的是为了使用它。对象访问方式主要有两种： 1）句柄访问 优点：引用中存储稳定句柄地址，对象被移动时只会改变句柄中实例数据指针即可，引用本身不需要被修改。 缺点：需要额外维护一个句柄，效率低。 2）直接指针(HotSpot采用) 优点：效率高 81.永久代为什么要被元空间替换？以前使用虚拟机内存，现在使用本地内存 ①为永久代设置空间大小很难确定 ②对永久代调优困难 方法区的垃圾收集主要分两部分 常量池中废弃的常量和不再使用的类型 82.方法区的内部结构尽管所有方法区在逻辑上是属于堆的一部分，但一些简单的实现可能不会选择去进行垃圾回收或者压缩，对于hotspot而言，方法区还有一个别名叫非堆，目的就是要和堆分开。所以方法区可以看做是一块独立于java堆的内存空间。方法区与堆空间一样，是多线程共享的，方法区在jvm启动的时候被创建，并且他的实际物理内存空间中和java堆区一样都可以是物理上不连续的。方法区的大小和堆一样可以选择固定大小和扩展。方法区的大小决定了系统可以保存多少个类，如果系统定义了太多的类，导致方法区溢出，虚拟机同样会OOM。关闭jvm就会释放这个区域的内存。 他用于存储已经被虚拟机加载的类型信息，常量，静态变量，即时编译器编译后的代码缓存等。 1）类型信息 对每个加载的类型，jv必须在方法区存储一下类型信息。 全限定类名，直接父类的全限定类名，这个类型的修饰符，这个类型直接接口的一个有序列表 2）域信息 jvm必须在方法区中保存类型的所有域的信息以及域的声明顺序。 域的相关信息包括：域名称，域类型，域修饰符。 3）方法信息 jvm必须保存所有方法的以下信息，同域信息一样包括声明顺序 方法名称，返回类型，参数的数量和类型，顺序，方法的修饰符，方法的字节码，操作数栈，局部变量表以及大小，异常表 每个异常处理的开始位置，结束位置，代码处理在程序计数器中的偏移地址，被捕获的异常类的常量池索引。 12345678910111213141516171819202122232425262728/** * @author yinhuidong * @createTime 2020-08-20-12:09 * 1.静态变量和类关联在一起，随着类的加载而加载，他们成为类数据在逻辑上的一部分。 * 2.类变量被类的所有实例共享，即使没有实例时你也可以放问他。 * 3.全局常量 static final * 被声明为final的类变量的处理方法则不同，每个全局常量在编译的时候就会被分配了。 *图：final-static * javap -v Order.class * javap -v -p DemoE.class &gt; DemoE.txt */public class DemoD &#123; public static void main(String[] args) &#123; Order order=null; order.hello(); &#125;&#125;class Order&#123; public static int a=1; public static final int b=2; static &#123; a=3; &#125; public final static void hello()&#123; System.out.println(&quot;a = &quot; + a+&quot;----&quot;+&quot;b = &quot; + b); System.out.println(&quot;hello&quot;); &#125;&#125; 4）class文件中常量池的理解 方法区内部包含了运行时常量池，字节码文件内部包含了常量池。 为什么需要常量池？ java中的字节码需要数据支持，通常这种数据会很大以至于不能直接存到字节码里，换另一种方式，可以存到常量池，这个字节码包含了指向常量池的引用，在动态链接的时候就会用到运行时常量池。 一个有效的字节码文件中除了包含类的版本信息，字段，方法以及接口等描述信息外，还包含一项信息那就是常量池表，包含各种字面量和对应类型，域和方法的符号引用。 总结：常量池，可以看做是一张表，虚拟机指令根据这张常量表找到要执行的类名，方法名，参数类型，字面量等类型。 5）运行时常量池 方法区的一部分，class文件的常量池被类加载器加载到运行时数据区就会在方法区生成对应的运行时常量池，JVM为每一个已经加载的类或接口都维护了一个常量池。池子中的数据就像数组一样，都是通过索引访问的，运行时常量池包括多种不同的变量，包括编译期就已经明确的数值字面量，也包括到运行期解析后才能获得的方法或者字段引用。此时不再是常量池中的符号地址了，这里换位真实地址。 运行时常量池动态性，当创建类或者接口的运行时常量池时，如果构造运行时常量池所需的内存空间超过了方法区所能提供的最大值，则会OOM。 83.图解常量池操作 123456789public class DemoE &#123; public static void main(String[] args) &#123; int x=500; int y=100; int a=x/y; int b=50; System.out.println(a+b); &#125;&#125; 运行时常量池 运行时常量池是方法区的一部分。 常量池表用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池。 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是说，并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可以将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。 84.方法区的垃圾回收1.一般来说，方法区的回收效果不好，特别是类型的卸载，但是有时候回收又是必要的 2.方法区的垃圾回收主要是两部分：废弃的常量和不再使用的类型 3.方法区常量池主要存放：字面量和符号引用 符号引用包括：1.类和接口的全限定类名2.字段的名称和描述符3.方法的名称和描述符 4.只要常量池中的常量没有被任何地方引用，就可以被回收 5.判断一个类是否可以被回收 1.该类所属的实例都已经被回收 2.加载该类的类加载器已经被回收 3.该类对应的Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的对象 85.直接内存不是虚拟机运行时数据区的一部分，也不是jvm规范中定义的内存区域。 直接内存是在java堆外，直接向系统申请的内存空间。 来源于NIO，通过存在堆中的DirectByteBuffer操作Native内存，通常，访问直接内存的速度会优于java堆，即读写性能高，因此出于性能考虑，读写频繁的场合可能会考虑使用直接内存，java的NIO库允许java程序使用直接内存，用于数据缓冲区。 12345678910public class DemoH &#123; private static Integer BUFFER=10*1024*1024; public static void main(String[] args) &#123; Scanner sc = new Scanner(System.in); ByteBuffer byteBuffer=ByteBuffer.allocate(BUFFER); sc.next(); byteBuffer=null; System.gc(); &#125;&#125; 也可能导致OOM异常，由于直接内存在java堆外，因此它的大小不会受限于-Xmx指定的最大堆的大小，但是系统内存是有限的，java堆和直接内存的总和依然受限于操作系统能给出的最大内存。 缺点：分配回收成本高，不受JVM内存回收管理，直接内存大小可以通过MaxDirectMemorySize设置，如果不指定，默认与堆的最大值-Xmx参数值一致，简单理解java进程内存=java堆+本地内存。 86.静态变量放在哪里12345678910111213141516171819202122/** * @author yinhuidong * @createTime 2020-08-20-15:38 * 分析： * 1.new出来的结构，也就是对象实例都在堆空间 * 2.1.6：obj1随着DemoG的类型信息放在方法区；1.7：放在堆空间 * 3.obj2是实例变量，在堆空间 * 4.obj3在foo方法对应的栈帧的局部变量表 */public class DemoG &#123; static class Test&#123; static Order obj1=new Order(); //jdk7以前放在永久代，7开始放在堆空间 Order obj2=new Order(); //实例变量 堆空间 void foo()&#123; Order obj3=new Order(); //方法内部局部变量 栈帧里面的局部变量表 &#125; &#125; static class Order&#123; &#125;&#125; 87.执行引擎1.虚拟机是一个相对于物理机的概念，这两种机器都有代码执行能力，其区别是物理机的执行引擎是直接建立在处理器，缓存，指令集和操作系统层面的，而虚拟机的执行引擎是由软件自行实现的，因此可以不受物理条件制约的定制指令集与执行引擎的结构体系，能够执行那些不被硬件直接支持的指令集格式。 2.jvm的主要作用负责装在字节码到其内部，但是字节码不能直接运行在操作系统之上，执行引擎就是将字节码指令编译为对应平台上的本地机器指令。 3.外观上看：jvm的执行引擎输入输出都是一致的，输入的是字节码二进制流，处理过程是字节码解析执行的过程，输出的是执行结果。 88.java代码的编译和执行过程 大部分的程序代码转换为物理机的目标代码或虚拟机能执行的指令集之前，都需要经过上图的步骤。 为什么java称为半解释型半编译型语言？ 因为jvm的执行引擎是解释器和jit即时编译器交互工作的。 什么是解释器？什么是JIT编译器？ 解释器：将字节码指令逐行翻译成机器指令并执行 编译器：将源代码直接编译成对应的机器指令。 89.JIT即时编译器HotSpot采用解释器与即时编译器共存的架构。由JVM决定何时使用哪种方式执行。 解释器可以边解释边执行，这样程序启动时间就会变快，及时编译器是都编译好了在执行，程序启动时间慢。但是一旦jit编译器把越来越多的代码编译成本地代码，执行效率立马起飞。 1）热点代码以及探测方式 判断是否启动jit编译器将字节码直接编译为对应平台的本地机器指令，需要根据执行的频率而定。热点代码就是需要被编译为本地代码的字节码，jit在运行时会针对频繁调用的热点代码直接编译为对应平台的本地机器指令，提升性能。 2）OSR编译 一个方法被多次调用，或者一个方法体内部循环次数较多的循环体都可以被称为热点代码，因此都可以通过jit编译器编译为本地机器指令，也就是栈上替换.hotspot采用的热点探测方式是基于计数器的热点探测。 为每个方法建立2个不同类型的计数器，分别为方法调用计数器和回边计数器 1.方法调用计数器：统计方法调用次数，默认client模式下1500，server模式下100002.回边计数器：统计循环执行次数 当一个方法被调用时，先判断有没有jit编译过，有的话直接使用jit编译后的机器指令，没有的话计数器+1，然后判断两个计数器之和是否超过方法调用计数器的阈值。如果超过，就会向jit发出即时编译申请。 热度衰减是在虚拟机进行垃圾回收的时候顺便进行的，也就是在一段时间方法一直没有执行，计数器的计数就会减半。 可以自己手动设置虚拟机采用哪种编译模式 3）JDK9引入了AOT编译器 aot编译器在程序执行之前，就将字节码转换为机器码过程。 好处：可以直接运行，不必预热。 坏处：由于提前编译成了机器指令，无法实现java一次编译到处运行。 4）JDK10的Graal编译器 全新的即时编译器，实验阶段，需要手动开启，前景大好","categories":[{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://zhangxin66666.github.io/tags/JVM/"}]},{"title":"ConcurrentHashMap源码解读","slug":"1.基础知识/ConcurrentHashMap","date":"2024-08-22T03:23:14.702Z","updated":"2024-08-22T03:23:14.702Z","comments":true,"path":"2024/08/22/1.基础知识/ConcurrentHashMap/","link":"","permalink":"https://zhangxin66666.github.io/2024/08/22/1.%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/ConcurrentHashMap/","excerpt":"","text":"#1.成员变量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147//散列表数组的最大限制private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//散列表默认值private static final int DEFAULT_CAPACITY = 16;static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;//并发级别：jdk7历史遗留问题，仅仅在初始化的时候使用到，并不是真正的代表并发级别private static final int DEFAULT_CONCURRENCY_LEVEL = 16;//负载因子，JDK1.8中 ConcurrentHashMap 是固定值private static final float LOAD_FACTOR = 0.75f;//树化阈值，指定桶位 链表长度达到8的话，有可能发生树化操作。static final int TREEIFY_THRESHOLD = 8;//红黑树转化为链表的阈值static final int UNTREEIFY_THRESHOLD = 6;//联合TREEIFY_THRESHOLD控制桶位是否树化，只有当table数组长度达到64且 某个桶位 中的链表长度达到8，才会真正树化static final int MIN_TREEIFY_CAPACITY = 64;//线程迁移数据最小步长，控制线程迁移任务最小区间一个值private static final int MIN_TRANSFER_STRIDE = 16;//计算扩容时候生成的一个 标识戳private static int RESIZE_STAMP_BITS = 16;//结果是65535 表示并发扩容最多线程数private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;//扩容相关private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;//当node节点hash=-1 表示当前节点已经被迁移了 ，fwd节点static final int MOVED = -1; // hash for forwarding nodes//node hash=-2 表示当前节点已经树化 且 当前节点为treebin对象 ，代理操作红黑树static final int TREEBIN = -2; // hash for roots of treesstatic final int RESERVED = -3; // hash for transient reservations//转化成二进制实际上是 31个 1 可以将一个负数通过位移运算得到一个正数static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash//当前系统的cpu数量static final int NCPU = Runtime.getRuntime().availableProcessors();//为了兼容7版本的chp保存的，核心代码并没有使用到private static final ObjectStreamField[] serialPersistentFields = &#123; new ObjectStreamField(&quot;segments&quot;, Segment[].class), new ObjectStreamField(&quot;segmentMask&quot;, Integer.TYPE), new ObjectStreamField(&quot;segmentShift&quot;, Integer.TYPE) &#125;;//散列表，长度一定是2次方数transient volatile Node&lt;K,V&gt;[] table;//扩容过程中，会将扩容中的新table 赋值给nextTable 保持引用，扩容结束之后，这里会被设置为Nullprivate transient volatile Node&lt;K,V&gt;[] nextTable;//LongAdder 中的 baseCount 未发生竞争时 或者 当前LongAdder处于加锁状态时，增量累到到baseCount中private transient volatile long baseCount;/** * sizeCtl &lt; 0 * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * sizeCtl &gt; 0 * * 1. 如果table未初始化，表示初始化大小 * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */private transient volatile int sizeCtl;/** * * 扩容过程中，记录当前进度。所有线程都需要从transferIndex中分配区间任务，去执行自己的任务。 */private transient volatile int transferIndex;/** * LongAdder中的cellsBuzy 0表示当前LongAdder对象无锁状态，1表示当前LongAdder对象加锁状态 */private transient volatile int cellsBusy;/** * LongAdder中的cells数组，当baseCount发生竞争后，会创建cells数组， * 线程会通过计算hash值 取到 自己的cell ，将增量累加到指定cell中 * 总数 = sum(cells) + baseCount */private transient volatile CounterCell[] counterCells;// Unsafe mechanicsprivate static final sun.misc.Unsafe U;/**表示sizeCtl属性在ConcurrentHashMap中内存偏移地址*/private static final long SIZECTL;/**表示transferIndex属性在ConcurrentHashMap中内存偏移地址*/private static final long TRANSFERINDEX;/**表示baseCount属性在ConcurrentHashMap中内存偏移地址*/private static final long BASECOUNT;/**表示cellsBusy属性在ConcurrentHashMap中内存偏移地址*/private static final long CELLSBUSY;/**表示cellValue属性在CounterCell中内存偏移地址*/private static final long CELLVALUE;/**表示数组第一个元素的偏移地址*/private static final long ABASE;private static final int ASHIFT;static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField(&quot;sizeCtl&quot;)); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField(&quot;transferIndex&quot;)); BASECOUNT = U.objectFieldOffset (k.getDeclaredField(&quot;baseCount&quot;)); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField(&quot;cellsBusy&quot;)); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField(&quot;value&quot;)); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); //表示数组单元所占用空间大小,scale 表示Node[]数组中每一个单元所占用空间大小 int scale = U.arrayIndexScale(ak); //1 0000 &amp; 0 1111 = 0 if ((scale &amp; (scale - 1)) != 0) throw new Error(&quot;data type scale not a power of two&quot;); //numberOfLeadingZeros() 这个方法是返回当前数值转换为二进制后，从高位到低位开始统计，看有多少个0连续在一块。 //8 =&gt; 1000 numberOfLeadingZeros(8) = 28 //4 =&gt; 100 numberOfLeadingZeros(4) = 29 //ASHIFT = 31 - 29 = 2 ？？ //ABASE + （5 &lt;&lt; ASHIFT） ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125; #2.基础方法##2.1 spread高位运算 123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; ##2.2 tabAt该方法获取对象中offset偏移地址对应的对象field的值。实际上这段代码的含义等价于tab[i],但是为什么不直接使用 tab[i]来计算呢？ getObjectVolatile，一旦看到 volatile 关键字，就表示可见性。因为对 volatile 写操作 happen-before 于 volatile 读操作，因此其他线程对 table 的修改均对 get 读取可见； 虽然 table 数组本身是增加了 volatile 属性，但是“volatile 的数组只针对数组的引用具有volatile 的语义，而不是它的元素”。 所以如果有其他线程对这个数组的元素进行写操作，那么当前线程来读的时候不一定能读到最新的值。出于性能考虑，Doug Lea 直接通过 Unsafe 类来对 table 进行操作。 123static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; ##2.3 casTabAtcas设置当前节点为桶位的头节点 1234static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; ##2.4 setTabAt 123static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; ##2.5 resizeStampresizeStamp 用来生成一个和扩容有关的扩容戳，具体有什么作用呢？ 123static final int resizeStamp(int n) &#123; return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1));&#125; Integer.numberOfLeadingZeros 这个方法是返回无符号整数 n 最高位非 0 位前面的 0 的个数。 比如 10 的二进制是 0000 0000 0000 0000 0000 0000 0000 1010，那么这个方法返回的值就是 28。 根据 resizeStamp 的运算逻辑，我们来推演一下，假如 n=16，那么 resizeStamp(16)=32796转化为二进制是[0000 0000 0000 0000 1000 0000 0001 1100] 接着再来看,当第一个线程尝试进行扩容的时候，会执行下面这段代码： U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)rs 左移 16 位，相当于原本的二进制低位变成了高位 1000 0000 0001 1100 0000 0000 00000000 然后再+2 =1000 0000 0001 1100 0000 0000 0000 0000+10=1000 0000 0001 1100 0000 00000000 0010 高 16 位代表扩容的标记、低 16 位代表并行扩容的线程数 这样来存储有什么好处呢？ 1，首先在 CHM 中是支持并发扩容的，也就是说如果当前的数组需要进行扩容操作，可以由多个线程来共同负责 2，可以保证每次扩容都生成唯一的生成戳，每次新的扩容，都有一个不同的 n，这个生成戳就是根据 n 来计算出来的一个数字，n 不同，这个数字也不同 第一个线程尝试扩容的时候，为什么是+2 因为 1 表示初始化，2 表示一个线程在执行扩容，而且对 sizeCtl 的操作都是基于位运算的，所以不会关心它本身的数值是多少，只关心它在二进制上的数值，而 sc + 1 会在低 16 位上加 1。##2.6 tableSizeFor经过多次位移返回大于等于c的最小的二次方数 1234567891011121314151617181920 /** * Returns a power of two table size for the given desired capacity. * See Hackers Delight, sec 3.2 * 返回&gt;=c的最小的2的次方数 * c=28 * n=27 =&gt; 0b 11011 * 11011 | 01101 =&gt; 11111 * 11111 | 00111 =&gt; 11111 * .... * =&gt; 11111 + 1 =100000 = 32 */private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; #3. 构造方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public ConcurrentHashMap() &#123;&#125;public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); //如果指定的容量超过允许的最大值，设置为最大值 int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); /** * sizeCtl &gt; 0 * 当目前table未初始化时，sizeCtl表示初始化容量 */ this.sizeCtl = cap;&#125;public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.sizeCtl = DEFAULT_CAPACITY; putAll(m);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor) &#123; this(initialCapacity, loadFactor, 1);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; //参数校验 if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); //如果初始容量小于并发级别，那就设置初始容量为并发级别 if (initialCapacity &lt; concurrencyLevel) initialCapacity = concurrencyLevel; //16/0.75 +1 = 22 long size = (long)(1.0 + (long)initialCapacity / loadFactor); // 22 - &gt; 32 int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); /** * sizeCtl &gt; 0 * 当目前table未初始化时，sizeCtl表示初始化容量 */ this.sizeCtl = cap;&#125; #4.put 1234public V put(K key, V value) &#123; //如果key已经存在，是否覆盖，默认是false return putVal(key, value, false);&#125; #5 putVal 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129final V putVal(K key, V value, boolean onlyIfAbsent) &#123; //控制k 和 v 不能为null if (key == null || value == null) throw new NullPointerException(); //通过spread方法，可以让高位也能参与进寻址运算。 int hash = spread(key.hashCode()); //binCount表示当前k-v 封装成node后插入到指定桶位后，在桶位中的所属链表的下标位置 //0 表示当前桶位为null，node可以直接放着 //2 表示当前桶位已经可能是红黑树 int binCount = 0; //tab 引用map对象的table //自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //f 表示桶位的头结点 //n 表示散列表数组的长度 //i 表示key通过寻址计算后，得到的桶位下标 //fh 表示桶位头结点的hash值 Node&lt;K,V&gt; f; int n, i, fh; //CASE1：成立，表示当前map中的table尚未初始化.. if (tab == null || (n = tab.length) == 0) //最终当前线程都会获取到最新的map.table引用。 tab = initTable(); //CASE2：i 表示key使用路由寻址算法得到 key对应 table数组的下标位置，tabAt 获取指定桶位的头结点 f else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //进入到CASE2代码块 前置条件 当前table数组i桶位是Null时。 //使用CAS方式 设置 指定数组i桶位 为 new Node&lt;K,V&gt;(hash, key, value, null),并且期望值是null //cas操作成功 表示ok，直接break for循环即可 //cas操作失败，表示在当前线程之前，有其它线程先你一步向指定i桶位设置值了。 //当前线程只能再次自旋，去走其它逻辑。 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //CASE3：前置条件，桶位的头结点一定不是null。 //条件成立表示当前桶位的头结点 为 FWD结点，表示目前map正处于扩容过程中.. else if ((fh = f.hash) == MOVED) //看到fwd节点后，当前节点有义务帮助当前map对象完成迁移数据的工作 //帮助扩容 tab = helpTransfer(tab, f); //CASE4：当前桶位 可能是 链表 也可能是 红黑树代理结点TreeBin else &#123; //当插入key存在时，会将旧值赋值给oldVal，返回给put方法调用处.. V oldVal = null; //使用sync 加锁“头节点”，理论上是“头结点” synchronized (f) &#123; //为什么又要对比一下，看看当前桶位的头节点 是否为 之前获取的头结点？ //为了避免其它线程将该桶位的头结点修改掉，导致当前线程从sync 加锁 就有问题了。之后所有操作都不用在做了。 if (tabAt(tab, i) == f) &#123;//条件成立，说明咱们 加锁 的对象没有问题，可以进来造了！ //条件成立，说明当前桶位就是普通链表桶位。 if (fh &gt;= 0) &#123; //1.当前插入key与链表当中所有元素的key都不一致时，当前的插入操作是追加到链表的末尾，binCount表示链表长度 //2.当前插入key与链表当中的某个元素的key一致时，当前插入操作可能就是替换了。binCount表示冲突位置（binCount - 1） binCount = 1; //迭代循环当前桶位的链表，e是每次循环处理节点。 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; //当前循环节点 key K ek; //条件一：e.hash == hash 成立 表示循环的当前元素的hash值与插入节点的hash值一致，需要进一步判断 //条件二：((ek = e.key) == key ||(ek != null &amp;&amp; key.equals(ek))) // 成立：说明循环的当前节点与插入节点的key一致，发生冲突了 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; //将当前循环的元素的 值 赋值给oldVal oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; //当前元素 与 插入元素的key不一致 时，会走下面程序。 //1.更新循环处理节点为 当前节点的下一个节点 //2.判断下一个节点是否为null，如果是null，说明当前节点已经是队尾了，插入数据需要追加到队尾节点的后面。 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //前置条件，该桶位一定不是链表 //条件成立，表示当前桶位是 红黑树代理结点TreeBin else if (f instanceof TreeBin) &#123; //p 表示红黑树中如果与你插入节点的key 有冲突节点的话 ，则putTreeVal 方法 会返回冲突节点的引用。 Node&lt;K,V&gt; p; //强制设置binCount为2，因为binCount &lt;= 1 时有其它含义，所以这里设置为了2 binCount = 2; //条件一：成立，说明当前插入节点的key与红黑树中的某个节点的key一致，冲突了 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; //将冲突节点的值 赋值给 oldVal oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; //说明当前桶位不为null，可能是红黑树 也可能是链表 if (binCount != 0) &#123; //如果binCount&gt;=8 表示处理的桶位一定是链表 if (binCount &gt;= TREEIFY_THRESHOLD) //调用转化链表为红黑树的方法 treeifyBin(tab, i); //说明当前线程插入的数据key，与原有k-v发生冲突，需要将原数据v返回给调用者。 if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //1.统计当前table一共有多少数据 //2.判断是否达到扩容阈值标准，触发扩容。 addCount(1L, binCount); return null;&#125; #6 initTable数组初始化方法，这个方法比较简单，就是初始化一个合适大小的数组。 sizeCtl ：这个标志是在 Node 数组初始化或者扩容的时候的一个控制位标识，负数代表正在进行初始化或者扩容操作。 -1 代表正在初始化 -N 代表有 N-1 个线程正在进行扩容操作，这里不是简单的理解成 n 个线程，sizeCtl 就是-N 0 标识 Node 数组还没有被初始化，正数代表初始化或者下一次扩容的大小 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Initializes table, using the size recorded in sizeCtl. * * sizeCtl &lt; 0 * * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * * * sizeCtl &gt; 0 * * * * 1. 如果table未初始化，表示初始化大小 * * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */private final Node&lt;K,V&gt;[] initTable() &#123; //tab 引用map.table //sc sizeCtl的临时值 Node&lt;K,V&gt;[] tab; int sc; //自旋 条件：map.table 尚未初始化 while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0) //大概率就是-1，表示其它线程正在进行创建table的过程，当前线程没有竞争到初始化table的锁。 Thread.yield(); // lost initialization race; just spin //1.sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 //2.如果table未初始化，表示初始化大小 //3.如果table已经初始化，表示下次扩容时的 触发条件（阈值） else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; //这里为什么又要判断呢？ 防止其它线程已经初始化完毕了，然后当前线程再次初始化..导致丢失数据。 //条件成立，说明其它线程都没有进入过这个if块，当前线程就是具备初始化table权利了。 if ((tab = table) == null || tab.length == 0) &#123; //sc大于0 创建table时 使用 sc为指定大小，否则使用 16 默认值. int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; //最终赋值给 map.table table = tab = nt; //n &gt;&gt;&gt; 2 =&gt; 等于 1/4 n n - (1/4)n = 3/4 n =&gt; 0.75 * n //sc 0.75 n 表示下一次扩容时的触发条件。 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; //1.如果当前线程是第一次创建map.table的线程话，sc表示的是 下一次扩容的阈值 //2.表示当前线程 并不是第一次创建map.table的线程，当前线程进入到else if 块 时，将 //sizeCtl 设置为了-1 ，那么这时需要将其修改为 进入时的值。 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; #7 addCount 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124private final void addCount(long x, int check) &#123; //as 表示 LongAdder.cells //b 表示LongAdder.base //s 表示当前map.table中元素的数量 CounterCell[] as; long b, s; //条件一：true-&gt;表示cells已经初始化了，当前线程应该去使用hash寻址找到合适的cell 去累加数据 // false-&gt;表示当前线程应该将数据累加到 base //条件二：false-&gt;表示写base成功，数据累加到base中了，当前竞争不激烈，不需要创建cells // true-&gt;表示写base失败，与其他线程在base上发生了竞争，当前线程应该去尝试创建cells。 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; //有几种情况进入到if块中？ //1.true-&gt;表示cells已经初始化了，当前线程应该去使用hash寻址找到合适的cell 去累加数据 //2.true-&gt;表示写base失败，与其他线程在base上发生了竞争，当前线程应该去尝试创建cells。 //a 表示当前线程hash寻址命中的cell CounterCell a; //v 表示当前线程写cell时的期望值 long v; //m 表示当前cells数组的长度 int m; //true -&gt; 未竞争 false-&gt;发生竞争 boolean uncontended = true; //条件一：as == null || (m = as.length - 1) &lt; 0 //true-&gt; 表示当前线程是通过 写base竞争失败 然后进入的if块，就需要调用fullAddCount方法去扩容 或者 重试.. LongAdder.longAccumulate //条件二：a = as[ThreadLocalRandom.getProbe() &amp; m]) == null 前置条件：cells已经初始化了 //true-&gt;表示当前线程命中的cell表格是个空，需要当前线程进入fullAddCount方法去初始化 cell，放入当前位置. //条件三：!(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x) // false-&gt;取反得到false，表示当前线程使用cas方式更新当前命中的cell成功 // true-&gt;取反得到true,表示当前线程使用cas方式更新当前命中的cell失败，需要进入fullAddCount进行重试 或者 扩容 cells。 if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) ) &#123; fullAddCount(x, uncontended); //考虑到fullAddCount里面的事情比较累，就让当前线程 不参与到 扩容相关的逻辑了，直接返回到调用点。 return; &#125; if (check &lt;= 1) return; //获取当前散列表元素个数，这是一个期望值 s = sumCount(); &#125; //表示一定是一个put操作调用的addCount if (check &gt;= 0) &#123; //tab 表示map.table //nt 表示map.nextTable //n 表示map.table数组的长度 //sc 表示sizeCtl的临时值 Node&lt;K,V&gt;[] tab, nt; int n, sc; /** * sizeCtl &lt; 0 * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * sizeCtl &gt; 0 * * 1. 如果table未初始化，表示初始化大小 * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */ //自旋 //条件一：s &gt;= (long)(sc = sizeCtl) // true-&gt; 1.当前sizeCtl为一个负数 表示正在扩容中.. // 2.当前sizeCtl是一个正数，表示扩容阈值 // false-&gt; 表示当前table尚未达到扩容条件 //条件二：(tab = table) != null // 恒成立 true //条件三：(n = tab.length) &lt; MAXIMUM_CAPACITY // true-&gt;当前table长度小于最大值限制，则可以进行扩容。 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; //扩容批次唯一标识戳 //16 -&gt; 32 扩容 标识为：1000 0000 0001 1011 int rs = resizeStamp(n); //条件成立：表示当前table正在扩容 // 当前线程理论上应该协助table完成扩容 if (sc &lt; 0) &#123; //条件一：(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // true-&gt;说明当前线程获取到的扩容唯一标识戳 非 本批次扩容 // false-&gt;说明当前线程获取到的扩容唯一标识戳 是 本批次扩容 //条件二： JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs &lt;&lt; 16 ) + 1 // true-&gt; 表示扩容完毕，当前线程不需要再参与进来了 // false-&gt;扩容还在进行中，当前线程可以参与 //条件三：JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs&lt;&lt;16) + MAX_RESIZERS // true-&gt; 表示当前参与并发扩容的线程达到了最大值 65535 - 1 // false-&gt;表示当前线程可以参与进来 //条件四：(nt = nextTable) == null // true-&gt;表示本次扩容结束 // false-&gt;扩容正在进行中 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //前置条件：当前table正在执行扩容中.. 当前线程有机会参与进扩容。 //条件成立：说明当前线程成功参与到扩容任务中，并且将sc低16位值加1，表示多了一个线程参与工作 //条件失败：1.当前有很多线程都在此处尝试修改sizeCtl，有其它一个线程修改成功了，导致你的sc期望值与内存中的值不一致 修改失败 // 2.transfer 任务内部的线程也修改了sizeCtl。 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) //协助扩容线程，持有nextTable参数 transfer(tab, nt); &#125; //1000 0000 0001 1011 0000 0000 0000 0000 +2 =&gt; 1000 0000 0001 1011 0000 0000 0000 0010 //条件成立，说明当前线程是触发扩容的第一个线程，在transfer方法需要做一些扩容准备工作 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) //触发扩容条件的线程 不持有nextTable transfer(tab, null); s = sumCount(); &#125; &#125;&#125; #8. transferConcurrentHashMap 支持并发扩容，实现方式是，把 Node 数组进行拆分，让每个线程处理自己的区域，假设 table 数组总长度是 64，默认情况下，那么每个线程可以分到 16 个 bucket。然后每个线程处理的范围，按照倒序来做迁移。 通过 for 自循环处理每个槽位中的链表元素，默认 advace 为真，通过 CAS 设置 transferIndex属性值，并初始化 i 和 bound 值，i 指当前处理的槽位序号，bound 指需要处理的槽位边界，先处理槽位 31 的节点； （bound,i） =(16,31) 从 31 的位置往前推动。 每存在一个线程执行完扩容操作，就通过 cas 执行 sc-1。 接着判断(sc-2) !=resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT ; 如果相等，表示当前为整个扩容操作的 最后一个线程，那么意味着整个扩容操作就结束了；如果不相等，说明还得继续。 这么做的目的，一方面是防止不同扩容之间出现相同的 sizeCtl，另外一方面，还可以避免sizeCtl 的 ABA 问题导致的扩容重叠的情况。 扩容图解判断是否需要扩容，也就是当更新后的键值对总数 baseCount &gt;= 阈值 sizeCtl 时，进行rehash，这里面会有两个逻辑。 如果当前正在处于扩容阶段，则当前线程会加入并且协助扩容。 如果当前没有在扩容，则直接触发扩容操作。 扩容操作的核心在于数据的转移，在单线程环境下数据的转移很简单，无非就是把旧数组中的数据迁移到新的数组。但是这在多线程环境下，在扩容的时候其他线程也可能正在添加元素，这时又触发了扩容怎么办？可能大家想到的第一个解决方案是加互斥锁，把转移过程锁住，虽然是可行的解决方案，但是会带来较大的性能开销。因为互斥锁会导致所有访问临界区的线程陷入到阻塞状态，持有锁的线程耗时越长，其他竞争线程就会一直被阻塞，导致吞吐量较低。而且还可能导致死锁。 而 ConcurrentHashMap 并没有直接加锁，而是采用 CAS 实现无锁的并发同步策略，最精华的部分是它可以利用多线程来进行协同扩容。 它把 Node 数组当作多个线程之间共享的任务队列，然后通过维护一个指针来划分每个线程锁负责的区间，每个线程通过区间逆向遍历来实现扩容，一个已经迁移完的bucket会被替换为一个ForwardingNode节点，标记当前bucket已经被其他线程迁移完了。接下来分析一下它的源码实现。 fwd:这个类是个标识类，用于指向新表用的，其他线程遇到这个类会主动跳过这个类，因为这个类要么就是扩容迁移正在进行，要么就是已经完成扩容迁移，也就是这个类要保证线程安全，再进行操作。 advance:这个变量是用于提示代码是否进行推进处理，也就是当前桶处理完，处理下一个桶的标识。 finishing:这个变量用于提示扩容是否结束用的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; //n 表示扩容之前table数组的长度 //stride 表示分配给线程任务的步长 int n = tab.length, stride; // stride 固定为 16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range //条件成立：表示当前线程为触发本次扩容的线程，需要做一些扩容准备工作 //条件不成立：表示当前线程是协助扩容的线程.. if (nextTab == null) &#123; // initiating try &#123; //创建了一个比扩容之前大一倍的table @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; //赋值给对象属性 nextTable ，方便协助扩容线程 拿到新表 nextTable = nextTab; //记录迁移数据整体位置的一个标记。index计数是从1开始计算的。 transferIndex = n; &#125; //表示新数组的长度 int nextn = nextTab.length; //fwd 节点，当某个桶位数据处理完毕后，将此桶位设置为fwd节点，其它写线程 或读线程看到后，会有不同逻辑。 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); //推进标记 boolean advance = true; //完成标记 boolean finishing = false; // to ensure sweep before committing nextTab //i 表示分配给当前线程任务，执行到的桶位 //bound 表示分配给当前线程任务的下界限制 int i = 0, bound = 0; //自旋 for (;;) &#123; //f 桶位的头结点 //fh 头结点的hash Node&lt;K,V&gt; f; int fh; /** * 1.给当前线程分配任务区间 * 2.维护当前线程任务进度（i 表示当前处理的桶位） * 3.维护map对象全局范围内的进度 */ while (advance) &#123; //分配任务的开始下标 //分配任务的结束下标 int nextIndex, nextBound; //CASE1: //条件一：--i &gt;= bound //成立：表示当前线程的任务尚未完成，还有相应的区间的桶位要处理，--i 就让当前线程处理下一个 桶位. //不成立：表示当前线程任务已完成 或 者未分配 if (--i &gt;= bound || finishing) advance = false; //CASE2: //前置条件：当前线程任务已完成 或 者未分配 //条件成立：表示对象全局范围内的桶位都分配完毕了，没有区间可分配了，设置当前线程的i变量为-1 跳出循环后，执行退出迁移任务相关的程序 //条件不成立：表示对象全局范围内的桶位尚未分配完毕，还有区间可分配 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; //CASE3: //前置条件：1、当前线程需要分配任务区间 2.全局范围内还有桶位尚未迁移 //条件成立：说明给当前线程分配任务成功 //条件失败：说明分配给当前线程失败，应该是和其它线程发生了竞争吧 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; //CASE1： //条件一：i &lt; 0 //成立：表示当前线程未分配到任务 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; //保存sizeCtl 的变量 int sc; if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; //条件成立：说明设置sizeCtl 低16位 -1 成功，当前线程可以正常退出 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; //1000 0000 0001 1011 0000 0000 0000 0000 //条件成立：说明当前线程不是最后一个退出transfer任务的线程 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) //正常退出 return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; //前置条件：【CASE2~CASE4】 当前线程任务尚未处理完，正在进行中 //CASE2: //条件成立：说明当前桶位未存放数据，只需要将此处设置为fwd节点即可。 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //CASE3: //条件成立：说明当前桶位已经迁移过了，当前线程不用再处理了，直接再次更新当前线程任务索引，再次处理下一个桶位 或者 其它操作 else if ((fh = f.hash) == MOVED) advance = true; // already processed //CASE4: //前置条件：当前桶位有数据，而且node节点 不是 fwd节点，说明这些数据需要迁移。 else &#123; //sync 加锁当前桶位的头结点 synchronized (f) &#123; //防止在你加锁头对象之前，当前桶位的头对象被其它写线程修改过，导致你目前加锁对象错误... if (tabAt(tab, i) == f) &#123; //ln 表示低位链表引用 //hn 表示高位链表引用 Node&lt;K,V&gt; ln, hn; //条件成立：表示当前桶位是链表桶位 if (fh &gt;= 0) &#123; //lastRun //可以获取出 当前链表 末尾连续高位不变的 node int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; //条件成立：说明lastRun引用的链表为 低位链表，那么就让 ln 指向 低位链表 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; //否则，说明lastRun引用的链表为 高位链表，就让 hn 指向 高位链表 else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; //条件成立：表示当前桶位是 红黑树 代理结点TreeBin else if (f instanceof TreeBin) &#123; //转换头结点为 treeBin引用 t TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //低位双向链表 lo 指向低位链表的头 loTail 指向低位链表的尾巴 TreeNode&lt;K,V&gt; lo = null, loTail = null; //高位双向链表 lo 指向高位链表的头 loTail 指向高位链表的尾巴 TreeNode&lt;K,V&gt; hi = null, hiTail = null; //lc 表示低位链表元素数量 //hc 表示高位链表元素数量 int lc = 0, hc = 0; //迭代TreeBin中的双向链表，从头结点 至 尾节点 for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; // h 表示循环处理当前元素的 hash int h = e.hash; //使用当前节点 构建出来的 新的 TreeNode TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); //条件成立：表示当前循环节点 属于低位链 节点 if ((h &amp; n) == 0) &#123; //条件成立：说明当前低位链表 还没有数据 if ((p.prev = loTail) == null) lo = p; //说明 低位链表已经有数据了，此时当前元素 追加到 低位链表的末尾就行了 else loTail.next = p; //将低位链表尾指针指向 p 节点 loTail = p; ++lc; &#125; //当前节点 属于 高位链 节点 else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 链表迁移原理 1）高低位原理分析 ConcurrentHashMap 在做链表迁移时，会用高低位来实现，这里有两个问题要分析一下 1，如何实现高低位链表的区分 假如有这样一个队列第 14 个槽位插入新节点之后，链表元素个数已经达到了 8，且数组长度为 16，优先通过扩容来缓解链表过长的问题 假如当前线程正在处理槽位为 14 的节点，它是一个链表结构，在代码中，首先定义两个变量节点 ln 和 hn，实际就是 lowNode 和 HighNode，分别保存 hash 值的第 x 位为 0 和不等于0 的节点 通过 fn&amp;n 可以把这个链表中的元素分为两类，A 类是 hash 值的第 X 位为 0，B 类是 hash 值的第 x 位为不等于 0（至于为什么要这么区分，稍后分析），并且通过 lastRun 记录最后要处理的节点。最终要达到的目的是，A 类的链表保持位置不动，B 类的链表为 14+16(扩容增加的长度)=30 把 14 槽位的链表单独伶出来，用蓝色表示 fn&amp;n=0 的节点，假如链表的分类是这样 1234567for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125;&#125; 通过上面这段代码遍历，会记录 runBit 以及 lastRun，按照上面这个结构，那么 runBit 应该是蓝色节点，lastRun 应该是第 6 个节点接着，再通过这段代码进行遍历，生成 ln 链以及 hn 链 1234567for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn);&#125; 接着，通过 CAS 操作，把 hn 链放在 i+n 也就是 14+16 的位置，ln 链保持原来的位置不动。并且设置当前节点为 fwd，表示已经被当前线程迁移完了。 123setTabAt(nextTab, i, ln);setTabAt(nextTab, i + n, hn);setTabAt(tab, i, fwd); 迁移完成以后的数据分布如下 2)为什么要做高低位的划分 要想了解这么设计的目的，我们需要从 ConcurrentHashMap 的根据下标获取对象的算法来看，在 putVal 方法中 1018 行： (f = tabAt(tab, i = (n - 1) &amp; hash)) == null 通过(n-1) &amp; hash 来获得在 table 中的数组下标来获取节点数据，【&amp;运算是二进制运算符，1&amp; 1=1，其他都为 0】。 #9.helpTransfer如果对应的节点存在，判断这个节点的 hash 是不是等于 MOVED(-1)，说明当前节点是ForwardingNode 节点，意味着有其他线程正在进行扩容，那么当前现在直接帮助它进行扩容，因此调用 helpTransfer方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; //nextTab 引用的是 fwd.nextTable == map.nextTable 理论上是这样。 //sc 保存map.sizeCtl Node&lt;K,V&gt;[] nextTab; int sc; //条件一：tab != null 恒成立 true //条件二：(f instanceof ForwardingNode) 恒成立 true //条件三：((ForwardingNode&lt;K,V&gt;)f).nextTable) != null 恒成立 true if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; //拿当前标的长度 获取 扩容标识戳 假设 16 -&gt; 32 扩容：1000 0000 0001 1011 int rs = resizeStamp(tab.length); //条件一：nextTab == nextTable //成立：表示当前扩容正在进行中 //不成立：1.nextTable被设置为Null 了，扩容完毕后，会被设为Null // 2.再次出发扩容了...咱们拿到的nextTab 也已经过期了... //条件二：table == tab //成立：说明 扩容正在进行中，还未完成 //不成立：说明扩容已经结束了，扩容结束之后，最后退出的线程 会设置 nextTable 为 table //条件三：(sc = sizeCtl) &lt; 0 //成立：说明扩容正在进行中 //不成立：说明sizeCtl当前是一个大于0的数，此时代表下次扩容的阈值，当前扩容已经结束。 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; //条件一：(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // true-&gt;说明当前线程获取到的扩容唯一标识戳 非 本批次扩容 // false-&gt;说明当前线程获取到的扩容唯一标识戳 是 本批次扩容 //条件二： JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs &lt;&lt; 16 ) + 1 // true-&gt; 表示扩容完毕，当前线程不需要再参与进来了 // false-&gt;扩容还在进行中，当前线程可以参与 //条件三：JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs&lt;&lt;16) + MAX_RESIZERS // true-&gt; 表示当前参与并发扩容的线程达到了最大值 65535 - 1 // false-&gt;表示当前线程可以参与进来 //条件四：transferIndex &lt;= 0 // true-&gt;说明map对象全局范围内的任务已经分配完了，当前线程进去也没活干.. // false-&gt;还有任务可以分配。 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table;&#125; #10.get 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public V get(Object key) &#123; //tab 引用map.table //e 当前元素 //p 目标节点 //n table数组长度 //eh 当前元素hash //ek 当前元素key Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //扰动运算后得到 更散列的hash值 int h = spread(key.hashCode()); //条件一：(tab = table) != null //true-&gt;表示已经put过数据，并且map内部的table也已经初始化完毕 //false-&gt;表示创建完map后，并没有put过数据，map内部的table是延迟初始化的，只有第一次写数据时会触发创建逻辑。 //条件二：(n = tab.length) &gt; 0 true-&gt;表示table已经初始化 //条件三：(e = tabAt(tab, (n - 1) &amp; h)) != null //true-&gt;当前key寻址的桶位 有值 //false-&gt;当前key寻址的桶位中是null，是null直接返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //前置条件：当前桶位有数据 //对比头结点hash与查询key的hash是否一致 //条件成立：说明头结点与查询Key的hash值 完全一致 if ((eh = e.hash) == h) &#123; //完全比对 查询key 和 头结点的key //条件成立：说明头结点就是查询数据 if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; //条件成立： //1.-1 fwd 说明当前table正在扩容，且当前查询的这个桶位的数据 已经被迁移走了 //2.-2 TreeBin节点，需要使用TreeBin 提供的find 方法查询。 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; //当前桶位已经形成链表的这种情况 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; #11.remove 123public V remove(Object key) &#123; return replaceNode(key, null, null);&#125; #12.replaceNode 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143final V replaceNode(Object key, V value, Object cv) &#123; //计算key经过扰动运算后的hash int hash = spread(key.hashCode()); //自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //f表示桶位头结点 //n表示当前table数组长度 //i表示hash命中桶位下标 //fh表示桶位头结点 hash Node&lt;K,V&gt; f; int n, i, fh; //CASE1： //条件一：tab == null true-&gt;表示当前map.table尚未初始化.. false-&gt;已经初始化 //条件二：(n = tab.length) == 0 true-&gt;表示当前map.table尚未初始化.. false-&gt;已经初始化 //条件三：(f = tabAt(tab, i = (n - 1) &amp; hash)) == null true -&gt; 表示命中桶位中为null，直接break， 会返回 if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) &amp; hash)) == null) break; //CASE2： //前置条件CASE2 ~ CASE3：当前桶位不是null //条件成立：说明当前table正在扩容中，当前是个写操作，所以当前线程需要协助table完成扩容。 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); //CASE3: //前置条件CASE2 ~ CASE3：当前桶位不是null //当前桶位 可能是 &quot;链表&quot; 也可能 是 &quot;红黑树&quot; TreeBin else &#123; //保留替换之前的数据引用 V oldVal = null; //校验标记 boolean validated = false; //加锁当前桶位 头结点，加锁成功之后会进入 代码块。 synchronized (f) &#123; //判断sync加锁是否为当前桶位 头节点，防止其它线程，在当前线程加锁成功之前，修改过 桶位 的头结点。 //条件成立：当前桶位头结点 仍然为f，其它线程没修改过。 if (tabAt(tab, i) == f) &#123; //条件成立：说明桶位 为 链表 或者 单个 node if (fh &gt;= 0) &#123; validated = true; //e 表示当前循环处理元素 //pred 表示当前循环节点的上一个节点 Node&lt;K,V&gt; e = f, pred = null; for (;;) &#123; //当前节点key K ek; //条件一：e.hash == hash true-&gt;说明当前节点的hash与查找节点hash一致 //条件二：((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) //if 条件成立，说明key 与查询的key完全一致。 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; //当前节点的value V ev = e.val; //条件一：cv == null true-&gt;替换的值为null 那么就是一个删除操作 //条件二：cv == ev || (ev != null &amp;&amp; cv.equals(ev)) 那么是一个替换操作 if (cv == null || cv == ev || (ev != null &amp;&amp; cv.equals(ev))) &#123; //删除 或者 替换 //将当前节点的值 赋值给 oldVal 后续返回会用到 oldVal = ev; //条件成立：说明当前是一个替换操作 if (value != null) //直接替换 e.val = value; //条件成立：说明当前节点非头结点 else if (pred != null) //当前节点的上一个节点，指向当前节点的下一个节点。 pred.next = e.next; else //说明当前节点即为 头结点，只需要将 桶位设置为头结点的下一个节点。 setTabAt(tab, i, e.next); &#125; break; &#125; pred = e; if ((e = e.next) == null) break; &#125; &#125; //条件成立：TreeBin节点。 else if (f instanceof TreeBin) &#123; validated = true; //转换为实际类型 TreeBin t TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //r 表示 红黑树 根节点 //p 表示 红黑树中查找到对应key 一致的node TreeNode&lt;K,V&gt; r, p; //条件一：(r = t.root) != null 理论上是成立 //条件二：TreeNode.findTreeNode 以当前节点为入口，向下查找key（包括本身节点） // true-&gt;说明查找到相应key 对应的node节点。会赋值给p if ((r = t.root) != null &amp;&amp; (p = r.findTreeNode(hash, key, null)) != null) &#123; //保存p.val 到pv V pv = p.val; //条件一：cv == null 成立：不必对value，就做替换或者删除操作 //条件二：cv == pv ||(pv != null &amp;&amp; cv.equals(pv)) 成立：说明“对比值”与当前p节点的值 一致 if (cv == null || cv == pv || (pv != null &amp;&amp; cv.equals(pv))) &#123; //替换或者删除操作 oldVal = pv; //条件成立：替换操作 if (value != null) p.val = value; //删除操作 else if (t.removeTreeNode(p)) //这里没做判断，直接搞了...很疑惑 setTabAt(tab, i, untreeify(t.first)); &#125; &#125; &#125; &#125; &#125; //当其他线程修改过桶位 头结点时，当前线程 sync 头结点 锁错对象时，validated 为false，会进入下次for 自旋 if (validated) &#123; if (oldVal != null) &#123; //替换的值 为null，说明当前是一次删除操作，oldVal ！=null 成立，说明删除成功，更新当前元素个数计数器。 if (value == null) addCount(-1L, -1); return oldVal; &#125; break; &#125; &#125; &#125; return null;&#125; #13.TreeBin##13.1 属性 1234567891011121314151617//红黑树 根节点 TreeNode&lt;K,V&gt; root;//链表的头节点volatile TreeNode&lt;K,V&gt; first;//等待者线程（当前lockState是读锁状态）volatile Thread waiter;/** * 1.写锁状态 写是独占状态，以散列表来看，真正进入到TreeBin中的写线程 同一时刻 只有一个线程。 1 * 2.读锁状态 读锁是共享，同一时刻可以有多个线程 同时进入到 TreeBin对象中获取数据。 每一个线程 都会给 lockStat + 4 * 3.等待者状态（写线程在等待），当TreeBin中有读线程目前正在读取数据时，写线程无法修改数据，那么就将lockState的最低2位 设置为 0b 10 */volatile int lockState;// values for lockStatestatic final int WRITER = 1; // set while holding write lockstatic final int WAITER = 2; // set when waiting for write lockstatic final int READER = 4; // increment value for setting read lock ##13.2 构造器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586TreeBin(TreeNode&lt;K,V&gt; b) &#123; //设置节点hash为-2 表示此节点是TreeBin节点 super(TREEBIN, null, null, null); //使用first 引用 treeNode链表 this.first = b; //r 红黑树的根节点引用 TreeNode&lt;K,V&gt; r = null; //x表示遍历的当前节点 for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; //强制设置当前插入节点的左右子树为null x.left = x.right = null; //条件成立：说明当前红黑树 是一个空树，那么设置插入元素 为根节点 if (r == null) &#123; //根节点的父节点 一定为 null x.parent = null; //颜色改为黑色 x.red = false; //让r引用x所指向的对象。 r = x; &#125; else &#123; //非第一次循环，都会来带else分支，此时红黑树已经有数据了 //k 表示 插入节点的key K k = x.key; //h 表示 插入节点的hash int h = x.hash; //kc 表示 插入节点key的class类型 Class&lt;?&gt; kc = null; //p 表示 为查找插入节点的父节点的一个临时节点 TreeNode&lt;K,V&gt; p = r; for (;;) &#123; //dir (-1, 1) //-1 表示插入节点的hash值大于 当前p节点的hash //1 表示插入节点的hash值 小于 当前p节点的hash //ph p表示 为查找插入节点的父节点的一个临时节点的hash int dir, ph; //临时节点 key K pk = p.key; //插入节点的hash值 小于 当前节点 if ((ph = p.hash) &gt; h) //插入节点可能需要插入到当前节点的左子节点 或者 继续在左子树上查找 dir = -1; //插入节点的hash值 大于 当前节点 else if (ph &lt; h) //插入节点可能需要插入到当前节点的右子节点 或者 继续在右子树上查找 dir = 1; //如果执行到 CASE3，说明当前插入节点的hash 与 当前节点的hash一致，会在case3 做出最终排序。最终 //拿到的dir 一定不是0，（-1， 1） else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); //xp 想要表示的是 插入节点的 父节点 TreeNode&lt;K,V&gt; xp = p; //条件成立：说明当前p节点 即为插入节点的父节点 //条件不成立：说明p节点 底下还有层次，需要将p指向 p的左子节点 或者 右子节点，表示继续向下搜索。 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; //设置插入节点的父节点 为 当前节点 x.parent = xp; //小于P节点，需要插入到P节点的左子节点 if (dir &lt;= 0) xp.left = x; //大于P节点，需要插入到P节点的右子节点 else xp.right = x; //插入节点后，红黑树性质 可能会被破坏，所以需要调用 平衡方法 r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; //将r 赋值给 TreeBin对象的 root引用。 this.root = r; assert checkInvariants(root);&#125; ##13.3 putTreeVal 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970final TreeNode&lt;K,V&gt; putTreeVal(int h, K k, V v) &#123; Class&lt;?&gt; kc = null; boolean searched = false; for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; K pk; if (p == null) &#123; first = root = new TreeNode&lt;K,V&gt;(h, k, v, null, null); break; &#125; else if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) &#123; if (!searched) &#123; TreeNode&lt;K,V&gt; q, ch; searched = true; if (((ch = p.left) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null) || ((ch = p.right) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null)) return q; &#125; dir = tieBreakOrder(k, pk); &#125; TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; //当前循环节点xp 即为 x 节点的爸爸 //x 表示插入节点 //f 老的头结点 TreeNode&lt;K,V&gt; x, f = first; first = x = new TreeNode&lt;K,V&gt;(h, k, v, f, xp); //条件成立：说明链表有数据 if (f != null) //设置老的头结点的前置引用为 当前的头结点。 f.prev = x; if (dir &lt;= 0) xp.left = x; else xp.right = x; if (!xp.red) x.red = true; else &#123; //表示 当前新插入节点后，新插入节点 与 父节点 形成 “红红相连” lockRoot(); try &#123; //平衡红黑树，使其再次符合规范。 root = balanceInsertion(root, x); &#125; finally &#123; unlockRoot(); &#125; &#125; break; &#125; &#125; assert checkInvariants(root); return null;&#125; ##13.4 find 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748final Node&lt;K,V&gt; find(int h, Object k) &#123; if (k != null) &#123; //e 表示循环迭代的当前节点 迭代的是first引用的链表 for (Node&lt;K,V&gt; e = first; e != null; ) &#123; //s 保存的是lock临时状态 //ek 链表当前节点 的key int s; K ek; //(WAITER|WRITER) =&gt; 0010 | 0001 =&gt; 0011 //lockState &amp; 0011 != 0 条件成立：说明当前TreeBin 有等待者线程 或者 目前有写操作线程正在加锁 if (((s = lockState) &amp; (WAITER|WRITER)) != 0) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; e = e.next; &#125; //前置条件：当前TreeBin中 等待者线程 或者 写线程 都没有 //条件成立：说明添加读锁成功 else if (U.compareAndSwapInt(this, LOCKSTATE, s, s + READER)) &#123; TreeNode&lt;K,V&gt; r, p; try &#123; //查询操作 p = ((r = root) == null ? null : r.findTreeNode(h, k, null)); &#125; finally &#123; //w 表示等待者线程 Thread w; //U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) //1.当前线程查询红黑树结束，释放当前线程的读锁 就是让 lockstate 值 - 4 //(READER|WAITER) = 0110 =&gt; 表示当前只有一个线程在读，且“有一个线程在等待” //当前读线程为 TreeBin中的最后一个读线程。 //2.(w = waiter) != null 说明有一个写线程在等待读操作全部结束。 if (U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) &amp;&amp; (w = waiter) != null) //使用unpark 让 写线程 恢复运行状态。 LockSupport.unpark(w); &#125; return p; &#125; &#125; &#125; return null;&#125; #总结在java8中，ConcurrentHashMap使用数组+链表+红黑树的组合方式，利用cas和synchronized保证并发写的安全。 引入红黑树的原因：链表查询的时间复杂度为On，但是红黑树的查询时间复杂度为O(log(n)),所以在节点比较多的情况下，使用红黑树可以大大提升性能。 链式桶是一个由node节点组成的链表。树状桶是一颗由TreeNode节点组成的红黑树。输的根节点为TreeBin类型。 当链表长度大于8整个hash表长度大于64的时候，就会转化为TreeBin。TreeBin作为根节点，其实就是红黑树对象。在ConcurrentHashMap的table数组中，存放的就是TreeBin对象，而不是TreeNoe对象。 数组table是懒加载的，只有第一次添加元素的时候才会初始化，所以initTable()存在线程安全问题。 重要的属性就是sizeCtl，用来控制table的初始化和扩容操作的过程： ● -1代表table正在初始化，其他线程直接join等待。 ● -N代表有N-1个线程正在进行扩容操作，严格来说，当其为负数的时候，只用到了低16位，如果低16位为M，此时有M-1个线程进行扩容。 ● 大于0有两种情况：如果table没有初始化，她就表示table初始化的大小，如果table初始化完了，就表示table的容量，默认是table大小的四分之三。 Transfer()扩容 table数据转移到nextTable。扩容操作的核心在于数据的转移，把旧数组中的数据迁移到新的数组。ConcurrentHashMap精华的部分是它可以利用多线程来进行协同扩容，简单来说，它把table数组当作多个线程之间共享的任务队列，然后通过维护一个指针来划分每个线程所负责的区间，每个线程通过区间逆向遍历来实现扩容，一个已经迁移完的 Bucket会被替换为一个Forwarding节点，标记当前Bucket已经被其他线程迁移完了。 helpTransfer()帮助扩容 ConcurrentHashMap并发添加元素时，如果正在扩容，其他线程会帮助扩容，也就是多线程扩容。 第一次添加元素时，默认初始长度为16，当往table中继续添加元素时，通过Hash值跟数组长度取余来决定放在数组的哪个Bucket位置，如果出现放在同一个位置，就优先以链表的形式存放，在同一个位置的个数达到了8个以上，如果数组的长度还小于64，就会扩容数组。如果数组的长度大于等于64，就会将该节点的链表转换成树。 通过扩容数组的方式来把这些节点分散开。然后将这些元素复制到扩容后的新数组中，同一个Bucket中的元素通过Hash值的数组长度位来重新确定位置，可能还是放在原来的位置，也可能放到新的位置。而且，在扩容完成之后，如果之前某个节点是树，但是现在该节点的“Key-Value对”数又小于等于6个，就会将该树转为链表。 put() JDK1.8在使用CAS自旋完成桶的设置时，使用synchronized内置锁保证桶内并发操作的线程安全。尽管对同一个Map操作的线程争用会非常激烈，但是在同一个桶内的线程争用通常不会很激烈，所以使用CAS自旋、synchronized不会降低ConcurrentHashMap的性能。为什么不用ReentrantLock显式锁呢?如果为每 个桶都创建一个ReentrantLock实 例，就会带来大量的内存消耗，反过来，使用CAS自旋、synchronized，内存消耗的增加更小。 get() get()通过UnSafe的getObjectVolatile()来读取数组中的元素。为什么要这样做?虽然HashEntry数组的引用是volatile类型，但是数组内元素的 用不是volatile类型，因此多线程对 数组元素的修改是不安全的，可能会在数组中读取到尚未构造完成的元素对象。get()方法通过UnSafe的getObjectVolatile方法来保证元素的读取安全，调用getObjectVolatile()去读取数组元素需要先获得元素在数组中的偏移量，在这里，get()方法根据哈希码计算出偏移量为u，然后通过偏移量u来尝试读取数值。","categories":[{"name":"1.基础知识","slug":"1-基础知识","permalink":"https://zhangxin66666.github.io/categories/1-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"集合源码分析","slug":"集合源码分析","permalink":"https://zhangxin66666.github.io/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]},{"title":"mysql执行流程","slug":"7.mysql/mysql执行流程","date":"2022-04-01T16:00:00.000Z","updated":"2024-08-22T03:23:14.706Z","comments":true,"path":"2022/04/02/7.mysql/mysql执行流程/","link":"","permalink":"https://zhangxin66666.github.io/2022/04/02/7.mysql/mysql%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/","excerpt":"","text":"一、MySQL的内部组件结构 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 Server层 主要包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数 （如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 Store层 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在 最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。也就是说如果我们在create table时不指定 表的存储引擎类型,默认会给你设置存储引擎为InnoDB。 本节课演示表的DDL： 123456789CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8; 下面我们重点来分析连接器、查询缓存、分析器、优化器、执行器分别主要干了哪些事情。 二、连接器我们知道由于MySQL是开源的，他有非常多种类的客户端：navicat,mysql front,jdbc,SQLyog等非常丰富的客户端,这些 客户端要向mysql发起通信都必须先跟Server端建立通信连接，而建立连接的工作就是有连接器完成的。 第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管 理连接。连接命令一般是这么写的： 1[root@192 ~]# mysql ‐h host[数据库地址] ‐u root[用户] ‐p root[密码] ‐P 3306 连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份， 这个时候用的就是你输入的用户名和密码。 1、如果用户名或密码不对，你就会收到一个”Access denied for user”的错误，然后客户端程序结束执行。 2、如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权 限。 这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权 限。修改完成后，只有再新建的连接才会使用新的权限设置。用户的权限表在系统表空间的mysql的user表中。 修改user密码 123456789mysql&gt; CREATE USER &#x27;username&#x27;@&#x27;host&#x27; IDENTIFIED BY &#x27;password&#x27;; //创建新用户 mysql&gt; grant all privileges on *.* to &#x27;username&#x27;@&#x27;%&#x27;; //赋权限,%表示所有(host) mysql&gt; flush privileges //刷新数据库 mysql&gt; update user set password=password(”123456″) where user=’root’;(设置用户名密码) mysql&gt; show grants for root@&quot;%&quot;; 查看当前用户的权限 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。文本中这个 图是 show processlist 的结果，其中的 Command 列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。 客户端如果长时间不发送command到Server端，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值 是 8 小时。 查看wait_timeout 123mysql&gt; show global variables like &quot;wait_timeout&quot;; mysql&gt;set global wait_timeout=28800; 设置全局服务器关闭非交互连接之前等待活动的秒数 如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次 查询就断开连接，下次查询再重新建立一个。 开发当中我们大多数时候用的都是长连接,把连接放在Pool内进行管理，但是长连接有些时候会导致 MySQL 占用内存涨得特别 快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如 果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。 怎么解决这类问题呢？ 1、定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 2、如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资 源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 三、查询缓存常用的一些操作 1234567mysql&gt;show databases; 显示所有数据库 mysql&gt;use dbname； 打开数据库： mysql&gt;show tables; 显示数据库mysql中所有的表； mysql&gt;describe user; 显示表mysql数据库中user表的列信息）； 连接建立完成后，你就可以执行 select 语句了。执行逻辑就会来到第二步：查询缓存。 MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找 到 key，那么这个 value 就会被直接返回给客户端。 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查 询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。 大多数情况查询缓存就是个鸡肋，为什么呢？ 因为查询缓存往往弊大于利。查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。 因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率 会非常低。 一般建议大家在静态表里使用查询缓存，什么叫静态表呢？就是一般我们极少更新的表。比如，一个系统配置表、字典 表，那这张表上的查询才适合使用查询缓存。好在 MySQL 也提供了这种“按需使用”的方式。你可以将my.cnf参数 query_cache_type 设置成 DEMAND。 1234567 my.cnf #query_cache_type有3个值 0代表关闭查询缓存OFF，1代表开启ON，2（DEMAND）代表当sql语句中有SQL_CACHE 关键词时才缓存 query_cache_type=2 这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，像下 面这个语句一样： 1mysql&gt; select SQL_CACHE * from test where ID=5; 查看当前mysql实例是否开启缓存机制 1mysql&gt; show global variables like &quot;%query_cache_type%&quot;; 监控查询缓存的命中率: 1mysql&gt; show status like&#x27;%Qcache%&#x27;; //查看运行的缓存信息 Qcache_free_blocks:表示查询缓存中目前还有多少剩余的blocks，如果该值显示较大，则说明查询缓存中的内存碎片 过多了，可能在一定的时间进行整理。 Qcache_free_memory:查询缓存的内存大小，通过这个参数可以很清晰的知道当前系统的查询内存是否够用，是多 了，还是不够用，DBA可以根据实际情况做出调整。 Qcache_hits:表示有多少次命中缓存。我们主要可以通过该值来验证我们的查询缓存的效果。数字越大，缓存效果越 理想。 Qcache_inserts: 表示多少次未命中然后插入，意思是新来的SQL请求在缓存中未找到，不得不执行查询处理，执行 查询处理后把结果insert到查询缓存中。这样的情况的次数，次数越多，表示查询缓存应用到的比较少，效果也就不理 想。当然系统刚启动后，查询缓存是空的，这很正常。 Qcache_lowmem_prunes:该参数记录有多少条查询因为内存不足而被移除出查询缓存。通过这个值，用户可以适当的 调整缓存大小。 Qcache_not_cached: 表示因为query_cache_type的设置而没有被缓存的查询数量。 Qcache_queries_in_cache:当前缓存中缓存的查询数量。 Qcache_total_blocks:当前缓存的block数量。 mysql8.0已经移除了查询缓存功能 四、分析器如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。 分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是 什么，代表什么。 MySQL 从你输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符 串“ID”识别成“列 ID”。 做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句 是否满足 MySQL 语法。 如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句 from 写成了 “rom”。 12345 mysql&gt; select * fro test where id=1; ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds t o your MySQL server version for the right syntax to use near &#x27;fro test where id=1&#x27; at line 1 词法分析器原理 词法分析器分成6个主要步骤完成对sql语句的分析 1、词法分析 2、语法分析 3、语义分析 4、构造执行树 5、生成执行计划 6、计划的执行 下图是SQL词法分析的过程步骤： SQL语句的分析分为词法分析与语法分析，mysql的词法分析由MySQLLex[MySQL自己实现的]完成，语法分析由Bison生 成。关于语法树大家如果想要深入研究可以参考这篇wiki文章：https://en.wikipedia.org/wiki/LR_parser。那么除了Bison 外，Java当中也有开源的词法结构分析工具例如Antlr4，ANTLR从语法生成一个解析器，可以构建和遍历解析树，可以在IDEA 工具当中安装插件：antlr v4 grammar plugin。插件使用详见课程 经过bison语法分析之后，会生成一个这样的语法树 至此我们分析器的工作任务也基本圆满了。接下来进入到优化器 五、优化器经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接 顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join： 123 mysql&gt; select * from test1 join test2 using(ID) where test1.name=yangguo and test2.name=xiaol ongnv; 既可以先从表 test1 里面取出 name=yangguo的记录的 ID 值，再根据 ID 值关联到表 test2，再判断 test2 里面 name的 值是否等于 yangguo。 也可以先从表 test2 里面取出 name=xiaolongnv 的记录的 ID 值，再根据 ID 值关联到 test1，再判断 test1 里面 name 的值是否等于 yangguo。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。优化器阶段 完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。如果你还有一些疑问，比如优化器是怎么选择索引的，有 没有可能选择错等等。 六、执行器开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在 工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权 限)。 1mysql&gt; select * from test where id=1; 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 比如我们这个例子中的表 test 中，ID 字段没有索引，那么执行器的执行流程是这样的： \\1. 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； \\2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 \\3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接 口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加 的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。","categories":[{"name":"7.mysql","slug":"7-mysql","permalink":"https://zhangxin66666.github.io/categories/7-mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"}]},{"title":"面试知识点总结","slug":"12.面试/常见面试题","date":"2022-03-30T16:00:00.000Z","updated":"2024-12-24T12:17:43.392Z","comments":true,"path":"2022/03/31/12.面试/常见面试题/","link":"","permalink":"https://zhangxin66666.github.io/2022/03/31/12.%E9%9D%A2%E8%AF%95/%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"","text":"一、基础知识1.ConcurrentHashMapConcurrentHashMap 2.Set,List,Map有什么区别 结构特点 List 和 Set 是存储单列数据的集合， Map 是存储键和值这样的双列数据的集合; List中存储的数据是有顺序，并且允许重复; Map 中存储的数据是没有顺序的，其键是不能重复的，它的值是可以有重复的， Set 中存储的数据是无序的，且不允许有重复，但元素在集合中的位置由元素的hashcode决定，位置是固定的(Set集合根据 hashcode 来进行数据的存储，所以位置是固定的，但是位置不是用户可以控制的，所以对于用户来说 set 中的元素还是无序的); 3.HashMap和HashTable有什么区别？区别： HashMap方法没有synchronized修饰，线程非安全，HashTable线程安全； HashMap允许key和value为null，而HashTable不允许 4.HashMap底层实现 jdk1.7底层采用的是数组+链表实现；jdk1.8底层采用数组+链表+红黑树实现，相对于1.7提升了查询的性能 链表过度到红黑树的阈值为8，红黑树退化为链表的阈值是6，原因在源码中332行给了明确解释(Poisson distribution)泊松分布，这是数学里面类似于正态分布的一个问题，这是统计学里面的一些东西 HashMap初始化的数组长度为16(算法导论中的除法散列法讲到 K取模m得到一个对应的位置，根据具体位置插入一个数据值，假如长度是16，长度范围就是015，取模之后我的余数为015之间，而算法导论中指出，一个不太接近2的整数幂的素数（质数：只能被1和它本身整出的数），常常是m的最好选择。但是hashMap中未采用素数，而是采用2的n次方(合数)作为数组长度)，原因为：1、更便于位置计算；2、方便做数据迁移。 为了保证存入的数据均匀的散列分布在数组中，需要设计良好的hash散列算法(hashMap中采用扰动函数来做的：移位和异或相关操作ConcurrentHashMap源码中684行spread方法，即：让int的高16位异或它本身)，尽可能的避免hash冲突(概率问题)，具体落到数组哪个节点并非是通过取模运算得出，而是通过与上数组长度-1即可(只需要int类型数32位的后四位参与运算)，即可得到0~15之间的数，因为取模运算效率远低于位运算，所以这也是hashMap要求底层数组长度必须为2的n次方的原因之一（ConcurrentHashMap源码中514行规定）。 5.HashMap put流程 HashMap/ConcurrentHashMap 并不是通过构造函数创建默认空间的，而是通过put数据的时候获取到对应的数据空间的，如果数组长度是0，则通过CAS操作后初始化数组，如果有线程正在做初始化数组操作，其他线程则让出时间片 6.HashMap扩容流程 hashMap规定了当我的数组快放满的时候就要开始扩容了，什么时候算是快放满？HashMap是通过扩容因子来规定的 HashMap规定扩容因子是0.75，如果默认长度是16，也就是说当HashMap底层数组的容量达到12的时候进行扩容操作。0.75则是根据统计学得来的。private static final float LOAD_FACTOR = 0.75f; 扩容首先要创建新的数组（原来大小左移1位）ConcurrentHaspMap源码2367行开始扩容操作 转移旧数据到新的数组中去（怎么计算扩容后数据下标位置？）原来：h&amp;(n-1)计算下标位置，扩容后 h$(31)，原来占用4个二进制位，扩容后占用5个二进制位，所以只需要看第五位即可，如果第五位是0，扩容后的的下标跟原位置一样，如果是1，新下标位置在原数组的位置的基础上加上原来数组长度即可，这也是数组长度采用2的N次方扩容的第二个原因 假设原来长度是128，扩容后是256，整体扩容方式是通过多线程方式运行的，但是要保证数据不能乱，将原来数组分段，规定每个线程最少负责16个桶的迁移工作，8个线程可以并行执行，如果小于16个桶，直接单线程执行 7.为什么选择用红黑树二叉树在极端情况下会退化为链表，查询时间复杂度跟链表相似，而AVL树，SB树，红黑树，都属于平衡二叉树，尽量保持左子树和右子树的高度差不要相差太大，而这三种树的差别就在于左树和右树的高度规则不同。 AVL树：严格意义平衡树，的要求左子树和右子树的高度差不能超过1，损失了部分插入性能，带来了高效的查询 SB树： 红黑树：要求最长子树不能超过最短子树的2倍即可，损失了部分查询性能，使得查询效率高于链表的同时，相比于其他树提升了插入性能，尽量做到了插入和查询的一个平衡点，而HashMap则是查多，插入少(这里的插入指的是产生Hash冲突下的插入)，所以红黑树更适合HashMap来做底层的存储结构。 8.面向对象的特征?面向对象的特征:封装、继承、多态、抽象。 封装:就是把对象的属性和行为(数据)结合为一个独立的整体，并尽可能隐藏对象的内 部实现细节，就是把不想告诉或者不该告诉别人的东西隐藏起来，把可以告诉别人的公开，别人只能用我提供的功能实现需求，而不知道是如何实现的。增加安全性。 继承:子类继承父类的数据属性和行为，并能根据自己的需求扩展出新的行为，提高了代 码的复用性。 多态:指允许不同的对象对同一消息做出相应。即同一消息可以根据发送对象的不同而采 用多种不同的行为方式(发送消息就是函数调用)。封装和继承几乎都是为多态而准备 的，在执行期间判断引用对象的实际类型，根据其实际的类型调用其相应的方法。 抽象:表示对问题领域进行分析、设计中得出的抽象的概念，是对一系列看上去不同，但是本质上相同的具体概念的抽象。在 Java 中抽象用 abstract 关键字来修饰，用abstract修饰类时，此类就不能被实例化，从这里可以看出，抽象类(接口)就是为了继承而存在的。 9.常见的RuntimeException java.lang.NullPointerException 空指针异常;出现原因:调用了未经初始化的对象或者是不存在 的对象。 java.lang.ClassNotFoundException 指定的类找不到;出现原因:类的名称和路径加载错误;通常 都是程序试图通过字符串来加载某个类时可能引发异常。 java.lang.NumberFormatException 字符串转换为数字异常;出现原因:字符型数据中包含非数 字型字符。 java.lang.IndexOutOfBoundsException 数组角标越界异常，常见于操作数组对象时发生。 java.lang.IllegalArgumentException 方法传递参数错误。 java.lang.ClassCastException 数据类型转换异常。 java.lang.NoClassDefFoundException 未找到类定义错误。 SQLException SQL 异常，常见于操作数据库时的 SQL 语句错误。 java.lang.InstantiationException实例化异常。 java.lang.NoSuchMethodException方法不存在异常。 10.常见的引用类型java的引用类型一般分为四种：强引用、软引用、弱引用、虚引用 强引用：普通的变量引用 软引用：将对象用SoftReference软引用类型的对象包裹，正常情况不会被回收，但是GC做完后发现释放不出空间存放新的对象，则会把这些软引用的对象回收掉。软引用可用来实现内存敏感的高速缓存。 弱引用：将对象用WeakReference软引用类型的对象包裹，弱引用跟没引用差不多，GC会直接回收掉，很少用 虚引用：虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系，几乎不用 四、缓存:Redis五、MQ消息队列 Rabbitmq生产高可用怎么实现的 生产部署为集群模式：避免单机模式下MQ服务器挂了导致服务不可用，还可以承载更高的并发量，但是集群模式有个问题，就是在哪个节点上创建队列，该队列只会存在该节点上，其他集群节点不会备份该队列，一旦该节点宕机，该队列中的消息就会丢失(亲测设置持久化也丢失) 使用镜像队列：可以解决集群模式下，每个机器上只有一个队列的问题，让消息在其他节点再备份一份。开启方式：任何节点添加policy策略即可，该集群就具有镜像队列能力，可设置备份的份数和备份队列规则，即使其中一个节点宕机，也会保证整个集群中保存2份 高可用负载均衡：haproxy+keepalive，nginx，lvs等实现高可用负载均衡 其他：Federation Exchange联邦交换机插件解决两地接受消息确认消息延迟问题，也可以用Shovel来做数据同步，需要安装插件 六、多线程死锁 死锁：是指一组互相竞争资源的线程，因为互相等待，导致永久阻塞的现象。 原因：必须同时满足以下四个条件 共享互斥条件：共享资源x和y只能被一个线程占用 占有且等待：线程t1已经占有共享资源x，在等待共享资源y的时候不释放共享资源x 不可抢占：其他线程不能强行抢占线程t1占有的资源 循环等待：线程t1等待线程t2占有的资源，线程t2等待线程t1占有的资源 如何避免死锁： 既然产生死锁必然满足四个条件，那我们只要打破四个条件中的一个就可以避免，第一个互斥条件是无法被破坏的，因为锁本身就是通过互斥来解决线程安全的 针对后三个条件，我们逐一分析，占有且等待这个条件我们可以一次性申请所有资源，不存在等待； 不可抢占这个条件：占有部分资源的线程进一步申请其他资源的时候如果申请不到，可以主动释放已占有的资源，这样不可抢占这条件就破坏了； 循环等待这个条件：可以按照顺序去申请资源进行预防，就是说资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，线性化之后，就不存在循环等待了 sleep和wait区别 sleep 是 Thread 类的静态本地方法，wait 则是 Object 类的本地方法 sleep方法不会释放锁，但是wait会释放，而且会加入到等待队列中 sleep方法不依赖于同步器synchronized，但是wait需要依赖synchronized关键字 sleep不需要被唤醒（休眠之后推出阻塞），但是wait需要（不指定时间需要被别人中断） sleep 一般用于当前线程休眠，或者轮循暂停操作，wait 则多用于多线程之间的通信 sleep 会让出 CPU 执行时间且强制上下文切换，而 wait 则不一定，wait 后可能还是有机会重新竞争到锁继续执行的。 sleep就是把cpu的执行资格和执行权释放出去，不再运行此线程，当定时时间结束再取回cpu资源，参与cpu的调度，获取到cpu资源后就可以继续运行了。而如果sleep时该线程有锁，那么sleep不会释放这个锁，而是把锁带着进入了冻结状态，也就是说其他需要这个锁的线程根本不可能获取到这个锁。也就是说无法执行程序。如果在睡眠期间其他线程调用了这个线程的interrupt方法，那么这个线程也会抛出interruptexception异常返回，这点和wait是一样的。 当我们调用wait（）方法后，线程会放到等待池当中，等待池的线程是不会去竞争同步锁。只有调用了notify（）或notifyAll()后等待池的线程才会开始去竞争锁，notify（）是随机从等待池选出一个线程放到锁池，而notifyAll()是将等待池的所有线程放到锁池当中 Volitile作用 可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。内存屏障-&gt;汇编Lock关键字-&gt;缓存一致性协议 原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性 有序性：对volatile修饰的变量的读写操作前后加上各种特定的内存屏障来禁止指令重排序来保障有序性 Volitile如何保证可见性 缓存一致性：在共享内存多处理器系统中，每个处理器都有一个单独的缓存内存，共享数据在主内存和各处理器私有缓存同时存在。当数据的一个副本发生更改，其他副本需要感知到最新修改数据。缓存一致性的手段主要有： 总线锁定：总线锁定就是使用处理器提供的一个 LOCK＃信号，当其中一个处理器在总线上输出此信号时，其它处理器的请求将被阻塞，那么该处理器可以独占共享内存。 总线窥探(Bus snooping)：是缓存中的一致性控制器窥探总线事务的一种方案（该方案由Ravishankar和Goodman于1983年提出）当数据被多个缓存共享时，一个处理器修改了共享数据的值，可以是刷新缓存块或使缓存块失效还可以通过缓存块状态的改变，来达到缓存一致性的目的，主要取决于窥探协议类型： 写失效（Write-invalidate） ：当一个处理器写入共享缓存时，其他缓存中的所有共享副本都会通过总线窥探失效。确保处理器只能读写一个数据副本，其他缓存中的所有其他副本都无效。这是最常用的窥探协议。MSI、MESI、MOSI、MOESI和MESIF协议属于该类型。 写更新（Write-update）：当处理器写入一个共享缓存块时，其他缓存的所有共享副本都会通过总线窥探更新。这个方法将写数据广播到总线上的所有缓存中。它比写失效协议引起更大的总线流量，因此这种方法不常见。Dragon和firefly协议属于此类别。 指令重排序Java语言规范规定JVM线程内部维持顺序化语义，即只要程序的最终结果与它顺序化情况的结果相等，那么指令的执行顺序可以与代码顺序不一致，此过程叫指令的重排序。指令重排序的意义：：JVM能根据处理器特性（CPU多级缓存系统、多核处理器等）适当的对机器指令进行重排序，使机器指令能更符合CPU的执行特性，最大限度的发挥机器性能。 Java线程的生命周期 NEW（初始化状态） RUNNABLE（可运行状态+运行状态） BLOCKED（阻塞状态） WAITING（无时限等待） TIMED_WAITING（有时限等待） TERMINATED（终止状态） 在操作系统层面，有五种状态，Java 线程中的 BLOCKED、WAITING、TIMED_WAITING 是一种状态，即前面我们提到的休眠状态。也就是说只要 Java 线程处于这三种状态之一，那么这个线程就永远没有 CPU 的使用权。 linux查看线程命令 ps - fe 查看所有进程 ps - fT - p 查看某个进程（ PID ）的所有线程 kill 杀死进程 top 按大写 H 切换是否显示线程 top - H - p 查看某个进程（ PID ）的所有线程 jps 命令查看所有 Java 进程 jstack 查看某个 Java 进程（ PID ）的所有线程状态 jconsole 来查看某个 Java 进程中线程的运行情况（图形界面） Java线程的实现方式 方式1：使用 Thread类或继承Thread类 实现 Runnable 接口配合Thread 使用有返回值的 Callable，借助线程池使用 使用 lambda 本质上Java中实现线程只有一种方式，都是通过new Thread()创建线程，调用Thread#start启动线程最终都会调用Thread#run方法 Thread常用方法sleep方法 调用 sleep 会让当前线程从 Running 进入TIMED_WAITING状态，不会释放对象锁 其它线程可以使用 interrupt 方法打断正在睡眠的线程，这时 sleep 方法会抛出InterruptedException，并且会清除中断标志 睡眠结束后的线程未必会立刻得到执行 sleep当传入参数为0时，和yield相同 yield方法 yield会释放CPU资源，让当前线程从 Running 进入 Runnable状态，让优先级更高（至少是相同）的线程获得执行机会，不会释放对象锁； 假设当前进程只有main线程，当调用yield之后，main线程会继续运行，因为没有比它优先级更高的线程； 具体的实现依赖于操作系统的任务调度器 ThreadLocal的实现原理ThreadLocal的实现原理，每一个Thread维护一个ThreadLocalMap，key为使用弱引用的ThreadLocal实例，value为线程变量的副本 ThreadLocal应用场景 在进行对象跨层传递的时候，使用ThreadLocal可以避免多次传递，打破层次间的约束。 线程间数据隔离 进行事务操作，用于存储线程事务信息。 数据库连接，Session会话管理。 Spring框架在事务开始时会给当前线程绑定一个Jdbc Connection,在整个事务过程都是使用该线程绑定的connection来执行数据库操作，实现了事务的隔离性。Spring框架里面就是用的ThreadLocal来实现这种隔离 ThreadLocal内存泄露原因，如何避免 强引用：使用最普遍的引用(new)，一个对象具有强引用，不会被垃圾回收器回收。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不回收这种对象。 弱引用：JVM进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。在java中，用java.lang.ref.WeakReference类来表示。可以在缓存中使用弱引用。 ThreadLocal正确的使用方法 每次使用完ThreadLocal都调用它的remove()方法清除数据 将ThreadLocal变量定义成private static，这样就一直存在ThreadLocal的强引用，也就能保证任何时候都能通过ThreadLocal的弱引用访问到Entry的value值，进而清除掉 。 synchronized和ReentranLock区别？synchronized是jvm层面的关键字，底层是通过monitor对象来完成，monitor对象底层依赖于操作系统的Mutex互斥量，底层调用的是操作系统的Pthread库，是由操作系统来维护的，而操作系统分为用户空间和内核空间的，JVM运行在用户空间，调用底层操作系统cpu会进行一轮状态切换，这个状态切换是比较重型操作，wait/notify等方法也依赖monitor对象只有在同步块或方法中才能调wait/notify等方法。 Lock是具体类（java.util.concurrent.Locks.Lock)是api层面的锁，是从jdk1.5开始引入的，那个时候synchronized性能还比较差，ReentranLock的出现是为了解决当时性能差的问题 使用方法：synchronized不需要手动释放锁，ReentranLock则需要用户手动释放锁(需要lock()和unlock()配合try/finally使用) 等待是否可中断：synchronized不可中断，除非抛出异常或者执行完成，ReentranLock可中断 设置超时方法tryLock(long timeout, timeUnit unit) lockInterruptibly()放代码块忠，调用interrupt()方法可中断 加锁是否公平：synchronized非公平锁；ReentranLock 两者都可以，默认非公平锁，构造方法传入true为公平锁，false为非公平锁 锁绑定多个条件Condition：synchronized 没有；ReentranLock 用来实现分组唤醒需要唤醒的线程，可以精确唤醒，不像synchronized要么随机唤醒一个线程，要么全部唤醒 线程池参数有，核心线程配置，原因？ corePoolSize：线程池中的核心线程数 maximumPoolSize：线程池中允许的最大线程数。如果当前阻塞队列满了，且继续提交任务，则创建新的线程执行任务 keepAliveTime：核心线程外的线程存活超时时间 unit：时间单位 workQueue：用来保存等待被执行的任务的阻塞队列，且任务必须实现Runable接口 threadFactory：用来创建新线程 线程池的饱和策略：( AbortPolicy：直接抛出异常，默认策略；CallerRunsPolicy：用调用者所在的线程来执行任务；DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务；DiscardPolicy：直接丢弃任务) CPU密集型（CPU-bound） CPU密集型也叫计算密集型，在多重程序系统中，大部份时间用来做计算、逻辑判断等CPU动作的程序称之CPU bound。例如一个计算圆周率至小数点一千位以下的程序，在执行的过程当中绝大部份时间用在三角函数和开根号的计算，便是属于CPU bound的程序。 线程数 = CPU核数+1 (现代CPU支持超线程) IO密集型（I/O bound） IO密集型指的是系统的CPU性能相对硬盘、内存要好很多，此时，系统运作，大部分的状况是CPU在等I/O (硬盘/内存) 的读/写操作 线程数 = （（线程等待时间+线程CPU时间）/线程CPU时间 ）* CPU数目 七、springspring是一个框架，更像是一个生态环境，在我们的开发流程中，所有的框架基本上都依赖于spring，spring起到了一个IOC容器的作用，用来承载我们整体的bean对象，它帮我们处理了bean对象从创建到销毁的整个生命周期的管理，我们在使用spring的时候，可以使用配置文件，也可以使用注解的方式进行相关开发。spring框架的工作主要流程是，当我们程序开始启动之后，spring把注解或者配置文件定义好的bean对象转化成为BeanDefinitionran，然后通BeanFactoryPostProcessor完成整个的BeanDefinitionran的解析和加载过程，然后根据BeanDefinitionran通过反射的方式创建bean对象，然后进行对象初始化，包括：aware接口相关操作，BeanPostProcessor操作，Init-methord操作完成整个bean的创建过程，之后我们就可以使用了。 Bean的初始化过程 Bean的生命周期 循环依赖AOP的顺序 spring4和spring5是不一样的 springMVC处理请求流程 spring事务的传播机制 TransactionDefinition.PROPAGATION_REQUIRED：支持当前事务，如果没有事务会创建一个新的事务 TransactionDefinition.PROPAGATION_SUPPORTS：支持当前事务，如果没有事务的话以非事务方式执行 TransactionDefinition.PROPAGATION_MANDATORY：支持当前事务，如果没有事务抛出异常 TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务并挂起当前事务 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式执行，如果当前存在事务则将当前事务挂起 TransactionDefinition.PROPAGATION_NEVER：以非事务方式进行，如果存在事务则抛出异常 TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。 八、网络网络七层模型 第一层：物理层 第二层：数据链路层 802.2、802.3ATM、HDLC、FRAME RELAY 第三层：网络层 IP、IPX、ARP、APPLETALK、ICMP 第四层：传输层 TCP、UDP、SPX 第五层：会话层 RPC、SQL、NFS 、X WINDOWS、ASP 第六层：表示层 ASCLL、PICT、TIFF、JPEG、 MIDI、MPEG 第七层：应用层 HTTP,FTP,SNMP等 物理层：物理层负责最后将信息编码成电流脉冲或其它信号用于网上传输； 数据链路层：数据链路层通过物理网络链路􏰁供数据传输。不同的数据链路层定义了不同的网络和协 议特征,其中包括物理编址、网络拓扑结构、错误校验、数据帧序列以及流控。可以简单的理解为：规定了0和1的分包形式，确定了网络数据包的形式； 网络层：网络层负责在源和终点之间建立连接。可以理解为，此处需要确定计算机的位置，怎么确定？IPv4，IPv6！ 传输层：传输层向高层提供可靠的端到端的网络数据流服务。可以理解为：每一个应用程序都会在网卡注册一个端口号，该层就是端口与端口的通信！常用的（TCP／IP）协议； 会话层：会话层建立、管理和终止表示层与实体之间的通信会话；建立一个连接（自动的手机信息、自动的网络寻址）; 表示层：表示层提供多种功能用于应用层数据编码和转化,以确保以一个系统应用层发送的信息 可以被另一个系统应用层识别;可以理解为：解决不同系统之间的通信，eg：Linux下的QQ和Windows下的QQ可以通信； 应用层：OSI 的应用层协议包括文件的传输、访问及管理协议(FTAM) ,以及文件虚拟终端协议(VIP)和公用管理系统信息(CMIP)等; http1.0和http1.1区别HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的HTTP协议。 主要区别主要体现在： 缓存处理，在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用，HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 错误通知的管理，在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。 Host头处理，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。 长连接，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。 HTTPS与HTTP的一些区别 HTTPS协议需要到CA申请证书，一般免费证书很少，需要交费。 HTTP协议运行在TCP之上，所有传输的内容都是明文，HTTPS运行在SSL/TLS之上，SSL/TLS运行在TCP之上，所有传输的内容都经过加密的。 HTTP和HTTPS使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 HTTPS可以有效的防止运营商劫持，解决了防劫持的一个大问题。 http三次握手四次挥手其实这么说是不太准确的，应该说是tcp协议建立连接要3次握手，断开连接要4次挥手，而http是基于tcp协议的，所以通常我们也这么说，tcp可以提供全双工的数据流传输服务 三次握手 TCP服务器进程先创建传输控制块TCB，时刻准备接受客户进程的连接请求，此时服务器就进入了LISTEN（监听）状态； TCP客户进程也是先创建传输控制块TCB，然后向服务器发出连接请求报文，这是报文首部中的同部位SYN=1，同时选择一个初始序列号 seq=x ，此时，TCP客户端进程进入了 SYN-SENT（同步已发送状态）状态。TCP规定，SYN报文段（SYN=1的报文段）不能携带数据，但需要消耗掉一个序号。 TCP服务器收到请求报文后，如果同意连接，则发出确认报文。确认报文中应该 ACK=1，SYN=1，确认号是ack=x+1，同时也要为自己初始化一个序列号 seq=y，此时，TCP服务器进程进入了SYN-RCVD（同步收到）状态。这个报文也不能携带数据，但是同样要消耗一个序号。 TCP客户进程收到确认后，还要向服务器给出确认。确认报文的ACK=1，ack=y+1，自己的序列号seq=x+1，此时，TCP连接建立，客户端进入ESTABLISHED（已建立连接）状态。TCP规定，ACK报文段可以携带数据，但是如果不携带数据则不消耗序号。 当服务器收到客户端的确认后也进入ESTABLISHED状态，此后双方就可以开始通信了。 四次挥手九、设计模式十、Dubbo你知道的json序列化方式？ 谷歌的Gson json-smart：号称是速度最快的JSON解析器 Common Lang3(3.1)的SerializationUtils 阿里巴巴的 FastJson、以及 Jackson 两个系统之间怎么交互的？ 套接字（socket）：这是一种更为一般得进程间通信机制，它可用于网络中不同机器之间的进程间通信，应用非常广泛。 可以通过RPC框架（dubbo、feign等）进行远程调用 也可以引入消息队列等消息中间件作为系统之间信息交互的桥梁 共享内存（shared memory）：可以说这是最有用的进程间通信方式。它使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据得更 dubbo的序列化？dubbo 支持 hession、Java 二进制序列化、json、SOAP 文本序列化多种序列化协议。 hessian 是其默认的序列化协议 dubbo 提供者(Provider)启动时，会向注册中心写入自己的元数据信息(调用方式)。 消费者(Consumer)启动时，也会在注册中心写入自己的元数据信息，并且订阅服务提供者，路由和配置元数据的信息。 服务治理中心(duubo-admin)启动时，会同时订阅所有消费者，提供者，路由和配置元数据的信息。 当提供者离开或者新提供者加入时，注册中心发现变化会通知消费者和服务治理中心。 十一、NettyNIO三大核心组件 Channel(通道)， Buffer(缓冲区)，Selector(多路复用器) channel 类似于流，每个 channel 对应一个 buffer缓冲区，buffer 底层就是个数组 channel 会注册到 selector 上，由 selector 根据 channel 读写事件的发生将其交由某个空闲的线程处理 NIO 的 Buffer 和 channel 都是既可以读也可以写 Netty线程模型Netty通过Reactor模型基于多路复用器接收并处理用户请求，内部实现boss和work两个线程池，其中boss的线程负责处理请求的accept事件，当接收到accept事件的请求时，把对应的socket封装到一个NioSocketChannel中，并交给work线程池，其中work线程池负责请求的read和write事件，交由对应的Handler处理。 Netty 抽象出两组线程池BossGroup和WorkerGroup，BossGroup专门负责接收客户端的连接, WorkerGroup专门负责网络的读写 BossGroup和WorkerGroup类型都是NioEventLoopGroup NioEventLoopGroup 相当于一个事件循环线程组, 这个组中含有多个事件循环线程 ， 每一个事件循环线程是NioEventLoop 每个NioEventLoop都有一个selector , 用于监听注册在其上的socketChannel的网络通讯 每个BossNioEventLoop线程内部循环执行的步骤有 3 步 处理accept事件 , 与client 建立连接 , 生成 NioSocketChannel 将NioSocketChannel注册到某个worker NIOEventLoop上的selector 处理任务队列的任务 ， 即runAllTasks 每个workerNIOEventLoop线程循环执行的步骤 轮询注册到自己selector上的所有NioSocketChannel 的read, write事件 处理 I/O 事件， 即read , write 事件， 在对应NioSocketChannel 处理业务 runAllTasks处理任务队列TaskQueue的任务 ，一些耗时的业务处理一般可以放入TaskQueue中慢慢处理，这样不影响数据在 pipeline 中的流动处理 Netty模块组件 Bootstrap、ServerBootstrap：Bootstrap 意思是引导，一个 Netty 应用通常由一个 Bootstrap 开始，主要作用是配置整个 Netty 程序，串联各个组件，Netty 中 Bootstrap 类是客户端程序的启动引导类，ServerBootstrap 是服务端启动引导类。 Future、ChannelFuture：正如前面介绍，在 Netty 中所有的 IO 操作都是异步的，不能立刻得知消息是否被正确处理。但是可以过一会等它执行完成或者直接注册一个监听，具体的实现就是通过 Future 和 ChannelFutures，他们可以注册一个监听，当操作执行成功或失败时监听会自动触发注册的监听事件。 Channel：Netty 网络通信的组件，能够用于执行网络 I/O 操作。Channel 为用户提供： 当前网络连接的通道的状态（例如是否打开？是否已连接？） 网络连接的配置参数 （例如接收缓冲区大小） 提供异步的网络 I/O 操作(如建立连接，读写，绑定端口)，异步调用意味着任何 I/O 调用都将立即返回，并且不保证在调用结束时所请求的 I/O 操作已完成。 调用立即返回一个 ChannelFuture 实例，通过注册监听器到 ChannelFuture 上，可以 I/O 操作成功、失败或取消时回调通知调用方。 支持关联 I/O 操作与对应的处理程序。 不同协议、不同的阻塞类型的连接都有不同的 Channel 类型与之对应。 NioSocketChannel，异步的客户端 TCP Socket 连接。 NioServerSocketChannel，异步的服务器端 TCP Socket 连接。 NioDatagramChannel，异步的 UDP 连接。 NioSctpChannel，异步的客户端 Sctp 连接。 NioSctpServerChannel，异步的 Sctp 服务器端连接。 这些通道涵盖了 UDP 和 TCP 网络 IO 以及文件 IO。 Selector：Netty 基于 Selector 对象实现 I/O 多路复用，通过 Selector 一个线程可以监听多个连接的 Channel 事件。当向一个 Selector 中注册 Channel 后，Selector 内部的机制就可以自动不断地查询(Select) 这些注册的 Channel 是否有已就绪的 I/O 事件（例如可读，可写，网络连接完成等），这样程序就可以很简单地使用一个线程高效地管理多个 Channel 。 NioEventLoop：NioEventLoop 中维护了一个线程和任务队列，支持异步提交执行任务，线程启动时会调用 NioEventLoop 的 run 方法，执行 I/O 任务和非 I/O 任务：I/O 任务，即 selectionKey 中 ready 的事件，如 accept、connect、read、write 等，由 processSelectedKeys 方法触发。非 IO 任务，添加到 taskQueue 中的任务，如 register0、bind0 等任务，由 runAllTasks 方法触发。 NioEventLoopGroup：NioEventLoopGroup，主要管理 eventLoop 的生命周期，可以理解为一个线程池，内部维护了一组线程，每个线程(NioEventLoop)负责处理多个 Channel 上的事件，而一个 Channel 只对应于一个线程。 ChannelHandler：ChannelHandler 是一个接口，处理 I/O 事件或拦截 I/O 操作，并将其转发到其 ChannelPipeline(业务处理链)中的下一个处理程序。 ChannelHandlerContext：保存 Channel 相关的所有上下文信息，同时关联一个 ChannelHandler 对象。 ChannelPipline：保存 ChannelHandler 的 List，用于处理或拦截 Channel 的入站事件和出站操作。ChannelPipeline 实现了一种高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，以及 Channel 中各个的 ChannelHandler 如何相互交互。在 Netty 中每个 Channel 都有且仅有一个 ChannelPipeline 与之对应，它们的组成关系如下： 一个 Channel 包含了一个 ChannelPipeline，而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表，并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler。read事件(入站事件)和write事件(出站事件)在一个双向链表中，入站事件会从链表 head 往后传递到最后一个入站的 handler，出站事件会从链表 tail 往前传递到最前一个出站的 handler，两种类型的 handler 互不干扰。 Netty粘包拆包TCP是一个流协议，就是没有界限的一长串二进制数据。TCP作为传输层协议并不不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行数据包的划分，所以在业务上认为是一个完整的包，可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。面向流的通信是无消息保护边界的。 解决方案 消息定长度，传输的数据大小固定长度，例如每段的长度固定为100字节，如果不够空位补空格 在数据包尾部添加特殊分隔符，比如下划线，中划线等，这种方法简单易行，但选择分隔符的时候一定要注意每条数据的内部一定不能出现分隔符。 发送长度：发送每条数据的时候，将数据的长度一并发送，比如可以选择每条数据的前4位是数据的长度，应用层处理时可以根据长度来判断每条数据的开始和结束。 Netty提供了多个解码器，可以进行分包的操作，如下： LineBasedFrameDecoder （回车换行分包） DelimiterBasedFrameDecoder（特殊分隔符分包） FixedLengthFrameDecoder（固定长度报文来分包） 十二、zookeeperzookeeper的理解 集群管理：提供了CP模型来保证集群中每个节点的数据一致性 分布式锁 集群选举 zookeeper选举机制 LeaderElection AuthFastLeaderElection FastLeaderElection （最新默认） 目前有5台服务器，每台服务器均没有数据，它们的编号分别是1,2,3,4,5,按编号依次启动，它们的选择举过程如下： 服务器1启动，给自己投票，然后发投票信息，由于其它机器还没有启动所以它收不到反馈信息，服务器1的状态一直属于Looking(选举状态)。 服务器2启动，给自己投票，同时与之前启动的服务器1交换结果，由于服务器2的编号大所以服务器2胜出，但此时投票数没有大于半数，所以两个服务器的状态依然是LOOKING。 服务器3启动，给自己投票，同时与之前启动的服务器1,2交换信息，由于服务器3的编号最大所以服务器3胜出，此时投票数正好大于半数，所以服务器3成为领导者，服务器1,2成为小弟。 服务器4启动，给自己投票，同时与之前启动的服务器1,2,3交换信息，尽管服务器4的编号大，但之前服务器3已经胜出，所以服务器4只能成为小弟。 服务器5启动，后面的逻辑同服务器4成为小弟。 十三、理论CAP理论​ CAP 理论又叫 Brewer 理论，这是加州大学伯克利分校的埃里克 · 布鲁尔（Eric Brewer）教授，在 2000 年 7 月“ACM 分布式计算原理研讨会（PODC）”上提出的一个猜想。CAP理论原稿（那时候还只是猜想）然后到了 2002 年，麻省理工学院的赛斯 · 吉尔伯特（Seth Gilbert）和南希 · 林奇（Nancy Lynch）就以严谨的数学推理证明了这个 CAP 猜想。在这之后，CAP 理论就正式成为了分布式计算领域公认的著名定理。这个定理里，描述了一个分布式的系统中，当涉及到共享数据问题时，以下三个特性最多只能满足其中两个： 一致性（Consistency）：代表在任何时刻、任何分布式节点中，我们所看到的数据都是没有矛盾的。这与 ACID 中的 C 是相同的单词，但它们又有不同的定义（分别指 Replication 的一致性和数据库状态的一致性）。在分布式事务中，ACID 的 C 要以满足 CAP 中的 C 为前提。 可用性（Availability）：代表系统不间断地提供服务的能力。 分区容忍性（Partition Tolerance）：代表分布式环境中，当部分节点因网络原因而彼此失联（即与其他节点形成“网络分区”）时，系统仍能正确地提供服务的能力。 https://mp.weixin.qq.com/s?__biz=MzU3MTQwNDEyMg==&amp;mid=2247483680&amp;idx=1&amp;sn=a844dc83df3316ac0102ea11511b8b46&amp;chksm=fce1fb15cb9672032e98857a74f4e971a1ff833a6e353cafee655587c70a93e730a652daf5a2&amp;scene=21#wechat_redirect DDDddd不是一种架构风格，而是一种方法论，什么是方法论，每个人按照自己的想法来设计就是一套方法论；ddd是一种业务比较认可，对于微服务拆分的一种方法论。 十四、项目 最近做的比较熟悉的项目是哪个？画一下项目技术架构图 写两个类，能够实现堆内存溢出和栈内存溢出 写一个线程安全的单例。 两个可变有序链表放到新数组中，有序 ES简介ES他是建立在全文搜索引擎库ApacheLucene基础上的一个开源搜索和分析引擎，ES本身具有一个分布式存储，检索速度快的特性，所以我们经常用它去实现全文检索这一类的场景，比如像网站搜索，公司内部用ELK做日志聚集和检索，来去快速查找服务器的日志记录，去定位问题，基本上涉及到TB级别的数据场景，用ES是一个比较好的选择 ES查询快的原因 第一、Lucene是擅长管理大量的索引数据的，另一方面他会对数据进行分词以后在保存索引，这样能搞提升数据的检索效率 第二、ES采用倒排索引，所谓倒排索引，就是通过属性值来确定数据记录的位置的索引，来避免全表扫描这样一个问题， 第三、ES采用分片存储机制 第四、ES扩展性好，我们可以水平扩展增加服务器，提升ES处理性能，可以达到百台服务器节点扩展 第五、ES内部提供了数据汇总和索引生命周期管理的一些功能，可以方便我们更加高效的存储和搜索数据 使用ES注意事项 ES里面不建议使用复杂的关联查询，会对性能影响很大 避免深度分页查询，因为ES分页是支持front和size参数，在查询的时候，每个分片必须构造一个长度为front加size的优先队列，然后回传到网关节点，网关节点再对这些队列进行排序再找到正确的size文档。而当front足够大的时候容易造成OOM以及网络传输性能差的一些问题","categories":[{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"数据结构和算法","slug":"2.算法/面试-数据结构和算法","date":"2022-03-30T16:00:00.000Z","updated":"2024-08-22T03:23:14.706Z","comments":true,"path":"2022/03/31/2.算法/面试-数据结构和算法/","link":"","permalink":"https://zhangxin66666.github.io/2022/03/31/2.%E7%AE%97%E6%B3%95/%E9%9D%A2%E8%AF%95-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/","excerpt":"","text":"排序算法高效交换算法(异或^)概念 0^N = N ; N^N = 0 相同为0，不同为1，也可以叫做无进位相加，这么做的前提：需要交换的两个数指向的内存是两位位置 异或运算满足交换律和结合律 不用额外变量交换两个数 123456// 交换arr的i和j位置上的值public static void swap(int[] arr, int i, int j) &#123; arr[i] = arr[i] ^ arr[j]; arr[j] = arr[i] ^ arr[j]; arr[i] = arr[i] ^ arr[j];&#125; 一种数出现奇数次一个数组中有一个数出现奇数次，其他数都出现偶数次，怎么找到这一个数？ 1234567public static void printOddTimesNum1(int[] arr) &#123; int eor = 0; for (int i = 0; i &lt; arr.length; i++) &#123; eor ^= arr[i]; &#125; System.out.println(eor);&#125; 两种数出现奇数次一个数组中有两个数出现奇数次，其他数都出现了偶数次，怎么找到这两个数？ 123456789101112131415161718192021public static void printOddTimesNum2(int[] arr) &#123; int eor = 0; for (int i = 0; i &lt; arr.length; i++) &#123; eor ^= arr[i]; &#125; // a 和 b是两种数 // eor != 0 // eor最右侧的1，提取出来 // eor : 00110010110111000 // rightOne :00000000000001000 int rightOne = eor &amp; (-eor); // 提取出最右的1 int onlyOne = 0; // eor&#x27; for (int i = 0 ; i &lt; arr.length;i++) &#123; // arr[1] = 111100011110000 // rightOne= 000000000010000 if ((arr[i] &amp; rightOne) != 0) &#123; onlyOne ^= arr[i]; &#125; &#125; System.out.println(onlyOne + &quot; &quot; + (eor ^ onlyOne));&#125; 冒泡排序 基本冒泡排序算法 123456789101112131415public static void bubbleSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; // 0 ~ N-1 // 0 ~ N-2 // 0 ~ N-3 for (int e = arr.length - 1; e &gt; 0; e--) &#123; // 0 ~ e for (int i = 0; i &lt; e; i++) &#123; if (arr[i] &gt; arr[i + 1]) &#123; swap(arr, i, i + 1); &#125; &#125; &#125; &#125; 冒泡排序算法优化 在一趟排序过程中如果一次都没有交换过，那说明后续的数都是有序的，不需要在进行后续的排序了，如果元素本来就是有序的，就只比较一次就结束了。 12345678910111213141516171819202122232425public static void bubbleSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; // 标识变量，表示是否进行过交换 boolean flag = false; for (int i = 0; i &lt; arr.length - 1; i++) &#123; for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123; // 如果前面的数比后面的数大，则交换 if (arr[j] &gt; arr[j + 1]) &#123; flag = true; int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; // 在一趟排序中，一次交换都没有发生过 if (!flag) &#123; break; &#125; else &#123; // 重置flag!!!, 进行下次判断 flag = false; &#125; &#125;&#125; 选择排序 0 ~ N-1 找到最小值，在哪，放到0位置上 1 ~ n-1 找到最小值，在哪，放到1 位置上 2 ~ n-1 找到最小值，在哪，放到2 位置上 123456789101112public static void selectionSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; for (int i = 0; i &lt; arr.length - 1; i++) &#123; int minIndex = i; for (int j = i + 1; j &lt; arr.length; j++) &#123; // i ~ N-1 上找最小值的下标 minIndex = arr[j] &lt; arr[minIndex] ? j : minIndex; &#125; swap(arr, i, minIndex); &#125;&#125; 插入排序 0~1单范围有序 0~2范围有序 0~3范围有序 0~N范围有序 1234567891011public static void insertionSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; // 不只1个数 for (int i = 1; i &lt; arr.length; i++) &#123; // 0 ~ i 做到有序 for (int j = i - 1; j &gt;= 0 &amp;&amp; arr[j] &gt; arr[j + 1]; j--) &#123; swap(arr, j, j + 1); &#125; &#125;&#125; 查找算法二分查找二分法的详解与扩展 在一个有序数组中，查找某个数是否存在 在一个有序数组中，找&gt;=某个数最左侧的位置 局部最小值问题 1234567891011121314151617181920public static boolean exist(int[] sortedArr, int num) &#123; if (sortedArr == null || sortedArr.length == 0) &#123; return false; &#125; int L = 0; int R = sortedArr.length - 1; int mid = 0; // L..R while (L &lt; R) &#123; // L..R 至少两个数的时候 mid = L + ((R - L) &gt;&gt; 1); if (sortedArr[mid] == num) &#123; return true; &#125; else if (sortedArr[mid] &gt; num) &#123; R = mid - 1; &#125; else &#123; L = mid + 1; &#125; &#125; return sortedArr[L] == num;&#125; 数组范围上求最大值递归解法 123456789101112public static int process(int[] arr, int L, int R) &#123; // arr[L..R]范围上只有一个数，直接返回，base case if (L == R) &#123; return arr[L]; &#125; // L...R 不只一个数 // mid = (L + R) / 2 int mid = L + ((R - L) &gt;&gt; 1); // 中点 int leftMax = process(arr, L, mid); int rightMax = process(arr, mid + 1, R); return Math.max(leftMax, rightMax);&#125; 对数器的概念 有一个你想要测试的方法a 实现复杂度不好但是容易实现的方法b 实现一个随机样本产生器 把方法a和方法b跑相同的随机样本，看看得到的结果是否一样 如果有一个随机样本时的比对结果不一致，打印样本进行人工干预，改对方法a或者方法b 当样本数量很多时比对测试依然正确，可以确定方法a已经正确 数组ab数组合并到a 题目：给出两个有序的整数数组A和B，请将数组B合并到数组A中，变成一个有序的数组。注意：可以假设A数组有足够的空间存放B数组的元素，A和B中初始的元素数目分别为m和n。 题解：最优解：从后往前处理,不需要开辟额外空间。从后往前，这样不需要进行冗余处理。 12345678910111213141516171819202122/** * @param a 数组a，有足够的空间合并数组b * @param m 数组a里面的元素个数 * @param b 数组b * @param n 数组b里面的元素个数 */public static void merge(int[] a, int m, int[] b, int n) &#123; int i = m - 1; int j = n - 1; int index = m + n - 1; while (i &gt;= 0 &amp;&amp; j &gt;= 0) &#123; if(a[i] &gt; b[j] )&#123; a[index--] = a[i--]; &#125; else &#123; a[index--] = b[j--]; &#125; &#125; //如果A的数字比B多，则不会进入后续处理；如果B的数字比A多，则进入后续处理，将B剩余数字添加到数组A中。 while (j &gt;= 0) &#123; a[index--] = b[j--]; &#125;&#125; ab数组合并到c 题目： 合并两个有序整型数据（入参两个需要合并的数组，返回值合并好的新数组） 123456789101112131415161718192021222324252627private static int[] margeArr(int[] a, int[] b) &#123; int[] c = new int[a.length + b.length]; int i = 0, j = 0; int index = 0; //比较指针i,j指向的值，小的值存入指针index指向的结果数组中，当有一个指针（i或j）先到达数组末尾时，比较结束； while (i &lt; a.length &amp;&amp; j &lt; b.length) &#123; if (a[i] &lt; b[j]) &#123; c[index++] = a[i++]; &#125; else &#123; c[index++] = b[j++]; &#125; &#125; int l; //将指针（i或j）没有到达数组末尾的数组复制到指针index指向的结果数组中 if (i &lt; a.length) &#123; for (l = i; l &lt; a.length; l++) &#123; c[index++] = a[l]; &#125; &#125; //将指针（i或j）没有到达数组末尾的数组复制到指针index指向的结果数组中 if (j &lt; b.length) &#123; for (l = j; l &lt; b.length; l++) &#123; c[index++] = b[l]; &#125; &#125; return c;&#125; 查找两数之和等于目标数 题目：给定一个数组和一个目标数，从数组中找到两个数，是这两个数之和等于目标数。返回其在数组中的编号 12345678910public static int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; if (map.containsKey(target - nums[i])) &#123; return new int[]&#123;map.get(target - nums[i]), i&#125;; &#125; map.put(nums[i], i); &#125; return null;&#125; 螺旋打印二维数组1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public static void printEdge(int[][] m, int tR, int tC, int dR, int dC) &#123; if (tR == dR) &#123; for (int i = tC; i &lt;= dC; i++) &#123; System.out.print(m[tR][i] + &quot; &quot;); &#125; &#125; else if (tC == dC) &#123; for (int i = tR; i &lt;= dR; i++) &#123; System.out.print(m[i][tC] + &quot; &quot;); &#125; &#125; else &#123; int curC = tC; int curR = tR; while (curC != dC) &#123; System.out.print(m[tR][curC] + &quot; &quot;); curC++; &#125; while (curR != dR) &#123; System.out.print(m[curR][dC] + &quot; &quot;); curR++; &#125; while (curC != tC) &#123; System.out.print(m[dR][curC] + &quot; &quot;); curC--; &#125; while (curR != tR) &#123; System.out.print(m[curR][tC] + &quot; &quot;); curR--; &#125; &#125;&#125;public static void spiralOrderPrint(int[][] matrix) &#123; int tR = 0; int tC = 0; int dR = matrix.length - 1; int dC = matrix[0].length - 1; while (tR &lt;= dR &amp;&amp; tC &lt;= dC) &#123; printEdge(matrix, tR++, tC++, dR--, dC--); &#125;&#125;public static void main(String[] args) &#123; int[][] matrix = &#123; &#123; 1, 2, 3, 4 &#125;, &#123; 5, 6, 7, 8 &#125;, &#123; 9, 10, 11, 12 &#125;, &#123; 13, 14, 15, 16 &#125; &#125;; spiralOrderPrint(matrix);&#125; 链表 对于笔试，不用太在乎空间复杂度，一切为了时间复杂度 对于面试，时间复杂度依然放在第一位，但是一定要找到空间最省的方法 重要技巧 额外数据结构记录（哈希表等） 快慢指针 链表反转单向链表反转123456789// 单向链表节点public static class Node &#123; public int value; public Node next; public Node(int data) &#123; value = data; &#125;&#125; 1234567891011121314// head 单向链表反转算法// a -&gt; b -&gt; c -&gt; null// c -&gt; b -&gt; a -&gt; nullpublic static Node reverseLinkedList(Node head) &#123; Node pre = null; Node next = null; while (head != null) &#123; next = head.next; head.next = pre; pre = head; head = next; &#125; return pre;&#125; 123456789101112/** * 递归反转方式 */private Node reverseLinkedList(Node head) &#123; if (head == null || head.next == null) &#123; return head; &#125; Node newHead = reverseLinkedList(head.next); head.next.next = head; head.next = null; return newHead;&#125; 双向链表反转12345678910// 双向链表节点public static class DoubleNode &#123; public int value; public DoubleNode last; public DoubleNode next; public DoubleNode(int data) &#123; value = data; &#125;&#125; 12345678910111213// 双向链表反转算法public static DoubleNode reverseDoubleList(DoubleNode head) &#123; DoubleNode pre = null; DoubleNode next = null; while (head != null) &#123; next = head.next; head.next = pre; head.last = next; pre = head; head = next; &#125; return pre;&#125; 查找链表中点快慢指针应用 查找链表中点或中点上一个1234567891011121314// head 头 解：查找链表中点或者中点前一个节点public static Node midOrUpMidNode(Node head) &#123; if (head == null || head.next == null || head.next.next == null) &#123; return head; &#125; // 链表有3个点或以上 Node slow = head.next; Node fast = head.next.next; while (fast.next != null &amp;&amp; fast.next.next != null) &#123; slow = slow.next; fast = fast.next.next; &#125; return slow;&#125; 查找链表中点或中点下一个12345678910111213// 查找链表中点或者中点下一个节点public static Node midOrDownMidNode(Node head) &#123; if (head == null || head.next == null) &#123; return head; &#125; Node slow = head.next; Node fast = head.next; while (fast.next != null &amp;&amp; fast.next.next != null) &#123; slow = slow.next; fast = fast.next.next; &#125; return slow;&#125; 判断一个链表是否是回文结构【题目】给定一个单向链表的头节点head，请判断该链表是否为回文结构。【例子】1-&gt;2-&gt;1,返回true;1-&gt;2-&gt;2-&gt;1,返回true；15-&gt;6-&gt;15，返回true；1-&gt;2-&gt;3，返回false。如果链表长度为N，时间复杂度达到O(N)，额外空间复杂度达到O(1)。 解题思路1： 1：遍历链表，把每个元素放到栈里面；2：从栈里面依次弹出元素和原来链表挨个元素比对。 解题思路2： 1：通过快慢指针找到中点和结束点，只把右侧部分放到栈里面去；2：从栈里面依次弹出元素和原来链表挨个元素比对。 12345678910111213141516// 1：遍历链表，把每个元素放到栈里面；2：从栈里面依次弹出元素和原来链表挨个元素比对public static boolean isPalindrome1(Node head) &#123; Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;(); Node cur = head; while (cur != null) &#123; stack.push(cur); cur = cur.next; &#125; while (head != null) &#123; if (head.value != stack.pop().value) &#123; return false; &#125; head = head.next; &#125; return true;&#125; 123456789101112131415161718192021222324// 1：通过快慢指针找到中点和结束点，只把右侧部分放到栈里面去；2：从栈里面依次弹出元素和原来链表挨个元素比对public static boolean isPalindrome2(Node head) &#123; if (head == null || head.next == null) &#123; return true; &#125; Node right = head.next; Node cur = head; while (cur.next != null &amp;&amp; cur.next.next != null) &#123; right = right.next; cur = cur.next.next; &#125; Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;(); while (right != null) &#123; stack.push(right); right = right.next; &#125; while (!stack.isEmpty()) &#123; if (head.value != stack.pop().value) &#123; return false; &#125; head = head.next; &#125; return true;&#125; 链表排序单向链表分左中右【题目】 将单向链表按照某值划分成左边小，中间相等，右边大的形式：给定一个单链表头节点head，节点的值类型是整型，再给定一个整数pivot。实现一个调整链表的函数，将链表调整为做部分都是值小于pivot的节点，中间部分都是值等于piovt的节点，有部分都是值大于piovt的节点。【进阶】在实现原问题功能的基础上增加要求：小于，等于，大于pivot节点之间顺序和之前一样，时间复杂度O(N)，额外空间复杂度O(1)。 解题思路1： 将链表放入数组，排序，再转成链表 解题思路2： 左中右分别准备两个指针(共6个变量)，然后遍历原链表，排序之后分别插入相应位置(考虑边界问题) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 将链表放入数组，排序，再转成链表public static Node listPartition1(Node head, int pivot) &#123; if (head == null) &#123; return head; &#125; Node cur = head; int i = 0; while (cur != null) &#123; i++; cur = cur.next; &#125; Node[] nodeArr = new Node[i]; i = 0; cur = head; for (i = 0; i != nodeArr.length; i++) &#123; nodeArr[i] = cur; cur = cur.next; &#125; arrPartition(nodeArr, pivot); for (i = 1; i != nodeArr.length; i++) &#123; nodeArr[i - 1].next = nodeArr[i]; &#125; nodeArr[i - 1].next = null; return nodeArr[0];&#125;public static void arrPartition(Node[] nodeArr, int pivot) &#123; int small = -1; int big = nodeArr.length; int index = 0; while (index != big) &#123; if (nodeArr[index].value &lt; pivot) &#123; swap(nodeArr, ++small, index++); &#125; else if (nodeArr[index].value == pivot) &#123; index++; &#125; else &#123; swap(nodeArr, --big, index); &#125; &#125;&#125;public static void swap(Node[] nodeArr, int a, int b) &#123; Node tmp = nodeArr[a]; nodeArr[a] = nodeArr[b]; nodeArr[b] = tmp;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// 左中右分别准备两个指针(共6个变量)，然后遍历原链表，排序之后分别插入相应位置(考虑边界问题)public static Node listPartition2(Node head, int pivot) &#123; Node sH = null; // small head Node sT = null; // small tail Node eH = null; // equal head Node eT = null; // equal tail Node mH = null; // big head Node mT = null; // big tail Node next = null; // save next node // every node distributed to three lists while (head != null) &#123; next = head.next; head.next = null; if (head.value &lt; pivot) &#123; if (sH == null) &#123; sH = head; sT = head; &#125; else &#123; sT.next = head; sT = head; &#125; &#125; else if (head.value == pivot) &#123; if (eH == null) &#123; eH = head; eT = head; &#125; else &#123; eT.next = head; eT = head; &#125; &#125; else &#123; if (mH == null) &#123; mH = head; mT = head; &#125; else &#123; mT.next = head; mT = head; &#125; &#125; head = next; &#125; // 小于区域的尾巴，连等于区域的头，等于区域的尾巴连大于区域的头 if (sT != null) &#123; // 如果有小于区域 sT.next = eH; eT = eT == null ? sT : eT; // 下一步，谁去连大于区域的头，谁就变成eT &#125; // 下一步，一定是需要用eT 去接 大于区域的头 // 有等于区域，eT -&gt; 等于区域的尾结点 // 无等于区域，eT -&gt; 小于区域的尾结点 // eT 尽量不为空的尾巴节点 if (eT != null) &#123; // 如果小于区域和等于区域，不是都没有 eT.next = mH; &#125; return sH != null ? sH : (eH != null ? eH : mH);&#125; 链表闭环及相交问题单链表查找闭环位置注：单链表闭环只能有一个环，如果产生环，必然会出现闭环 解法1：额外空间解决 申请一个set集合，从头节点遍历链表，每遍历一个元素就查询该节点是否在集合中，如果没有就把该节点放进去，如果有，该节点就是环位置 1.... 解法2：有限几个变量解决 快慢指针从单链表头节点开始走，直至两个节点相遇，说明有环，最后指向null，说明无环 相遇之后快指针回到头节点，之后一次走一步，慢指针停在原地，再次相遇的位置即是环节点位置(记住结论，不要问为什么) 123456789101112131415161718192021222324252627282930public static class Node &#123; public int value; public Node next; public Node(int data) &#123; this.value = data; &#125;&#125;// 找到链表第一个入环节点，如果无环，返回nullpublic static Node getLoopNode(Node head) &#123; if (head == null || head.next == null || head.next.next == null) &#123; return null; &#125; // n1 慢 n2 快 Node slow = head.next; // n1 -&gt; slow Node fast = head.next.next; // n2 -&gt; fast while (slow != fast) &#123; if (fast.next == null || fast.next.next == null) &#123; return null; &#125; fast = fast.next.next; slow = slow.next; &#125; // slow fast 相遇 fast = head; // n2 -&gt; walk again from head while (slow != fast) &#123; slow = slow.next; fast = fast.next; &#125; return slow;&#125; 两个无环链表交点位置注： 如果两个单向链表相交，相交后面的部分必然是共有的，那么两个链表最后的那个节点必然是同一个节点 长链表先走两个链表差值的步数，然后短链表在开始走，他俩一定会在第一个相交的位置相遇 123456789101112131415161718192021222324252627282930313233// 如果两个链表都无环，返回第一个相交节点，如果不想交，返回nullpublic static Node noLoop(Node head1, Node head2) &#123; if (head1 == null || head2 == null) &#123; return null; &#125; Node cur1 = head1; Node cur2 = head2; int n = 0; while (cur1.next != null) &#123; n++; cur1 = cur1.next; &#125; while (cur2.next != null) &#123; n--; cur2 = cur2.next; &#125; if (cur1 != cur2) &#123; return null; &#125; // n : 链表1长度减去链表2长度的值 cur1 = n &gt; 0 ? head1 : head2; // 谁长，谁的头变成cur1 cur2 = cur1 == head1 ? head2 : head1; // 谁短，谁的头变成cur2 n = Math.abs(n); while (n != 0) &#123; n--; cur1 = cur1.next; &#125; while (cur1 != cur2) &#123; cur1 = cur1.next; cur2 = cur2.next; &#125; return cur1;&#125; 两条有环链表查找交点分为两种情况：入环节点可能是同一个节点，也可能不是同一个节点，如图 情况1：入环节点可能是同一个节点，就是求单链表的第一个环节点问题，只不过从相交位置开始走 情况2：如果链表1在转回到自己的过程中没有遇到链表2，就说明是各自成环的，相交节点返回空就醒来，如果遇到，就是情况2，返回两个节点，都对，都属于第一个相交的节点 123456789101112131415161718192021222324252627282930313233343536373839// 两个有环链表，返回第一个相交节点，如果不想交返回nullpublic static Node bothLoop(Node head1, Node loop1, Node head2, Node loop2) &#123; Node cur1 = null; Node cur2 = null; if (loop1 == loop2) &#123; cur1 = head1; cur2 = head2; int n = 0; while (cur1 != loop1) &#123; n++; cur1 = cur1.next; &#125; while (cur2 != loop2) &#123; n--; cur2 = cur2.next; &#125; cur1 = n &gt; 0 ? head1 : head2; cur2 = cur1 == head1 ? head2 : head1; n = Math.abs(n); while (n != 0) &#123; n--; cur1 = cur1.next; &#125; while (cur1 != cur2) &#123; cur1 = cur1.next; cur2 = cur2.next; &#125; return cur1; &#125; else &#123; cur1 = loop1.next; while (cur1 != loop1) &#123; if (cur1 == loop2) &#123; return loop1; &#125; cur1 = cur1.next; &#125; return null; &#125;&#125; 查找两个链表相交节点注：难点为考虑是否有环，有环链表相交以及无环链表相交问题，没有用到额外数据结构，只用到有限几个变量 1234567891011121314151617public static Node getIntersectNode(Node head1, Node head2) &#123; if (head1 == null || head2 == null) &#123; return null; &#125; // 分别找到两个链表的环位置 Node loop1 = getLoopNode(head1); Node loop2 = getLoopNode(head2); if (loop1 == null &amp;&amp; loop2 == null) &#123; // 无环链表相交问题 return noLoop(head1, head2); &#125; if (loop1 != null &amp;&amp; loop2 != null) &#123; // 两条有环链表相交问题 return bothLoop(head1, loop1, head2, loop2); &#125; return null;&#125; 二叉树数据结构 12345678public static class Node &#123; public int value; public Node left; public Node right; public Node(int v) &#123; value = v; &#125;&#125; 二叉树遍历 先序遍历：头-&gt;左-&gt;右 中序遍历：左-&gt;头-&gt;右 后序遍历：左-&gt;右-&gt;头 递归遍历二叉树遍历说明 递归通过打印时机不同，实现先，中，后序遍历 12345678910public static void f(Node head) &#123; if (head == null) &#123; return; &#125; // 1 前序 f(head.left); // 2 中序 f(head.right); // 3 后序&#125; 非递归遍历二叉树 先序遍历(深度性遍历) 准备一个栈，根节点入栈弹出，打印，然后先压右，再压左 弹出打印，先压右再压左，周而复始 123456789101112131415161718public static void pre(Node head) &#123; System.out.print(&quot;pre-order: &quot;); if (head != null) &#123; Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;(); stack.add(head); while (!stack.isEmpty()) &#123; head = stack.pop(); System.out.print(head.value + &quot; &quot;); if (head.right != null) &#123; stack.push(head.right); &#125; if (head.left != null) &#123; stack.push(head.left); &#125; &#125; &#125; System.out.println();&#125; 后序遍历 在先序遍历的基础之上，增加一个收集栈，弹出来就放到收集栈中(不打印)，然后先压左，再压右 把收集栈中的元素依次出栈，打印 1234567891011121314151617181920212223public static void pos1(Node head) &#123; System.out.print(&quot;pos-order: &quot;); if (head != null) &#123; Stack&lt;Node&gt; s1 = new Stack&lt;Node&gt;(); Stack&lt;Node&gt; s2 = new Stack&lt;Node&gt;(); s1.push(head); while (!s1.isEmpty()) &#123; head = s1.pop(); // 头 右 左 s2.push(head); if (head.left != null) &#123; s1.push(head.left); &#125; if (head.right != null) &#123; s1.push(head.right); &#125; &#125; // 左 右 头 while (!s2.isEmpty()) &#123; System.out.print(s2.pop().value + &quot; &quot;); &#125; &#125; System.out.println();&#125; 中序遍历 整棵树左边界进栈，依次弹出的过程中，打印，对弹出节点的右树周而复始 为什么？ 因为整个树都会被他的左边界分解掉，我们把头和左边界压栈，然后再右，出栈的时候就是左，头，右 1234567891011121314151617public static void in(Node cur) &#123; System.out.print(&quot;in-order: &quot;); if (cur != null) &#123; Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;(); while (!stack.isEmpty() || cur != null) &#123; if (cur != null) &#123; stack.push(cur); cur = cur.left; &#125; else &#123; cur = stack.pop(); System.out.print(cur.value + &quot; &quot;); cur = cur.right; &#125; &#125; &#125; System.out.println();&#125; 宽度遍历宽度遍历就是横着遍历，也是层次遍历 用队列，头节点放队列，每一次弹出就打印，然后先放左再放右，每一个元素出队列都是先放左再放右 123456789101112131415161718// 从node出发，进行宽度优先遍历public static void width(Node head) &#123; if (head == null) &#123; return; &#125; Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;(); queue.add(head); while (!queue.isEmpty()) &#123; Node cur = queue.poll(); System.out.println(cur.value); if (cur.left != null) &#123; queue.add(cur.left); &#125; if (cur.right != null) &#123; queue.add(cur.right); &#125; &#125;&#125; 求二叉树的最大宽度(难)分析：宽度性遍历的时候要知道每一层的节点个数 解：遍历每个节点的时候，知道他在第几层 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// 用hash表的解法public static int maxWidthUseMap(Node head) &#123; if (head == null) &#123; return 0; &#125; Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;(); queue.add(head); // key 在 哪一层，value HashMap&lt;Node, Integer&gt; levelMap = new HashMap&lt;&gt;(); levelMap.put(head, 1); int curLevel = 1; // 当前你正在统计哪一层的宽度 int curLevelNodes = 0; // 当前层curLevel层，宽度目前是多少 int max = 0; while (!queue.isEmpty()) &#123; Node cur = queue.poll(); int curNodeLevel = levelMap.get(cur); if (cur.left != null) &#123; levelMap.put(cur.left, curNodeLevel + 1); queue.add(cur.left); &#125; if (cur.right != null) &#123; levelMap.put(cur.right, curNodeLevel + 1); queue.add(cur.right); &#125; if (curNodeLevel == curLevel) &#123; curLevelNodes++; &#125; else &#123; max = Math.max(max, curLevelNodes); curLevel++; curLevelNodes = 1; &#125; &#125; max = Math.max(max, curLevelNodes); return max;&#125;// 不用hash表的方法 (难度高)public static int maxWidthNoMap(Node head) &#123; if (head == null) &#123; return 0; &#125; Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;(); queue.add(head); Node curEnd = head; // 当前层，最右节点是谁 Node nextEnd = null; // 下一层，最右节点是谁 int max = 0; int curLevelNodes = 0; // 当前层的节点数 while (!queue.isEmpty()) &#123; Node cur = queue.poll(); if (cur.left != null) &#123; queue.add(cur.left); nextEnd = cur.left; &#125; if (cur.right != null) &#123; queue.add(cur.right); nextEnd = cur.right; &#125; curLevelNodes++; if (cur == curEnd) &#123; max = Math.max(max, curLevelNodes); curLevelNodes = 0; curEnd = nextEnd; &#125; &#125; return max;&#125; 折纸问题 结果可看做头节点是凹，所有的左子树头节点都是凹，所有右子树头肩点都是凸的二叉树 中序遍历即可打印出从上到下的所有结构 12345678910111213141516public static void main(String[] args) &#123; int N = 4; process(1, N, true);&#125;// 这个节点在第i层，一共有N层，N固定不变的// 这个节点如果是凹的话，down = T// 这个节点如果是凸的话，down = F// 函数的功能：中序打印以你想象的节点为头的整棵树！public static void process(int i, int N, boolean down) &#123; if (i &gt; N) &#123; return; &#125; process(i + 1, N, true); System.out.print(down ? &quot;凹 &quot; : &quot;凸 &quot;); process(i + 1, N, false);&#125; 搜索二叉树(递归套路) 题目：如何判断一颗二叉树是搜索二叉树？ 搜索二叉树的特点：任何一个节点，左子树的节点一定比它小，右子树的节点一定比它大 解题： 中序遍历一定是升序，如果某个位置有降序，一定不是搜索二叉树 递归套路题解 :向我左树要信息，右树要信息，左树必须是搜索二叉树且左树最大值小于我，右树是搜索二叉树并且最小值大于我；左树信息：1.是否是搜索二叉树，2.最大值，3.最小值；右树也是 注意：递归套路，可以解决一切树形DP 问题，无非是可能性的罗列有难度 12345678910111213141516171819202122232425262728293031323334// 解法1： 额外引入数组// 节点public static class Node &#123; public int value; public Node left; public Node right; public Node(int data) &#123; this.value = data; &#125;&#125;// 中序遍历到一个数组中，然后判断数组是不是升序public static boolean isBST1(Node head) &#123; if (head == null) &#123; return true; &#125; ArrayList&lt;Node&gt; arr = new ArrayList&lt;&gt;(); in(head, arr); for (int i = 1; i &lt; arr.size(); i++) &#123; if (arr.get(i).value &lt;= arr.get(i - 1).value) &#123; return false; &#125; &#125; return true;&#125;// 中序遍历public static void in(Node head, ArrayList&lt;Node&gt; arr) &#123; if (head == null) &#123; return; &#125; in(head.left, arr); arr.add(head); in(head.right, arr);&#125; 12345678910111213141516171819// 解法2：递归方式public static int preValue = Integer.MIN_VALUE;public static boolean checkBST(Node head) &#123; if (head == null) &#123; return true; &#125; boolean checkLeft = checkBST(head.left); if (!checkLeft) &#123; return false; &#125; // 中序打印的时机，换成了中序比较的时机 if (head.value &lt;= preValue) &#123; return false; &#125; else &#123; preValue = head.value; &#125; return checkBST(head.right);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 解法3：递归套路/** * 递归返回值 */public static class Info &#123; public boolean isBST; // 是否是搜索二叉树 public int max; // 最大值 public int min; // 最小值 public Info(boolean i, int ma, int mi) &#123; isBST = i; max = ma; min = mi; &#125;&#125;public static Info process(Node x) &#123; if (x == null) &#123; return null; &#125; Info leftInfo = process(x.left); Info rightInfo = process(x.right); int min = x.value; int max = x.value; // 最小值和最大值就是我当前节点的值和它比较得出的最小值和最大值 if (leftInfo != null) &#123; min = Math.min(min, leftInfo.min); max = Math.max(max, leftInfo.max); &#125; if (rightInfo != null) &#123; min = Math.min(min, rightInfo.min); max = Math.max(max, rightInfo.max); &#125; // 是否是搜索二叉树 boolean isBST = true; // 左边有信息并且左边不是搜索二叉树 if (leftInfo != null &amp;&amp; !leftInfo.isBST) &#123; isBST = false; &#125; if (rightInfo != null &amp;&amp; !rightInfo.isBST) &#123; isBST = false; &#125; // 左边有信息，但是左边的最大值大于等于我的值 if (leftInfo != null &amp;&amp; leftInfo.max &gt;= x.value) &#123; isBST = false; &#125; // 右边有信息，右边的最小值小于等于我当前值 if (rightInfo != null &amp;&amp; rightInfo.min &lt;= x.value) &#123; isBST = false; &#125; return new Info(isBST, max, min);&#125; 完全二叉树 如何判断一个二叉树是完全二叉树 完全二叉树：一颗二叉树从左到右是依次变满的，即使不满，也是变满的样子 解题：二叉树宽度遍历，1.任何一个节点如果有有节点，没左节点，false；2.在第一个条件不违规情况，如果遇到第一个左右两个节点不双全情况，接下来遇到的所有节点，必须是叶子节点 1234567891011121314151617181920212223242526272829303132public static boolean isCBT(Node head) &#123; if (head == null) &#123; return true; &#125; LinkedList&lt;Node&gt; queue = new LinkedList&lt;&gt;(); // 是否遇到过左右两个孩子不双全的节点 boolean leaf = false; Node l = null; Node r = null; queue.add(head); while (!queue.isEmpty()) &#123; head = queue.poll(); l = head.left; r = head.right; // 如果遇到了不双全的节点之后，又发现当前节点不是叶节点 if ((leaf &amp;&amp; (l != null || r != null)) || (l == null &amp;&amp; r != null)) &#123; return false; &#125; if (l != null) &#123; queue.add(l); &#125; if (r != null) &#123; queue.add(r); &#125; // 遇到左右两个节点不双全的情况，修改标记 if (l == null || r == null) &#123; leaf = true; &#125; &#125; return true;&#125; 平衡二叉树 如何判断一棵树是平衡二叉树 平衡二叉树特性：对于任何一个子树来说，左树的高度和右树的高度差，都不超过1 解决思路：假设我可以向我的左树要信息，可以向右树要信息，如果我整棵树是平衡二叉树，我左树得是平的，右树得是平的，对于X节点来说，左树-右树高度差&lt;=1；我向左树要信息：1.是否是平的；2.高度是多少，右树要信息：1.是否是平的；2.高度是多少 12345678910111213141516171819202122232425262728293031323334/** * 返回值信息 */public static class Info &#123; public boolean isBalanced; public int height; public Info(boolean i, int h) &#123; isBalanced = i; height = h; &#125;&#125;public static Info process(Node x) &#123; if (x == null) &#123; return new Info(true, 0); &#125; Info leftInfo = process(x.left); Info rightInfo = process(x.right); // x为头的节点的高度：左树和右树较大的那个高度再加上我自己(+1) int height = Math.max(leftInfo.height, rightInfo.height) + 1; // 是否是平衡树：我左树得是平衡，右树得是平衡树，并且我左树和右树的高度差的绝对值得小于2 boolean isBalanced = true; if (!leftInfo.isBalanced) &#123; isBalanced = false; &#125; if (!rightInfo.isBalanced) &#123; isBalanced = false; &#125; if (Math.abs(leftInfo.height - rightInfo.height) &gt; 1) &#123; isBalanced = false; &#125; return new Info(isBalanced, height);&#125; 满二叉树 如何判断一棵树是满二叉树 树最大深度L，节点个数N，满足N=2(L次方)-1 解法(递归套路)：先求二叉树最大深度L，再求节点个数N，满足N=2(L次方)-1 12345678910111213141516171819202122232425262728293031323334// 满二叉树解法：递归套路/** * 返回值信息 */public static class Info &#123; public int height; public int nodes; public Info(int h, int n) &#123; height = h; nodes = n; &#125;&#125;public static boolean isFull(Node head) &#123; if (head == null) &#123; return true; &#125; Info all = process(head); // N=2(L次方)-1 return (1 &lt;&lt; all.height) - 1 == all.nodes;&#125;public static Info process(Node head) &#123; if (head == null) &#123; return new Info(0, 0); &#125; Info leftInfo = process(head.left); Info rightInfo = process(head.right); // 高度等于左树和右树最高的高度+1 int height = Math.max(leftInfo.height, rightInfo.height) + 1; // 总个数等于左边的个数加上右边的个数加1 int nodes = leftInfo.nodes + rightInfo.nodes + 1; return new Info(height, nodes);&#125; 二叉树最低公共祖先 给定两个二叉树节点node1和node2，找到他们的最低公共祖先节点 解法1：遍历整棵树，把所有节点的父节点都维护到一个Map中，然后找到node1的所有父节点维护到set中，再遍历node2的所有的父，第一个在set中遇到的节点就是最低公共祖先 1234567891011121314151617181920212223242526272829303132333435// 解法1public static Node lca(Node head, Node o1, Node o2) &#123; if (head == null) &#123; return null; &#125; HashMap&lt;Node, Node&gt; parentMap = new HashMap&lt;&gt;(); parentMap.put(head, null); fillParentMap(head, parentMap); HashSet&lt;Node&gt; set = new HashSet&lt;&gt;(); Node cur = o1; set.add(cur); // 只有头节点才等于自己的父，如果当前节点不等于自己的父，就可以往上走 while (null != parentMap.get(cur)) &#123; cur = parentMap.get(cur); set.add(cur); &#125; // o1往上所有节点都在这个set里面，只有最初的head不在里面 cur = o2; while (!set.contains(cur)) &#123; cur = parentMap.get(cur); &#125; return cur;&#125;// 维护整棵树的所有父节点到Map中private static void fillParentMap(Node head, HashMap&lt;Node, Node&gt; fatherMap) &#123; if (head.left != null) &#123; fatherMap.put(head.left, head); fillParentMap(head.left, fatherMap); &#125; if (head.right != null) &#123; fatherMap.put(head.right, head); fillParentMap(head.right, fatherMap); &#125;&#125; 解法2(非常抽象)：可能的情况有1：node1和node2互为公共祖先；2：node1和node2不互为公共祖先； 1234567891011121314/** * 解法2：可能的情况有1：node1和node2互为公共祖先；2：node1和node2不互为公共祖先； */public static Node lowestCommonAncestor(Node head, Node o1, Node o2) &#123; if (head == null || head == o1 || head == o2) &#123; return head; &#125; Node left = lowestCommonAncestor(head.left, o1, o2); Node right = lowestCommonAncestor(head.right, o1, o2); if (left != null &amp;&amp; right != null) &#123; return head; &#125; return left != null ? left : right;&#125; 图 常见的表示图的方法：临接表法，和临接矩阵法，数组等 做模板，然后把所有的图的问题转化为自己熟悉的数据结构，带着模板上考场 图的宽度优先遍历约瑟夫圆环问题逆波兰计算器 实现一个计算器，只有加减乘除法，没有括号，输入是一个字符串如10+2+3*5 解题：表达式转化为后缀表达式（逆波兰表达式） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125// 解题public static void main(String[] args) &#123; String str = &quot;10+2+3*5&quot;; // 表达式转换为操作数和操作符的中缀表达式数组 String[] strArr = changeStrArr(str); // 中缀表达式转化为后缀表达式 List&lt;String&gt; polandList = polandNotation(strArr); System.out.println(polandList); // 后缀表达式计算 System.out.println(calculate(polandList));&#125;/** * 字符串转成中缀的数组 * 只有加减乘除，没有扩号 */private static String[] changeStrArr(String str) &#123; StringBuilder stringBuilder = new StringBuilder(); for (char c : str.toCharArray()) &#123; if (&quot;+-*/&quot;.contains(String.valueOf(c))) &#123; stringBuilder.append(&quot;;&quot;); stringBuilder.append(c); stringBuilder.append(&quot;;&quot;); &#125; else &#123; stringBuilder.append(c); &#125; &#125; return stringBuilder.toString().split(&quot;;&quot;);&#125;/** * 中缀表达式转化为后缀表达式 * 1.初始化两个栈，运算符栈s1和储存中间结果的栈s2 * 2.从左到右扫描中缀表达式 * 3.遇到操作数，入栈s2 * 4.遇到运算符，比较其与s1栈顶运算符的优先级： * (1).如果s1为空，或者栈顶运算符为左括号&quot;(&quot;,直接将运算符入栈； * (2).否则，若优先级比栈顶运算符高，也将运算符入栈s1; * (3).否则，将s1栈顶运算符弹出入栈s2中，再次转到(4-1) 与s1中新的栈顶运算符比较； * 本方法只考虑到加减乘除操作，不考虑扩号 */private static List&lt;String&gt; polandNotation(String[] str) &#123; List&lt;String&gt; s2 = new ArrayList&lt;&gt;(); Stack&lt;String&gt; s1 = new Stack&lt;&gt;(); for (String itm : str) &#123; // 遇到操作数，直接入栈s2 if (itm.matches(&quot;\\\\d+&quot;)) &#123; s2.add(itm); &#125; else &#123; // 如果栈不为空，并且栈顶优先级比我目前运算符优先级高，就把栈顶运算符如s2，然后吧当前运算符如s1 while (!s1.isEmpty() &amp;&amp; Operation.getValue(s1.peek()) &gt;= Operation.getValue(itm)) &#123; s2.add(s1.pop()); &#125; s1.push(itm); &#125; &#125; while (!s1.isEmpty()) &#123; s2.add(s1.pop()); &#125; return s2;&#125;/** * 运算符比较 */private static class Operation &#123; private static int ADD = 1; private static int SUB = 1; private static int MUL = 2; private static int DIV = 2; public static int getValue(String operation) &#123; int result = 0; switch (operation) &#123; case &quot;+&quot;: result = ADD; break; case &quot;-&quot;: result = SUB; break; case &quot;*&quot;: result = MUL; break; case &quot;/&quot;: result = DIV; break; default: break; &#125; return result; &#125;&#125;/** * 逆波兰表达式计算 * 1.定义一个栈，匹配到非运算符就入栈 * 2.遇到运算符就把栈顶两个数字出栈，用后出栈的数和先出栈的数做运算，把运算结果再入栈 * 3.直到最后，栈顶结果即为计算结果 */private static int calculate(List&lt;String&gt; polandList) &#123; Stack&lt;String&gt; stack = new Stack&lt;&gt;(); for (String itm : polandList) &#123; // 匹配的是多位数 if (itm.matches(&quot;\\\\d+&quot;)) &#123; stack.push(itm); &#125; else &#123; // 弹出两个数，并运算，再入栈 int num2 = Integer.parseInt(stack.pop()); int num1 = Integer.parseInt(stack.pop()); int res = 0; if (itm.equals(&quot;+&quot;)) &#123; res = num1 + num2; &#125; else if (itm.equals(&quot;-&quot;)) &#123; res = num1 - num2; &#125; else if (itm.equals(&quot;*&quot;)) &#123; res = num1 * num2; &#125; else if (itm.equals(&quot;/&quot;)) &#123; res = num1 / num2; &#125; else &#123; throw new RuntimeException(&quot;运算符有误&quot;); &#125; stack.push(String.valueOf(res)); &#125; &#125; return Integer.parseInt(stack.pop());&#125; 前缀树1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// 前缀树数据结构public static class Node1 &#123; public int pass; public int end; public Node1[] nexts; // char tmp = &#x27;b&#x27; (tmp - &#x27;a&#x27;) public Node1() &#123; pass = 0; end = 0; // 0 a // 1 b // 2 c // .. .. // 25 z // nexts[i] == null i方向的路不存在 // nexts[i] != null i方向的路存在 nexts = new Node1[26]; &#125;&#125;// 添加元素public void insert(String word) &#123; if (word == null) &#123; return; &#125; char[] str = word.toCharArray(); Node1 node = root; node.pass++; int path = 0; for (int i = 0; i &lt; str.length; i++) &#123; // 从左往右遍历字符 path = str[i] - &#x27;a&#x27;; // 由字符，对应成走向哪条路 if (node.nexts[path] == null) &#123; node.nexts[path] = new Node1(); &#125; node = node.nexts[path]; node.pass++; &#125; node.end++;&#125;// word这个单词之前加入过几次public int search(String word) &#123; if (word == null) &#123; return 0; &#125; char[] chs = word.toCharArray(); Node1 node = root; int index = 0; for (int i = 0; i &lt; chs.length; i++) &#123; index = chs[i] - &#x27;a&#x27;; if (node.nexts[index] == null) &#123; return 0; &#125; node = node.nexts[index]; &#125; return node.end;&#125;// 所有加入的字符串中，有几个是以pre这个字符串作为前缀的public int prefixNumber(String pre) &#123; if (pre == null) &#123; return 0; &#125; char[] chs = pre.toCharArray(); Node1 node = root; int index = 0; for (int i = 0; i &lt; chs.length; i++) &#123; index = chs[i] - &#x27;a&#x27;; if (node.nexts[index] == null) &#123; return 0; &#125; node = node.nexts[index]; &#125; return node.pass;&#125;// 删除public void delete(String word) &#123; if (search(word) != 0) &#123; char[] chs = word.toCharArray(); Node1 node = root; node.pass--; int path = 0; for (int i = 0; i &lt; chs.length; i++) &#123; path = chs[i] - &#x27;a&#x27;; if (--node.nexts[path].pass == 0) &#123; node.nexts[path] = null; return; &#125; node = node.nexts[path]; &#125; node.end--; &#125;&#125; 贪心算法在某一个标准下，优先考虑最满足标准的样本，最后考虑最不满足标准的样本，最终得到一个答案的算法，叫做贪心算法。也就是说，不从整体最优上加以考虑，所做出的是某种意义上的局部最优解。 会议室占用问题问题：只有一个会议室，多个会议占用会议室的时间有冲突，如何让会议室进行的会议最多，返回最多的会议场次 解：哪个会议结束时间早，就优先安排，然后接下来继续找下一个会议结束时间早的会议 1234567891011121314151617181920212223242526272829303132// 会议开始时间和结束时间public static class Program &#123; public int start; public int end; public Program(int start, int end) &#123; this.start = start; this.end = end; &#125;&#125;// 比较器public static class ProgramComparator implements Comparator&lt;Program&gt; &#123; @Override public int compare(Program o1, Program o2) &#123; return o1.end - o2.end; &#125;&#125;// 会议的开始时间和结束时间，都是数值，不会 &lt; 0public static int bestArrange2(Program[] programs) &#123; Arrays.sort(programs, new ProgramComparator()); int timeLine = 0; int result = 0; // 依次遍历每一个会议，结束时间早的会议先遍历 for (int i = 0; i &lt; programs.length; i++) &#123; if (timeLine &lt;= programs[i].start) &#123; result++; timeLine = programs[i].end; &#125; &#125; return result;&#125; 八皇后问题多线程相关三个线程交替打印 三个线程交替打印，1线程打印1；2线程打印2；3线程打印3；1线程打印4……一直打印到100 解题方式由很多，无非就是涉及到线程通信问题以及修改共享变量问题 题解1：不加锁，利用线程可见性 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class ThreadPrint &#123; private static volatile int i = 0; private static volatile int flag = 0; public static void main(String[] args) &#123; Thread thread1 = new Thread(new Thread1()); Thread thread2 = new Thread(new Thread2()); Thread thread3 = new Thread(new Thread3()); thread1.start(); thread2.start(); thread3.start(); &#125; public static class Thread1 implements Runnable &#123; public void run() &#123; while (i &lt;= 100) &#123; if (flag == 0) &#123; System.out.println(&quot;t1=&quot; + i); i++; flag = 1; &#125; &#125; &#125; &#125; public static class Thread2 implements Runnable &#123; public void run() &#123; while (i &lt;= 100) &#123; if (flag == 1) &#123; System.out.println(&quot;t2=&quot; + i); i++; flag = 2; &#125; &#125; &#125; &#125; public static class Thread3 implements Runnable &#123; public void run() &#123; while (i &lt;= 100) &#123; if (flag == 2) &#123; System.out.println(&quot;t3=&quot; + i); i++; flag = 0; &#125; &#125; &#125; &#125;&#125; 题解2：LockSupport 实现 123456789101112131415161718192021222324252627282930313233343536373839404142public class ThreadPrint2 &#123; private static int number = 1; private static Thread thread1, thread2,thread3; public static void main(String[] args) &#123; thread1 = new Thread(ThreadPrint2::thread1, &quot;thread1&quot;); thread2 = new Thread(ThreadPrint2::thread2, &quot;thread2&quot;); thread3 = new Thread(ThreadPrint2::thread3, &quot;thread3&quot;); thread1.start(); thread2.start(); thread3.start(); &#125; public static void thread1() &#123; while (ThreadPrint2.number &lt;= 100) &#123; System.out.println(&quot;thread1:&quot; + ThreadPrint2.number); ThreadPrint2.number++; LockSupport.unpark(thread2); LockSupport.park(); &#125; &#125; public static void thread2() &#123; while (ThreadPrint2.number &lt;= 100) &#123; LockSupport.park(); System.out.println(&quot;thread2:&quot; + ThreadPrint2.number); ThreadPrint2.number++; LockSupport.unpark(thread3); &#125; &#125; public static void thread3() &#123; while (ThreadPrint2.number &lt;= 100) &#123; LockSupport.park(); System.out.println(&quot;thread3:&quot; + ThreadPrint2.number); ThreadPrint2.number++; LockSupport.unpark(thread1); &#125; &#125;&#125; 题解3：经典实现 实现wait-&gt;notifyAll 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class ThreadPrint3 &#123; private static int count = 1; public static void main(String[] args) &#123; Object lock = new Object(); new Thread(() -&gt; &#123; try &#123; synchronized (lock) &#123; while (count &lt;= 100) &#123; if (count % 3 == 1) &#123; System.out.println(Thread.currentThread().getName() + &quot;---&gt;&quot; + count); count++; &#125; lock.notifyAll(); lock.wait(); &#125; lock.notifyAll(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, &quot;T1&quot;).start(); new Thread(() -&gt; &#123; try &#123; synchronized (lock) &#123; while (count &lt;= 100) &#123; if (count % 3 == 2) &#123; System.out.println(Thread.currentThread().getName() + &quot;---&gt;&quot; + count); count++; &#125; lock.notifyAll(); lock.wait(); &#125; lock.notifyAll(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, &quot;T2&quot;).start(); new Thread(() -&gt; &#123; try &#123; synchronized (lock) &#123; while (count &lt;= 100) &#123; if (count % 3 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;---&gt;&quot; + count); count++; &#125; lock.notifyAll(); lock.wait(); &#125; lock.notifyAll(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, &quot;T3&quot;).start(); &#125;&#125; 题解4：Lock实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class ThreadPrint4 &#123; private static int cnt = 0; public static void main(String[] args) &#123; Lock lock = new ReentrantLock(); Condition c1 = lock.newCondition(); Condition c2 = lock.newCondition(); Condition c3 = lock.newCondition(); new Thread(() -&gt; &#123; lock.lock(); try &#123; while (cnt &lt;= 100 &amp;&amp; cnt % 3 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;----&gt;&quot; + cnt); cnt++; c2.signal(); c1.await(); &#125; c3.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;, &quot;T1&quot;).start(); new Thread(() -&gt; &#123; lock.lock(); try &#123; while (cnt &lt;= 100 &amp;&amp; cnt % 3 == 1) &#123; System.out.println(Thread.currentThread().getName() + &quot;----&gt;&quot; + cnt); cnt++; c3.signal(); c2.await(); &#125; c1.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;, &quot;T2&quot;).start(); new Thread(() -&gt; &#123; lock.lock(); try &#123; while (cnt &lt;= 100 &amp;&amp; cnt % 3 == 2) &#123; System.out.println(Thread.currentThread().getName() + &quot;----&gt;&quot; + cnt); cnt++; c1.signal(); c3.await(); &#125; c2.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;, &quot;T3&quot;).start(); &#125;&#125; 其他算法题","categories":[{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://zhangxin66666.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"mysql面试","slug":"7.mysql/面试-mysql","date":"2022-03-30T16:00:00.000Z","updated":"2024-08-22T03:23:14.704Z","comments":true,"path":"2022/03/31/7.mysql/面试-mysql/","link":"","permalink":"https://zhangxin66666.github.io/2022/03/31/7.mysql/%E9%9D%A2%E8%AF%95-mysql/","excerpt":"","text":"1.索引底层结构&amp;行格式&amp;页格式文章内容包含索引结构，联合索引结构，聚簇索引、非聚簇索引区别，在页上如何搜索具体数据过程，一次完成的索引检索过程 地址：行页索引底层结构 2.一条 SQL 的执行过程详解 从准备更新一条数据到事务的提交的流程描述 首先执行器根据 MySQL 的执行计划来查询数据，先是从缓存池中查询数据，如果没有就会去数据库中查询，如果查询到了就将其放到缓存池中 在数据被缓存到缓存池的同时，会写入 undo log 日志文件 更新的动作是在 BufferPool 中完成的，同时会将更新后的数据添加到 redo log buffer 中 完成以后就可以提交事务，在提交的同时会做以下三件事 将redo log buffer中的数据刷入到 redo log 文件中 将本次操作记录写入到 bin log文件中 将 bin log 文件名字和更新内容在 bin log 中的位置记录到redo log中，同时在 redo log 最后添加 commit 标记 至此表示整个更新事务已经完成 为什么Mysql不能直接更新磁盘上的数据而且设置这么一套复杂的机制来执行SQL了？ 因为来一个请求就直接对磁盘文件进行随机读写，然后更新磁盘文件里的数据性能可能相当差。因为磁盘随机读写的性能是非常差的，所以直接更新磁盘文件是不能让数据库抗住很高并发的。 Mysql这套机制看起来复杂，但它可以保证每个更新请求都是更新内存BufferPool，然后顺序写日志文件，同时还能 保证各种异常情况下的数据一致性。 更新内存的性能是极高的，然后顺序写磁盘上的日志文件的性能也是非常高的，要远高于随机读写磁盘文件。 正是通过这套机制，才能让我们的MySQL数据库在较高配置的机器上每秒可以抗下几干的读写请求。 细节介绍server层连接器、词法分析器、优化器、执行器、查询缓存功能见单独文章 地址：mysql执行流程 3.ACID及实现原理 （Atomicity）原子性： 事务是最小的执行单位，不允许分割。要么全都执行,要么全都不执行；——-&gt;原子性是由undolog日志来保证的，它记录了需要回滚的日志信息，事务回滚时，撤销已经执行成功的sql （Consistency）一致性： 执行事务前后，数据保持一致；——-&gt;原子性、隔离性、持久性就是为了来保证一致性 （Isolation）隔离性： 并发访问数据库时，一个事务不被其他事务所干扰，数据库系统提供一定的隔离机制,保证事务在不受外部并发操作影响的“独立”环境执行；——-&gt;锁机制和MVCC共同实现 （Durability）持久性: 一个事务被提交之后。对数据库中数据的改变是持久的，即使数据库发生故障也不会受到影响。——-&gt;持久性由redolog来保证的，mysql修改数据的时候会在redolog中记录一份日志数据，就算数据没有保存成功，只要日志保存成功了，数据仍然不会丢失。 4.MVCCMVCC叫做多版本并发控制，实际上就是保存了数据在某个时间节点的快照。 MVCC主要解决三个问题： 第一、通过MVCC可以解决读写并发阻塞问题，从而提高数据库的并发处理能力； 第二、MVCC采用的是乐观锁的方式实现，降低了死锁的概率； 第三、解决了一致性读的问题，也就是事务启动的时候，根据某个条件去读取到的数据，知道事务结束的时候再去执行相同条件，还是读到同一份数据，不会发生变化变化。 MVCC用于读已提交（RC）和可重复读级别（RR）的控制，主要通过undo log日志版本链和read view来实现， InnoDB中MVCC的实现依赖四个条件: 隐藏字段, 事务链表，read view, undo log 数据表额外字段 DB_TRX_ID(6字节): 在innoDB中, 会为每个事物分配一个事务ID. 而该字段表示的就是插入或更新该行的最后一个事务的事务标识符. DB_ROLL_PTR(7字节): 这个字段称为回滚指针, 指向了回滚段的撤销日志. (undo log) DB_ROW_ID(6字节): 这个字段包含了一个行ID. 该ID在插入新行时会单调增加. 这个字段是为了在表里没有主键ID的时候, 生成聚簇索引使用的. rowid存的是什么？ rowid默认是主键，如果没有主键，会选择唯一键，如果没有唯一键，会生成6字节的rowid undo日志版本链是指一行数据被多个事务依次修改过后，在每个事务修改完后，Mysql会保留修改前的数据undo回滚 日志，并且用两个隐藏字段trx_id和roll_pointer把这些undo日志串联起来形成一个历史记录版本链 在可重复读隔离级别（RR），当事务开启，执行任何查询sql时会生成当前事务的一致性视图read-view，该视图在事务结束 之前都不会变化(如果是读已提交隔离级别在每次执行查询sql时都会重新生成)，这个视图由执行查询时所有未提交事 务id数组（数组里最小的id为min_id）和已创建的最大事务id（max_id）组成，事务里的任何sql查询结果需要从对应 版本链里的最新数据开始逐条跟read-view做比对从而得到最终的快照结果。 版本链比对规则： 如果 row 的 trx_id 落在绿色部分( trx_id&lt;min_id )，表示这个版本是已提交的事务生成的，这个数据是可见的； 如果 row 的 trx_id 落在红色部分( trx_id&gt;max_id )，表示这个版本是由将来启动的事务生成的，是不可见的(若 row 的 trx_id 就是当前自己的事务是可见的）；3. 如果 row 的 trx_id 落在黄色部分(min_id &lt;=trx_id&lt;= max_id)，那就包括两种情况 ​ a. 若 row 的 trx_id 在视图数组中，表示这个版本是由还没提交的事务生成的，不可见(若 row 的 trx_id 就是当前自 己的事务是可见的)； ​ b. 若 row 的 trx_id 不在视图数组中，表示这个版本是已经提交了的事务生成的，可见。 对于删除的情况可以认为是update的特殊情况，会将版本链上最新的数据复制一份，然后将trx_id修改成删除操作的 trx_id，同时在该条记录的头信息（record header）里的（deleted_flag）标记位写上true，来表示当前记录已经被 删除，在查询时按照上面的规则查到对应的记录如果delete_flag标记位为true，意味着记录已被删除，则不返回数 据。 注意：begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个修改操作InnoDB表的语句， 事务才真正启动，才会向mysql申请事务id，mysql内部是严格按照事务的启动顺序来分配事务id的。 总结： MVCC机制的实现就是通过read-view机制与undo版本链比对机制，使得不同的事务会根据数据版本链对比规则读取 同一条数据在版本链上的不同版本数据。 5.mysql三大日志地址：mysql三大日志 6.redo日志地址：redo日志 7.undo日志地址：undo日志 8.能说下myisam 和 innodb的区别吗 区别 InnoDB MyISAM 事务 支持 不支持 外键 支持 不支持 索引 聚簇索引和非聚簇索引 非聚簇索引 行锁 支持 不支持 表锁 支持 支持 存储文件 frm(表结构)，ibd(数据和索引) frm，myi(索引文件)，myd(数据文件) 具体行数 全表扫描统计行数 通过变量保存行数 MyISAM不支持事务，在执行查询语句SELECT前，会自动给涉及的所有表加读锁,在执行update、insert、delete操作会自动给涉及的表加写锁。 InnoDB在执行查询语句SELECT时(非串行隔离级别)，(因为有mvcc机制)不会加锁。但是update、insert、delete操作会加行锁。简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁则会把读和写都阻塞。 9.说下mysql的索引有哪些吧，聚簇和非聚簇索引又是什么，有什么区别聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因 10.什么是覆盖索引和回表覆盖索引指的是在一次查询中，如果一个索引包含或者说覆盖所有需要查询的字段的值，我们就称之为覆盖索引，而不再需要回表查询。 辅助索引的存在并不影响数据在聚集索引中的组织，因此每张表上可以有多个辅助 索引。当通过辅助索引来寻找数据时，InnoDB存储引擎会遍历辅助索引并通过叶级别的指针获得指向主键索引的主键，然后再通过主键索引（聚集索引）来找到一个完整的行记录。这个过程也被称为回表。也就是根据辅助索引的值查询一条完整的用户记录需要使用到2棵B+树—-一次辅助索引，一次聚集索引。 11.mysql的各种锁。 间隙锁锁分类图解 按照兼容性分类 InnoDB实现了以下两种类型的行锁。 「共享锁（S)」：又称读锁 (read lock)，是读取操作创建的锁。其他用户可以并发读取数据， 但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。当如果事务对读锁进行修改操作，很可能会造成死锁。 「排他锁（X)」：exclusive lock（也叫writer lock）又称写锁。 若某个事物对某一行加上了排他锁，只能这个事务对其进行读写，在此事务结束之前， 其他事务不能对其进行加任何锁，其他进程可以读取,不能进行写操作，需等待其释放。 「排它锁是悲观锁的一种实现」。 另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁。 「意向共享锁（IS）」：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。 「意向排他锁（IX）」：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。 意向锁是有数据引擎自己维护的，用户无法手动操作意向锁，在为数据行加共享 / 排他锁之前，InooDB 会先获取该数据行所在在数据表的对应意向锁。 「InnoDB行锁模式兼容性列表」 请求锁模式 是否兼容当前锁模式 X IX S IS X 冲突 冲突 冲突 冲突 IX 冲突 兼容 冲突 兼容 S 冲突 冲突 兼容 兼容 IS 冲突 兼容 兼容 兼容 如果一个事务请求的锁模式与当前的锁兼容，InnoDB就将请求的锁授予该事务；反之，如果两者不兼容，该事务就要等待锁释放。 意向锁是InnoDB自动加的，不需用户干预。对于UPDATE、DELETE和INSERT写操作，InnoDB会自动给涉及数据集加排他锁（X)； 对于普通SELECT语句，InnoDB不会加任何锁；事务可以通过以下语句显示给记录集加共享锁或排他锁。 共享锁（S）：SELECT * FROM table_name WHERE … LOCK IN SHARE MODE。 排他锁（X)：SELECT * FROM table_name WHERE … FOR UPDATE。 用SELECT … IN SHARE MODE获得共享锁，主要用在需要数据依存关系时来确认某行记录是否存在， 并确保没有人对这个记录进行UPDATE或者DELETE操作。但是如果当前事务也需要对该记录进行更新操作， 则很有可能造成死锁，对于锁定行记录后需要进行更新操作的应用，应该使用SELECT… FOR UPDATE方式获得排他锁。 InnoDB在事务执行过程中，使用两阶段锁协议： 随时都可以执行锁定，InnoDB会根据隔离级别在需要的时候自动加锁； 锁只有在执行commit或者rollback的时候才会释放，并且所有的锁都是在同一时刻被释放。 按算法分类 「Record Lock」： 单个行记录上的锁 锁总会锁住索引记录，锁住的是key。 如果InnoDB存储引擎表在建立的时候没有设置任何一个索引，那么这时InnoDB会使用隐式的主键进行锁定。 如果要锁的没有索引，则会进行全表记录加锁。 「Gap Lock」 ：间隙锁，锁定一个范围，但不包含记录本身 锁定索引记录间隙，确保索引记录的间隙不变 间隙锁时针对事务隔离级别为可重复读或以上级别而配的 1Gap Lock在InnoDB的唯一作用就是防止其他事务的插入操作，以此防止幻读 「Next-Key Lock」：临键锁，Gap Lock + Record Lock，锁定一个范围，并且包含记录本身 当查询的索引含有唯一属性时，InnoDB存储引擎会对Next-Key Lock 进行优化，将其降级为 Record Lock，即仅锁住索引本身，而不是范围。 当查询的索引为辅助索引时，默认使用Next-Key Locking技术进行加锁，锁定范围是前一个索引到后一个索引之间范围。 12.MRR每次从二级索引中读取到一条记录后，就会根据该记录的主键值 执行回表操作。而在某个扫描区间中的二级索引记录的主键值是无序的，也就是说这些 二级索引记录对应的聚簇索引记录所在的页面的页号是无序的。 每次执行回表操作时都相当于要随机读取一个聚簇索引页面，而这些随机IO带来的 性能开销比较大。MySQL中提出了一个名为Disk-Sweep Multi-Range Read (MRR，多范围 读取)的优化措施，即先读取一部分二级索引记录，将它们的主键值排好序之后再统一执 行回表操作。 相对于每读取一条二级索引记录就立即执行回表操作，这样会节省一些IO开销。使用这个 MRR优化措施的条件比较苛刻，所以我们直接认为每读取一条二级索引记录就立即执行回表操作。MRR的详细信息，可以查询官方文档。 13.分库分表14.分布式唯一id15.数据库三大范式是什么第一范式：每个列都不可以再拆分。 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。 在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。 16.InnoDB引擎的4大特性插入缓冲（insert buffer) 二次写(double write) 自适应哈希索引(ahi) 预读(read ahead) 17.索引失效的情况谈到索引主要从两方面来分析：IO 和数据结构：不管索引数据还是行数据，都是存在磁盘里面的，我们尽可能少的取出数据 IO—–&gt;读取次数少、量少——&gt;分块读取——&gt;局部性原理、磁盘预读 数据结构——&gt;B+树——&gt;二叉树、AVL树、红黑树、B树 组合索引不遵循最左匹配原则 组合索引前面索引列使用范围查询(&lt;,&gt;,like),会导致后续索引失效； 不要在索引上做任何操作（计算，函数，类型转换） is null和is not null 无法使用索引 尽量少使用or操作符，否则连接时索引会失效 字符串不添加引号会导致索引失效（隐式类型转换） 两表关联使用的条件字段中字段的长度，编码不一致会导致索引失效 like语句中，以%开头的模糊查询会导致索引失效 如果mysql中使用全表扫描比索引快，也会导致索引失效 （force index:强制使用索引） 18.最左前缀原则。 联合索引使用分析实战顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式 实战？？ 19.sql优化之explain 详解在 select 语句之前增加 explain 关键字，MySQL 会在查询上设置一个标记，执行查询会返回执行计划的信息，而不是执行这条SQL 接下来我们将展示 explain 中每个列的信息 19.1. id列 id列的编号是 select 的序列号，有几个 select 就有几个id，并且id的顺序是按 select 出现的顺序增长的。 id列越大执行优先级越高，id相同则从上往下执行，id为NULL最后执行。 19.2. select_type列 select_type 表示对应行是简单还是复杂的查询。 1）simple： 简单查询。查询不包含子查询和union 11 mysql&gt; explain select * from film where id = 2; 2）primary： 复杂查询中最外层的 select 3）subquery： 包含在 select 中的子查询（不在 from 子句中） 4）derived： 包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表（derived的英文含义） 用这个例子来了解 primary、subquery 和 derived 类型 1231 mysql&gt; set session optimizer_switch=&#x27;derived_merge=off&#x27;; #关闭mysql5.7新特性对衍生表的合 并优化 2 mysql&gt; explain select (select 1 from actor where id = 1) from (select * from film where id = 1) der; 11 mysql&gt; set session optimizer_switch=&#x27;derived_merge=on&#x27;; #还原默认配置 5）union： 在 union 中的第二个和随后的 select 11 mysql&gt; explain select 1 union all select 1; 19.3. table列 这一列表示 explain 的一行正在访问哪个表。 当 from 子句中有子查询时，table列是 格式，表示当前查询依赖 id=N 的查询，于是先执行 id=N 的查 询。 当有 union 时，UNION RESULT 的 table 列的值为&lt;union1,2&gt;，1和2表示参与 union 的 select 行id。 19.4. type列 这一列表示关联类型或访问类型，即MySQL决定如何查找表中的行，查找数据行记录的大概范围。 依次从最优到最差分别为：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL 一般来说，得保证查询达到range级别，最好达到ref NULL： mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可 以单独查找索引来完成，不需要在执行时访问表 1 mysql&gt; explain select min(id) from film; const, system： mysql能对查询的某部分进行优化并将其转化成一个常量（可以看show warnings 的结果）。用于 primary key 或 unique key 的所有列与常数比较时，所以表最多有一个匹配行，读取1次，速度比较快。system是 const的特例，表里只有一条元组匹配时为system 1 mysql&gt; explain extended select * from (select * from film where id = 1) tmp; 1 mysql&gt; show warnings; eq_ref： primary key 或 unique key 索引的所有部分被连接使用 ，最多只会返回一条符合条件的记录。这可能是在 const 之外最好的联接类型了，简单的 select 查询不会出现这种 type。 1 mysql&gt; explain select * from film_actor left join film on film_actor.film_id = film.id; ref： 相比 eq_ref，不使用唯一索引，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会 找到多个符合条件的行。 简单 select 查询，name是普通索引（非唯一索引） 1 mysql&gt; explain select * from film where name = ‘film1’; 2.关联表查询，idx_film_actor_id是film_id和actor_id的联合索引，这里使用到了film_actor的左边前缀film_id部分。 1 mysql&gt; explain select film_id from film left join film_actor on film.id = film_actor.fi lm_id; range： 范围扫描通常出现在 in(), between ,&gt; ,&lt;, &gt;= 等操作中。使用一个索引来检索给定范围的行。 1 mysql&gt; explain select * from actor where id &gt; 1;index：扫描全索引就能拿到结果，一般是扫描某个二级索引，这种扫描不会从索引树根节点开始快速查找，而是直接 对二级索引的叶子节点遍历和扫描，速度还是比较慢的，这种查询一般为使用覆盖索引，二级索引一般比较小，所以这 种通常比ALL快一些。 1 mysql&gt; explain select * from film; ALL： 即全表扫描，扫描你的聚簇索引的所有叶子节点。通常情况下这需要增加索引来进行优化了。 1 mysql&gt; explain select * from actor; 19.5. possible_keys列 这一列显示查询可能使用哪些索引来查找。 explain 时可能出现 possible_keys 有列，而 key 显示 NULL 的情况，这种情况是因为表中数据不多，mysql认为索引 对此查询帮助不大，选择了全表查询。 如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查 where 子句看是否可以创造一个适当的索引来提 高查询性能，然后用 explain 查看效果。 19.6. key列 这一列显示mysql实际采用哪个索引来优化对该表的访问。 如果没有使用索引，则该列是 NULL。如果想强制mysql使用或忽视possible_keys列中的索引，在查询中使用 force index、ignore index。 19.7. key_len列 这一列显示了mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列。 举例来说，film_actor的联合索引 idx_film_actor_id 由 film_id 和 actor_id 两个int列组成，并且每个int是4字节。通 过结果中的key_len=4可推断出查询使用了第一个列：film_id列来执行索引查找。 11 mysql&gt; explain select * from film_actor where film_id = 2; key_len计算规则如下： 字符串，char(n)和varchar(n)，5.0.3以后版本中，n均代表字符数，而不是字节数，如果是utf-8，一个数字或字母占1个字节，一个汉字占3个字节 ​ char(n)：如果存汉字长度就是 3n 字节 ​ varchar(n)：如果存汉字则长度是 3n + 2 字节，加的2字节用来存储字符串长度，因为varchar是变长字符串 数值类型 ​ tinyint：1字节 ​ smallint：2字节 ​ int：4字节 ​ bigint：8字节 时间类型 ​ date：3字节timestamp：4字节 ​ datetime：8字节 如果字段允许为 NULL，需要1字节记录是否为 NULL 索引最大长度是768字节，当字符串过长时，mysql会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索引。 19.8. ref列 这一列显示了在key列记录的索引中，表查找值所用到的列或常量，常见的有：const（常量），字段名（例：film.id） 19.9. rows列 这一列是mysql估计要读取并检测的行数，注意这个不是结果集里的行数。 19.10. Extra列 这一列展示的是额外信息。常见的重要值如下： 1）Using index：使用覆盖索引 覆盖索引定义：mysql执行计划explain结果里的key有使用索引，如果select后面查询的字段都可以从这个索引的树中获取，这种情况一般可以说是用到了覆盖索引，extra里一般都有using index；覆盖索引一般针对的是辅助索引，整个查询结果只通过辅助索引就能拿到结果，不需要通过辅助索引树找到主键，再通过主键去主键索引树里获取其它字段值 11 mysql&gt; explain select film_id from film_actor where film_id = 1; 2）Using where：使用 where 语句来处理结果，并且查询的列未被索引覆盖 11 mysql&gt; explain select * from actor where name = &#x27;a&#x27;; 3）Using index condition：查询的列不完全被索引覆盖，where条件中是一个前导列的范围； 11 mysql&gt; explain select * from film_actor where film_id &gt; 1; 4）Using temporary：mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索 引来优化。 actor.name没有索引，此时创建了张临时表来distinct 11 mysql&gt; explain select distinct name from actor; film.name建立了idx_name索引，此时查询时extra是using index,没有用临时表 11 mysql&gt; explain select distinct name from film; 5）Using filesort：将用外部排序而不是索引排序，数据较小时从内存排序，否则需要在磁盘完成排序。这种情况下一般也是要考虑使用索引来优化的。 actor.name未创建索引，会浏览actor整个表，保存排序关键字name和对应的id，然后排序name并检索行记录 11 mysql&gt; explain select * from actor order by name; film.name建立了idx_name索引,此时查询时extra是using index 11 mysql&gt; explain select * from film order by name; 6）Select tables optimized away：使用某些聚合函数（比如 max、min）来访问存在索引的某个字段是 11 mysql&gt; explain select min(id) from film; 20.mysql主从复制原理 master提交完事务后，写入binlog slave连接到master，获取binlog master创建dump线程，推送binglog到slave slave启动一个IO线程读取同步过来的master的binlog，记录到relay log中继日志中 slave再开启一个sql线程读取relay log事件并在slave执行，完成同步 slave记录自己的binglog 21.主从同步延时如何解决22.隔离级别首先回忆四种mysql隔离级别 隔离级别 说明 读未提交（Read uncommitted） 一个事务还没提交时，它做的变更就能被别的事务看到 读提交（Read committed） 一个事务提交之后，它做的变更才会被其他事务看到。出 于性能考虑互联网公司会使用RC+乐观锁 可重复读（Repeatable read） 一个事务中，对同一份数据的读取结果总是相同的，无论是否有其他事务对这份数据进行操作，以及这个事务是否提交。InnoDB默认级别。 串行化（Serializable ） 事务串行化执行，每次读都需要获得表级共享锁，读写相互都会阻塞，隔离级别最高，牺牲系统并发性。 查看当前数据库的事务隔离级别:show variables like ‘%tx_isolation%’; 设置事务隔离级别：set tx_isolation=’REPEATABLE-READ’; 不同的隔离级别是为了解决不同的问题。也就是脏读、幻读、不可重复读。 脏读：事务A读取到了事务B已经修改但尚未提交的数据，。 不可重复读：事务A内部的相同查询语句在不同时刻读出的结果不一致，不符合隔离性。 幻读：事务A读取到了事务B提交的新增数据，不符合隔离性 不可重复读&amp;&amp;幻读区别 不可重复读，重点是修改。在同一事务中，同样的条件，第一次读的数据和第二次读的数据不一样，数据变化 幻读，重点在于新增或者删除。在同一事务中，同样的条件,，第一次和第二次读出来的记录数不一样，行数变化。 隔离级别 脏读 不可重复读 幻读 读未提交 可以出现 可以出现 可以出现 读提交 不允许出现 可以出现 可以出现 可重复读 不允许出现 不允许出现 可以出现 序列化 不允许出现 不允许出现 不允许出现 不同的隔离级别下，隔离性是靠锁机制和MVCC共同实现的。 23.RC和RR的本质区别。readview层面RC级别下每次都是重新生成read view，而在RR级别下则是一直使用事务内第一次查询时产生的read view。 24.什么是readviewReadView可以理解为一个数据结构，在事务开始的时候会根据事务链表构造一个ReadView,初始化方法如下： 12345678910111213// readview 初始化// m_low_limit_id = trx_sys-&gt;max_trx_id; // m_up_limit_id = !m_ids.empty() ? m_ids.front() : m_low_limit_id;ReadView::ReadView() : m_low_limit_id(), m_up_limit_id(), m_creator_trx_id(), m_ids(), m_low_limit_no()&#123; ut_d(::memset(&amp;m_view_list, 0x0, sizeof(m_view_list)));&#125; trx_ids: 表示事务开启的时候, 其它未提交的活跃的事务ID. 这个集合中的事务对于当前事务来说, 一直都是不可见的. low_limit_id: 表示在生成ReadView时当前系统中最大事务id. up_limit_id: 表示未提交活跃事务Id(trx_ids)的最大值. 如果trx_ids为空, 则up_limit_id=low_limit_id. 通过该ReadView，新的事务可以根据查询到的所有活跃事务记录的事务ID来匹配能够看见该记录，从而实现数据库的事务隔离，主要逻辑如下： 通过聚簇索引的行结构中DB_TRX_ID隐藏字段可以知道最近被哪个事务ID修改过。一个新的事务开始时会根据事务链表构造一个ReadView。当前事务根据ReadView中的数据去跟检索到的每一条数据去校验,看看当前事务是不是能看到这条数据 1234567891011121314151617181920212223242526// 判断数据对应的聚簇索引中的事务id在这个readview中是否可见bool changes_visible( trx_id_t id, // 记录的id const table_name_t&amp; name) constMY_ATTRIBUTE((warn_unused_result))&#123; ut_ad(id &gt; 0); // 如果当前记录id &lt; 事务链表的最小值或者等于创建该readview的id就是它自己,那么是可见的 if (id &lt; m_up_limit_id || id == m_creator_trx_id) &#123; return(true); &#125; check_trx_id_sanity(id, name); // 如果该记录的事务id大于事务链表中的最大值,那么不可见 if (id &gt;= m_low_limit_id) &#123; return(false); // 如果事务链表是空的,那也是可见的 &#125; else if (m_ids.empty()) &#123; return(true); &#125; const ids_t::value_type* p = m_ids.data(); //判断是否在ReadView中，如果在说明在创建ReadView时 此条记录还处于活跃状态则不应该查询到，否则说明创建ReadView是此条记录已经是不活跃状态则可以查询到 return(!std::binary_search(p, p + m_ids.size(), id));&#125; 可见性算法逻辑总结： 当检索到的数据的事务ID小于事务链表中的最小值(数据行的DB_TRX_ID &lt; m_up_limit_id)表示这个数据在当前事务开启前就已经被其他事务修改过了,所以是可见的。 当检索到的数据的事务ID表示的是当前事务自己修改的数据(数据行的DB_TRX_ID = m_creator_trx_id) 时，数据可见。 当检索到的数据的事务ID大于事务链表中的最大值(数据行的DB_TRX_ID &gt;= m_low_limit_id) 表示这个数据在当前事务开启后到下一次查询之间又被其他的事务修改过,那么就是不可见的。 如果事务链表为空,那么也是可见的,也就是当前事务开始的时候,没有其他任意一个事务在执行。 当检索到的数据的事务ID在事务链表中的最小值和最大值之间，从m_low_limit_id到m_up_limit_id进行遍历，取出DB_ROLL_PTR指针所指向的回滚段的事务ID，把它赋值给 trx_id_current ，然后从步骤1重新开始判断，这样总能最后找到一个可用的记录。 25.快照读与当前读的区别对于这种读取历史数据的方式，我们叫它快照读 (snapshot read)，而读取数据库当前版本数据的方式，叫当前读 (current read)。很显然，在MVCC中： 快照读：就是select select * from table ….; 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，处理的都是当前的数据，需要加锁。 select * from table where ? lock in share mode; select * from table where ? for update; insert; update ; delete; 26.幻读和不可重复读区别不可重复读，重点是修改。在同一事务中，同样的条件，第一次读的数据和第二次读的数据不一样，数据变化 幻读，重点在于新增或者删除。在同一事务中，同样的条件,，第一次和第二次读出来的记录数不一样，行数变化。 27.MySQL在RR级别中完全解决了幻读的问题么？28.脏页是什么，刷脏页时机29.bufferpool30.几大核心线程purge线程 对于一条delete语句 delete from t where a = 1，如果列a有聚集索引，则不会进行真正的删除，而只是在主键列等于1的记录delete flag设置为1，即记录还是存在在B+树中。而对于update操作，不是直接对记录进行更新，而是标识旧记录为删除状态，然后新产生一条记录。那这些旧版本标识位删除的记录何时真正的删除？怎么删除？ 其实InnoDB是通过undo日志来进行旧版本的删除操作的，在InnoDB内部，这个操作被称之为purge操作，原来在srv_master_thread主线程中完成，后来进行优化，开辟了purge线程进行purge操作，并且可以设置purge线程的数量。purge操作每10s进行一次。 为了节省存储空间，InnoDB存储引擎的undo log设计是这样的：一个页上允许多个事务的undo log存在。虽然这不代表事务在全局过程中提交的顺序，但是后面的事务产生的undo log总在最后。此外，InnoDB存储引擎还有一个history列表，它根据事务提交的顺序，将undo log进行连接，如下面的一种情况： 在执行purge过程中，InnoDB存储引擎首先从history list中找到第一个需要被清理的记录，这里为trx1，清理之后InnoDB存储引擎会在trx1所在的Undo page中继续寻找是否存在可以被清理的记录，这里会找到事务trx3，接着找到trx5，但是发现trx5被其他事务所引用而不能清理，故再去history list中取查找，发现最尾端的记录时trx2，接着找到trx2所在的Undo page，依次把trx6、trx4清理，由于Undo page2中所有的记录都被清理了，因此该Undo page可以进行重用。 InnoDB存储引擎这种先从history list中找undo log，然后再从Undo page中找undo log的设计模式是为了避免大量随机读操作，从而提高purge的效率。 31.事务传播机制32.分布式事务通用方案33.成本分析34.表中数据量如何计算总量35.count(1) count(*) count(id) count(字段) 性能对比分析四个sql的执行计划一样，说明这四个sql执行效率应该差不多 字段有索引：count(*)≈count(1)&gt;count(字段)&gt;count(主键 id) //字段有索引，count(字段)统计走二级索引，二 级索引存储数据比主键索引少，所以count(字段)&gt;count(主键 id) 字段无索引：count(*)≈count(1)&gt;count(主键 id)&gt;count(字段) //字段没有索引count(字段)统计走不了索引， count(主键 id)还可以走主键索引，所以count(主键 id)&gt;count(字段) count(1)跟count(字段)执行过程类似，不过count(1)不需要取出字段统计，就用常量1做统计，count(字段)还需要取出 字段，所以理论上count(1)比count(字段)会快一点。 count(*) 是例外，mysql并不会把全部字段取出来，而是专门做了优化，不取值，按行累加，效率很高，所以不需要用 count(列名)或count(常量)来替代 count(*)。 为什么对于count(id)，mysql最终选择辅助索引而不是主键聚集索引？因为二级索引相对主键索引存储数据更少，检索 性能应该更高，mysql内部做了点优化(应该是在5.7版本才优化)。 36.mysql中in 和exists 区别37.join查询原理38.索引下推索引下推（Index Condition Pushdown，ICP), like KK%其实就是用到了索引下推优化 什么是索引下推了？ 对于辅助的联合索引(name,age,position)，正常情况按照最左前缀原则，SELECT * FROM employees WHERE name like ‘LiLei%’ AND age = 22 AND position =’manager’ 这种情况只会走name字段索引，因为根据name字段过滤完，得到的索引行里的age和 position是无序的，无法很好的利用索引。 在MySQL5.6之前的版本，这个查询只能在联合索引里匹配到名字是 ‘LiLei’ 开头的索引，然后拿这些索引对应的主键逐个回表，到主键索 引上找出相应的记录，再比对age和position这两个字段的值是否符合。 MySQL 5.6引入了索引下推优化，可以在索引遍历过程中，对索引中包含的所有字段先做判断，过滤掉不符合条件的记录之后再回表，可 以有效的减少回表次数。使用了索引下推优化后，上面那个查询在联合索引里匹配到名字是 ‘LiLei’ 开头的索引之后，同时还会在索引里过 滤age和position这两个字段，拿着过滤完剩下的索引对应的主键id再回表查整行数据。 索引下推会减少回表次数，对于innodb引擎的表索引下推只能用于二级索引，innodb的主键索引（聚簇索引）树叶子节点上保存的是全 行数据，所以这个时候索引下推并不会起到减少查询全行数据的效果。 个人理解升华： 我觉得like是这样，用到索引下推的场景，直接利用三个索引都过滤玩，然后回表查数据; 关闭索引下推，利用的第一个索引完成过滤，直接回表，然后继续在server层对第二三字段过滤,都是用到了三个字段，但是第二三个字段用的地方不一样 为什么范围查找Mysql没有用索引下推优化？ 然后理论上like和大于小于都是range类型，只不过大于小于这种有点正负无穷的感觉，所以mysql没有特殊处理，like ‘xx%’ mysql默认认为范围可控，就使用刚才的逻辑来处理，当然这也不是绝对的，有时like KK% 也不一定就会走索引下推。 39.两种排序方式1、MySQL支持两种方式的排序filesort和index，Using index是指MySQL扫描索引本身完成排序。index 效率高，filesort效率低。 40.filesort排序原理Using filesort文件排序原理详解 filesort文件排序方式 单路排序：是一次性取出满足条件行的所有字段，然后在sort buffer中进行排序；用trace工具可 以看到sort_mode信息里显示&lt; sort_key, additional_fields &gt;或者&lt; sort_key, packed_additional_fields &gt; 双路排序（又叫回表排序模式）：是首先根据相应的条件取出相应的排序字段和可以直接定位行 数据的行 ID，然后在 sort buffer 中进行排序，排序完后需要再次取回其它需要的字段；用trace工具 可以看到sort_mode信息里显示&lt; sort_key, rowid &gt; MySQL 通过比较系统变量 max_length_for_sort_data(默认1024字节) 的大小和需要查询的字段总大小来 41.filesort判断使用哪种排序模式。 （单路or双路）如果 字段的总长度小于max_length_for_sort_data ，那么使用 单路排序模式； 如果 字段的总长度大于max_length_for_sort_data ，那么使用 双路排序模∙式。 40.单路排序、双路排序详细过程我们先看单路排序的详细过程： 从索引name找到第一个满足 name = ‘zhuge’ 条件的主键 id 根据主键 id 取出整行，取出所有字段的值，存入 sort_buffer 中 从索引name找到下一个满足 name = ‘zhuge’ 条件的主键 id 重复步骤 2、3 直到不满足 name = ‘zhuge’ 对 sort_buffer 中的数据按照字段 position 进行排序 返回结果给客户端 我们再看下双路排序的详细过程： 从索引 name 找到第一个满足 name = ‘zhuge’ 的主键id 根据主键 id 取出整行，把排序字段 position 和主键 id 这两个字段放到 sort buffer 中 从索引 name 取下一个满足 name = ‘zhuge’ 记录的主键 id 重复 3、4 直到不满足 name = ‘zhuge’ 对 sort_buffer 中的字段 position 和主键 id 按照字段 position 进行排序 遍历排序好的 id 和字段 position，按照 id 的值回到原表中取出 所有字段的值返回给客户端 其实对比两个排序模式，单路排序会把所有需要查询的字段都放到 sort buffer 中，而双路排序只会把主键和需要排序的字段放到 sort buffer 中进行排序，然后再通过主键回到原表查询需要的字段。 41.三星索引对于一个查询而言，一个三星索引，可能是其最好的索引。 索引将相关的记录放到一起则获得一星； 如果索引中的数据顺序和查找中的排列顺序一致则获得二星； 如果索引中的列包含了查询中需要的全部列则获得三星。 42.场景题新建一张数据表user，后续所有操作都依托于初始化的这三条数据。 id name age 1 zhangsan 1 2 lisi 5 3 wangwu 10 操作1： 时刻 t1 t2 1 begin begin 2 update user set name=’zhangsan2’ where age=1;commit; 3 select * from user where age=1; 此时不管是RC还是RR，t1的select都能够读取到t2update的值 因为在t1select的时刻才会去生成read view，根据可见性算法得出能看到最新的t2数据 操作2: 时刻 t1 t2 1 begin begin 2 select * from user where age=1; 3 update user set name=’zhangsan2’ where age=1;commit; 4 select * from user where age=1; 此时RC级别下，t1的 第二次select能够查看到t2update的值，因为RC级别下每次select都会生成新的read view 在RR级别下，t1的 第二次不能查询到t2update的值，因为RR级别下的后面查询都是使用的第一次select生成的read view 同理，t2语句为insert时也是一样的情况。 操作3: 时刻 t1 t2 1 begin begin update user set name=’zhangsan2’ where age=1; 3 insert into user values (4,’haha’,3);waiting~~~ 5 commit; 6 插入成功commit; RR级别下，t1进行当前读时，会对数据加锁，具体是间隙锁｜行锁｜next-key，后面细说，此时t2的插入会waiting，直到t1释提交事务，来解决insert情况下的幻读问题 那么，感觉RR就已经解决了幻读，为啥还需要串行化（Serializable ）？？？ 操作4: 时刻 t1 t2 1 begin begin select * from user where age=1; 3 insert into user values (4,’haha’,1);commit; 5 update user set name=’zhangsan2’ where age=1; 6 commit; RR级别下，此时想象中的结果为下面，因为实际上在第五行如果不是update而是select，是肯定差不到新数据的 id name age 1 zhangsan2 1 2 lisi 5 3 wangwu 10 4 haha 1 但是实际的数据库中结果为： id name age 1 zhangsan2 1 2 lisi 5 3 wangwu 10 4 zhangsan2 1 其实在 MySQL在RR级别中并不是完全解决了幻读的问题，对照操作2中t2使用insert、操作3、以及操作4可以看出某些场景下RR解决了幻读，但是MVCC 对于幻读的解决是不彻底的。（快照读和当前读的混合使用会产生幻读问题）","categories":[{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"}]},{"title":"业务架构-稳定性建设方法论","slug":"11.方法论/业务架构-稳定性建设方法论","date":"2022-01-16T16:00:00.000Z","updated":"2024-08-22T03:23:14.703Z","comments":true,"path":"2022/01/17/11.方法论/业务架构-稳定性建设方法论/","link":"","permalink":"https://zhangxin66666.github.io/2022/01/17/11.%E6%96%B9%E6%B3%95%E8%AE%BA/%E4%B8%9A%E5%8A%A1%E6%9E%B6%E6%9E%84-%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%BB%BA%E8%AE%BE%E6%96%B9%E6%B3%95%E8%AE%BA/","excerpt":"","text":"先介绍几个概念，同步一下认知： 容灾：是指系统冗余部署，当一处由于意外停止工作，整个系统应用还可以正常工作。 容错：是指在运行中出现错误（如上下游故障或概率性失败）仍可正常提供服务。 可用性：描述的是系统可提供服务的时间长短。用公式来说就是正常工作时间/(正常工作时间+故障时间)。 可靠性：描述的是系统指定时间单位内无故障的次数。比如：一年365天，以天为单位来衡量。有天发生了故障，哪怕只有1秒，这天算不可靠。其他没有故障的是可靠的。 稳定性：这个业界没有明确的定义，我的理解是：在受到各种干扰时仍然能够提供符合预期的服务的能力。 一、容灾建设1.代码容灾 现代互联网公司最值钱的就是程序员写出来的代码啦，所以我们需要对代码进行容灾备份，防止放代码的服务器哪天一不高兴宕机异或爆炸啦数据都丢了就完蛋了，现在代码同步的方案比较多，是在不行运维人员写个脚本，每天或者几个小时自动同步一下全量代码到另一个机房还是可以做到滴。 2.服务容灾服务容灾的解决方案就是冗余。多几个备份来切换。 2.1同城容灾同城 容灾 是在同城或相近区域内 （ ≤ 200K M ）建立两个数据中心 : 一个为数据中心，负责日常生产运行 ; 另一个为灾难备份中心，负责在灾难发生后的应用系统运行。同城灾难备份的数据中心与灾难备份中心的距离比较近，通信线路质量较好，比较容易实现数据的同步 复制 ，保证高度的数据完整性和数据零丢失。同城灾难备份一般用于防范火灾、建筑物破坏、供电故障、计算机系统及人为破坏引起的灾难 2.2异地/跨洋容灾异地 容灾 主备中心之间的距离较远 （＞ 200KM ) ， 因此一般采用异步镜像，会有少量的数据丢失。异地灾难备份不仅可以防范火灾、建筑物破坏等可能遇到的风险隐患，还能够防范战争、地震、水灾等风险。由于同城灾难备份和异地灾难备份各有所长，为达到最理想的防灾效果，数据中心应考虑采用同城和异地各建立一个灾难备份中心的方式解决。 2.3两地三中心两地三中心 ： 是指 同城双中心 加 异地灾备 一种商用容灾备份解决方案； 两地 是指同城、异地； 三中心 是指生产中心、同城容灾中心、异地容灾中心。（ 生产中心、同城灾备中心、异地 灾备 中心 ） 结合近年国内出现的大范围自然灾害，以同城双中心加异地灾备中心的 “两地三中心”的灾备模式也随之出现，这一方案兼具高可用性和灾难备份的能力。 同城双中心 是指在同城或邻近城市建立两个可独立承担关键系统运行的数据中心，双中心具备基本等同的业务处理能力并通过高速链路实时同步数据，日常情况下可同时分担业务及管理系统的运行，并可切换运行；灾难情况下可在基本不丢失数据的情况下进行灾备应急切换，保持业务连续运行。与异地灾备模式相比较，同城双中心具有投资成本低、建设速度快、运维管理相对简单、可靠性更高等优点。 异地灾备中心 是指在异地的城市建立一个备份的灾备中心，用于双中心的数据备份，当双中心出现自然灾害等原因而发生故障时，异地灾备中心可以用备份数据进行业务的恢复。 二、服务治理1.链路梳理需要梳理核心接口服务的血缘依赖关系、相关干系人及联系方式，做到心中有数，关键接口调用链路流程尽量短，下游依赖接口tp999尽量越快越好。 2.强弱依赖分析强依赖：假定服务A依赖于服务B，服务B出现故障不可用时，服务A也不可用，通常服务A会返回错误信息，我们称这种依赖为强依赖。 弱依赖：假定服务A依赖于服务B，服务B出现故障不可用时，服务A仍然可用，通常服务A会返回正确信息，只是与服务B相关的信息会不返回或者做默认处理，我们称这种依赖为弱依赖。 通过下面的流程图，我们来分析一下强弱依赖。从图中可以看到，在服务A中调用了服务B和服务C，但是他们的处理逻辑是不一样的。 1、调用服务B：如果调用成功，则接着调用服务C；如果调用失败，则服务A直接结束。这种场景我们称之为服务A强依赖于服务B。 2、调用服务C：不管调用成功还是失败，服务A会接续执行后续逻辑处理。这种场景我们称之为服务A弱依赖于服务C。 12345678910111213141516171819public void serviceA()&#123; try &#123; System.out.println(&quot;it`s serviceA&quot;); /// serviceA 强依赖于 serviceB serviceB(); try &#123; /// serviceA 弱依赖于 serviceC serviceC(); &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; System.out.println(&quot;xxxxxxx&quot;); &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; 在代码的处理上，可以分为一下三种模式：1、弱依赖：服务A的主流程不依赖服务B的返回结果。该场景可以有两种解决方式：1）可以启动单独的线程进行服务B调用。2）在当前线程中发消息，在消息消费线程中访问服务B。 2、弱依赖：服务A的主流程依赖服务B的返回结果。与强依赖处理有些类似，一般建议对服务B做降级处理，根据请求超时时间、并发请求数、请求失败数、请求返回的错误码做降级(或者熔断)处理。同时使用默认值来代替服务B的返回结果，默认值的设置需要根据具体的业务场景进行分析。 3、强依赖：服务A的主流程依赖服务B的返回结果。一般建议对服务A整体做降级处理，请求返回的错误码。 在系统设计时一定要考虑系统的强弱依赖关系，根据需要采用不同的处理方案。 3.异步化建设普通流量下的接口流程设计上可以一个接口，做了好多好多事情，之后接口同步实时返回处理成功或者处理失败。在高并发场景下是非常糟糕的设计，接口一方面业务逻辑非常的重，另一方面，同步接口依赖的很多的三方接口，也可能会多次的操作数据库，一个点出问题都会造成整个业务的瘫痪。此时可以把整个流程异步化，同步接口里面只做一些校验工作以及数据的保存，之后发送mq触发后续流程的执行，此时实时接口返回处理中，异步流程中处理完毕后通过回调的方式通知调用方结果，同时提供查询接口供调用方主动获取结果。 需要注意的是因为是通过发mq的方式触发后续流程，那么就要保证mq一定发送成功，此处牵扯到分布式事务相关的方案，不同的业务逻辑可能会使用不同的解决办法。（本地事务表或者依靠状态来驱动都可以） 4.切量/降级/限流/熔断4.1切量切量是新业务上线初期，或者大版本更新的时候，会对用户进行切量开放xx功能，用少量的生产流量验证流程，如果没有问题慢慢切量剩余用户，如果有问题，也会将问题影响面降到最低。 4.2降级降级是通过开关配置将某些不重要的业务功能屏蔽掉，以提高服务处理能力。 在大促场景中经常会对某些服务进行降级处理，大促结束之后再进行复原。 本系统或者下游系统出现问题时，当前系统可以手动对服务进行降级操作，问题修复后关闭降级。 大促场景时会对日志打印进行降级，防止磁盘io对系统造成影响，另外也会避免正好大促时磁盘空间满了，系统没来得及回收日志造成服务不可用。 为什么要降级在不影响业务核心链路的情况下，屏蔽某些不重要的业务功能，可以节省系统的处理时间，提供系统的响应能力，在服务器资源固定的前提下处理更多的请求。 4.3限流限流是针对服务请求数量的一种自我保护机制，当请求数量超出服务的处理能力时，会自动丢弃新来的请求。 为什么要限流任何一个系统的处理能力都是有极限的，假定服务A的处理能力为TPS=100，当TPS&lt;100时服务A可以提供正常的服务。当TPS&gt;100时，由于请求量增大，会出现争抢服务资源的情况（数据库连接、CPU、内存等），导致服务A处理缓慢；当TPS继续增大时，可能会造成服务A响应更加缓慢甚至奔溃。如果不进行限流控制，服务A始终会面临着被大流量冲击的风险。所以我们需要做好系统请求流量的评估，制定合理的限流策略。 4.4熔断在服务的依赖调用中，被调用方出现故障时，出于自我保护的目的，调用方会主动停止调用，并根据业务需要进行相应处理。调用方这种主动停止调用的行为我们称之为熔断。 为什么要熔断假定服务A依赖服务B，当服务B处于正常状态，整个调用是健康的，服务A可以得到服务B的正常响应。当服务B出现故障时，比如响应缓慢或者响应超时，如果服务A继续请求服务B，那么服务A的响应时间也会增加，进而导致服务A响应缓慢。如果服务A不进行熔断处理，服务B的故障会传导至服务A，最终导致服务A也不可用。 5.traceid日志中串一次请求的id，系统并发的时候，多次请求的日志会交叉打印，排查问题会比较难受，可以在代码中加入traceid，可以串起来一次完整请求，有条件的可以把traceid进行传递，这样就能用traceid把一起完整的请求在各应用系统中一起串起来，也就实现了链路追踪功能。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172//Constant中的TRACE_ID定义//日志中的traceidpublic static final String TRACE_ID = &quot;traceId&quot;;//traceidInterceptorimport org.slf4j.MDC;import org.springframework.util.StringUtils;import org.springframework.web.servlet.HandlerInterceptor;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.util.UUID;@WebServletpublic class TraceIDInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; String traceID = request.getHeader(Constant.TRACE_ID); if (StringUtils.isEmpty(traceID)) &#123; traceID = UUID.randomUUID().toString(); &#125; MDC.put(Constant.TRACE_ID, traceID); return true; &#125;&#125;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.InterceptorRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;@Configurationpublic class InterceptorConfig implements WebMvcConfigurer &#123; @Override public void addInterceptors(InterceptorRegistry interceptorRegistry) &#123; // 多个拦截器组成一个拦截器链 // addPathPatterns 用于添加拦截规则 // excludePathPatterns 用户排除拦截 interceptorRegistry.addInterceptor(new TraceIDInterceptor()).addPathPatterns(&quot;/**&quot;); &#125;&#125;//如果feign方式调用往下游传递的可以加上下面的Interceptorimport feign.RequestInterceptor;import feign.RequestTemplate;import org.slf4j.MDC;import org.springframework.context.annotation.Configuration;import org.springframework.util.StringUtils;import java.util.UUID;@Configurationpublic class FeignInterceptor implements RequestInterceptor &#123; @Override public void apply(RequestTemplate requestTemplate) &#123; String traceID = MDC.get(Constant.TRACE_ID); if (StringUtils.isEmpty(traceID)) &#123; traceID = UUID.randomUUID().toString(); &#125; requestTemplate.header(Constant.HEADER_TRACE_ID, traceID); &#125;&#125;//日志配置文件中加入[%X&#123;traceId&#125;]//此时接口调用的traceid加完了，但是例如mq、调度之类的可能不行，需要在此类代码入口手动设置traceidMDC.put(Constant.TRACE_ID, UUID.randomUUID().toString()); 6.方法执行时间注解一个接口调用了一次数据库查询，一次数据库保存，两次rpc。某一天上游反馈接口慢，该如何进行排查呢？ 优化接口慢，那就得找到影响接口耗时的关键点，我们如果知道了数据库查询耗时，数据库保存耗时，两次rpc的分别耗时，是否问题就迎刃而解了呢 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//自定义Timer注解import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;/** * 标注需要记录时间消耗的方法 */@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface Timer &#123;&#125;//aop环绕获取方法执行耗时import lombok.extern.slf4j.Slf4j;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.springframework.stereotype.Component;/** * 时间记录切面，收集接口的运行时间 */@Slf4j@Aspect@Componentpublic class TimeAspect &#123; // 修正Timer注解的全局唯一限定符 @Around(&quot;@annotation(com.renrenche.pricing.process.annotation.Timer)&quot;) public Object around(ProceedingJoinPoint joinPoint) throws Throwable &#123; // 获取目标类名称 String clazzName = joinPoint.getTarget().getClass().getName(); // 获取目标类方法名称 String methodName = joinPoint.getSignature().getName(); long start = System.currentTimeMillis();// log.info(&quot;================================&#123;&#125;: &#123;&#125;: start...&quot;, clazzName, methodName); // 调用目标方法 Object result = joinPoint.proceed(); long time = System.currentTimeMillis() - start; log.info(&quot;耗时统计:clazzName:&#123;&#125;,methodName:&#123;&#125;,time:&#123;&#125; ms&quot;, clazzName, methodName, time); return result; &#125;&#125;//用法，在需要监控耗时的方法上添加@Timer注解即可耗时统计:clazzName:com.xx.xx.XX,methodName:queryXX,time:10 ms耗时统计:clazzName:com.xx.xx.XX,methodName:saveXX,time:20 ms耗时统计:clazzName:com.xx.xx.YY,methodName:queryYY,time:50 ms耗时统计:clazzName:com.xx.xx.ZZ,methodName:queryZZ,time:500 ms我们只需要去找下游zz系统负责人沟通接口耗时问题即可 7.业务自定义异常优化关于系统内部自定义业务异常，单系统内部定义一个即可，另外需要重写Throwable中fillInStackTrace方法，原因有二，一方面原fillInStackTrace方法被synchronized修饰，另一方面fillInStackTrace方法中设置了很多堆栈信息，对于自定义业务异常来说是不太需要的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 自定义业务异常 */public class BizException extends RuntimeException &#123; private static final long serialVersionUID = 1L; private String errorCode; private String errorMessage; public BizException(String errorCode, String errorMessage) &#123; this.errorMessage = errorMessage; this.errorCode = errorCode; &#125; public String getErrorCode() &#123; return this.errorCode; &#125; public void setErrorCode(String errorCode) &#123; this.errorCode = errorCode; &#125; public String getErrorMessage() &#123; return this.errorMessage; &#125; public void setErrorMessage(String errorMessage) &#123; this.errorMessage = errorMessage; &#125; //重写以提高性能 @Override public Throwable fillInStackTrace() &#123; return this; &#125; @Override public String toString() &#123; return &quot;AppRuntimeException&#123;&quot; + &quot;errorCode=&#x27;&quot; + errorCode + &#x27;\\&#x27;&#x27; + &quot;, errorMessage=&#x27;&quot; + errorMessage + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 8.错误文案转换器提供给app前端或者web管理后台相关的写接口，接口返回错误码的时候，前端同学一般会tost提示用户错误信息，一般情况下前端同学要么全部提示系统太火爆了，要么会跟后端同学要接口的错误码列表，然后跟产品一起确认每个错误码该tost什么信息，之后if else 错误码和tost文案一一映射，如果后端新加了错误码，需要前端一起更改，没有及时通知，可能前端页面会弹出技术性文案。 如果所有的友好性提示统一是后端下发呢？是不是就可以规避类似的问题，前端同学只需关心如果正常返回，提示成功，接口没有成功，直接tost后端返回的错误msg提示即可，你好我好大家好～ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475###############yml文件配置################错误信息转换，给前端以友好提示errormsgconfig: errorMsgConvertMap: #key以controller_method命名 PricingController_setPricing: default: 系统太火爆了，请您稍后再试~ 0: 定价成功 1: 系统太火爆了，请您稍后再试~ 5: 请刷新后重试 PricingController_getPricingOrderDetail: default: 系统太火爆了，请您稍后再试~ -42: 没有权限！###############config文件###############@Slf4j@ConfigurationProperties(prefix = &quot;errormsgconfig&quot;)@Component@Setter@Getterpublic class ErrorMsgConvertConf &#123; private Map&lt;String, Map&lt;String,String&gt;&gt; errorMsgConvertMap;&#125;###############错误文案转换Adapter###############/** * 错误信息转换，给前端以友好提示 */@Slf4j@Servicepublic class ErrorMsgAdapter &#123; @Autowired private ErrorMsgConvertConf errorMsgConvertConf; /** * 错误信息转换，给前端以友好提示 */ @Timer public void handlerErrorMsg(String convertKey, Response res) &#123; Map&lt;String, Map&lt;String, String&gt;&gt; errorMsgConvertMap = errorMsgConvertConf.getErrorMsgConvertMap(); Map&lt;String, String&gt; convertMap = errorMsgConvertMap.get(convertKey); if (StringUtils.isBlank(convertKey) || Objects.isNull(res)) &#123; log.info(&quot;ErrorMsgAdapter.handlerErrorMsg,参数为空，直接返回&quot;); return; &#125; log.info(&quot;出参返回信息转换开始，convertKey:&#123;&#125;,oldRes:&#123;&#125;&quot;, convertKey, JSONObject.toJSONString(res)); if (ErrorCode.OK.getCode() == res.getStatus() &amp;&amp; !CollectionUtils.isEmpty(convertMap)) &#123; String codeMsg = convertMap.get(Integer.toString(ErrorCode.OK.getCode())); if (StringUtils.isNotBlank(codeMsg)) &#123; res.setErrMsg(codeMsg); &#125; &#125; else if (ErrorCode.OK.getCode() != res.getStatus() &amp;&amp; !CollectionUtils.isEmpty(convertMap)) &#123; String defaultMsg = convertMap.get(&quot;default&quot;); if (StringUtils.isBlank(defaultMsg)) &#123; defaultMsg = &quot;系统太火爆了，请您稍后再试~&quot;; &#125; String codeMsg = convertMap.get(Integer.toString(res.getStatus())); if (StringUtils.isNotBlank(codeMsg)) &#123; res.setErrMsg(codeMsg); &#125; else &#123; res.setErrMsg(defaultMsg); &#125; &#125; log.info(&quot;出参返回信息转换结束，convertKey:&#123;&#125;,oldRes:&#123;&#125;&quot;, convertKey, JSONObject.toJSONString(res)); &#125;&#125;###############使用方法#################controller出参前调用即可，convertKey建议使用controllerName_methodName//错误码转换errorMsgAdapter.handlerErrorMsg(&quot;PricingController_setPricing&quot;, ret); 9.validationutil很多人喜欢在controller层的req前面增加 @Validated，直接对入参进行格式校验，但是这样做如果字段格式不符合要求，不会进行入参打印，不利于问题排查，另一方面抛出的异常也不是我们指定的。 12345678910111213141516171819202122232425262728293031import com.renrenche.pricing.process.exception.BizException;import com.renrenche.rest.protocol.ErrorCode;import lombok.extern.slf4j.Slf4j;import org.springframework.util.CollectionUtils;import javax.validation.ConstraintViolation;import javax.validation.Validation;import javax.validation.Validator;import javax.validation.ValidatorFactory;import java.util.Set;@Slf4jpublic class ValidationUtil &#123; private static ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); public static &lt;T&gt; void validate(T t) &#123; Validator validator = factory.getValidator(); Set&lt;ConstraintViolation&lt;T&gt;&gt; constraintViolations = validator.validate(t); if(!CollectionUtils.isEmpty(constraintViolations))&#123; StringBuilder errMsg=new StringBuilder(&quot;参数校验失败&quot;); for (ConstraintViolation&lt;T&gt; constraintViolation : constraintViolations) &#123; errMsg.append(&quot;,&quot;).append(constraintViolation.getMessage()); &#125; log.info(&quot;ValidationUtil校验参数失败&quot;); throw new BizException(ErrorCode.PARAM_INVALID, errMsg.toString()); //此处可以定义一个专门的业务异常码值来代表参数异常，msg放的是全量信息，可以for循环返回全量未通过原因，也可以只返回单条，不过多次请求返回的信息会有不同。 &#125; &#125;&#125;//用法 代码显示调用ValidationUtil.validate(xxx); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import com.renrenche.pricing.process.validation.ValidationUtil;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;import org.hibernate.validator.constraints.Length;import javax.validation.constraints.*;import java.io.Serializable;import java.math.BigDecimal;import java.util.List;@Setter@Getter@NoArgsConstructorpublic class DemoReq implements Serializable &#123; @NotBlank(message = &quot;name不能为空&quot;) @Length(min = 6, max = 30, message = &quot;name应该在6-30字符之间&quot;) private String name; @NotNull(message = &quot;年龄不能为空&quot;) @Min(value = 18, message = &quot;年龄必须大于等于18&quot;) @Max(value = 20, message = &quot;年龄必须小于等于20&quot;) private Integer age; @NotEmpty(message = &quot;nameList不能为空&quot;) private List&lt;String&gt; nameList; @Pattern(regexp = &quot;^(.+)@(.+)$&quot;, message = &quot;邮箱的格式不合法&quot;) private String email; private Byte type; private Integer type2; @Pattern(regexp = &quot;aaaaa|bbbbb|ccccc&quot;, message = &quot;type3不在枚举范围内&quot;) private String type3; @DecimalMin(value = &quot;150&quot;, message = &quot;必须大于等于150&quot;) @DecimalMax(value = &quot;300&quot;, message = &quot;必须小于等于300&quot;) @Digits(integer=3,fraction = 2,message = &quot;整数位上限为3位，小数位上限为2位&quot;) private BigDecimal money; public static void main(String[] args) &#123; DemoReq demoReq=new DemoReq(); demoReq.setName(&quot;1111111&quot;);// demoReq.setType3(&quot;222&quot;); ValidationUtil.validate(demoReq); &#125; //如果有对象实体，需要深入校验对象内部字段格式的，需要在实体上增加@valid注解&#125; 10.统一日志打印各公司内部应该针对于日志打印格式形成一个统一的规范，方便日志采集，不管是最终收集到公司内部自建日志平台，还是elk，格式还是得统一的。 抛砖引玉： log4j2 配置： 1Pattern: &quot;[%X&#123;trace_id&#125;] %-d&#123;yyyy-MM-dd HH:mm:ss&#125; - [%p] [%C&#123;1&#125; %M] %m%n&quot; logback配置 1&lt;pattern&gt;[%X&#123;trace_id&#125;] %d&#123;yyyy-MM-dd HH:mm:ss&#125; - [%level] [%class&#123;0&#125; %method] %msg%n&lt;/pattern&gt; 日志示例： [ed02f4ae5f18415b8a0143100b585f0e] 2019-03-14 18:32:08 - [INFO] [LogController test] this is my log!!! 另外很关键的日志打印必不可少，包括自己接口的入参&amp;返回，以及调用三方rpc接口的入参&amp;返回，用于查询线上问题，跟三方扯皮 ，屡试不爽。 11.接入统一日志平台之前服务是单体架构，一个服务就对应一台服务器，排插生产问题时，登陆服务器即可。 现在都是微服务架构，一个服务一般对应多台服务器，jd、阿里之类的核心系统服务器一个项目都是上千上万台，此时排查问题如果一台一台登陆去查找就根本没法处理了。 所以一般公司内部都会有自建日志平台或者elk来支持生产日志的存储及查询。 一般的一个查询流程：根据关键词查询到单条日志，获取到traceid，然后继续根据traceid查询出来本次请求的全量日志进行排查。 12.日志级别动态变更一般有三个很实际的场景会使用，一个是大促（双十一、618），一个是秒杀，另一个是线上出问题但是还未定位到问题时。 前两种是因为用户合法请求非常多，打印的日志非常多，最后一种是用户会频繁点击，流量激增且错误日志打印非常多。此时都会造成机器io压力非常大。再介绍一下一般情况下运维清理服务器日志的流程，一般是定时每隔xx时间去扫描服务器磁盘空间剩余百分比，到达阈值会进行日志备份，之后删除服务器上日志。但是上面三种情况下，会有两个问题，首先是磁盘io会非常高，会影响接口性能，另外在运维同学脚本扫描的间隙，磁盘满了，此时服务就废了，这种关键时间点出事情，P0少不了，轻则个人拜拜，重则部门解散，后果很严重。 一般会对日志进行级别的动态调整，平时线上为info，大促时降到error，线上大流量服务出问题时，可以关闭error日志。更优化的一个解决方案时不按照项目级别统一对日志级别进行调整，还是针对单个的logger类来进行调整，这样会更有针对性一些，因为日志降级是个双刃剑，会提升系统性能，但是同样会丢失日志。 13.丝滑上线三个方面。 技术层面。 业务层面。发版经验 。 技术层面： 以Nacos注册中心为例，服务a有两台服务器，一般上线的时候大家就直接先部署服务器1，启动成功后再部署服务器2，此时会有什么问题呢，直接重启服务器1的时候，Nacos是不知道服务器1下线了，还会把请求转发到服务器1上面，过一段时间Nacos心跳检测发现服务器1宕机了，此时才会停止转发请求到服务器1，但是上游来看已经有超时失败的请求了。 一个合理的上线流程是先在Nacos上对服务器1的机器进行下线，观察服务器1日志，确认无请求进入后，重启服务器1，此时Nacos会自动重新上线服务器1，之后观察服务器1有正常请求进入后，在Nacos上对服务器2的机器进行下线，观察服务器2的日志，确认无请求进入后，重启服务器2，确保服务器2请求正常进入，本次发版结束。 为啥需要Nacos上对服务器下线以后还要继续观察日志呢，因为在Nacos下线一台服务器后，实际生效会有一个短暂的间隔。 上面这样做对于一个服务就三五台服务器的机器来说，还算有可操作性，大厂里面服务器都是几十台起步，有的上千台，这样搞效率很低，所以一般每个公司都会有自建的发版平台（eg：devops），在对机器发版的时候，会自动进行服务流量下线、发版、上线这样的操作，对用户来讲就是选择几台机器，然后发版就ok了。 业务层面： 经常会有的一个需求是对某个接口的流程进行升级，而且还有可能是颠覆性的，此时如果直接上线，出点问题，整个流程都废了，P0怕是跑不了。 一般这种颠覆性或者重构性质的升级会保留老流程的逻辑，这就保证了有问题的快速流程切换，另外一方面会对新老流程进行切量，上线后都是100%老流程，之后1%新流量，观察，看请求是否符合预期，有问题，立马100%切老流程，没问题，接口逻辑正常，符合预期，继续5%，10%，直到100%新流程。 发版经验： 发版的时候一定要先只发一台，完了以后观察本次代码修改部分是否正常，然后也要观察error日志，看是否有影响其他流程，观察一段时间无问题，在批量对其他服务器进行发版。 有些公司服务器会分组，有的按机房分组，有的按调用方分组，此时不同分组的配置文件可能是会有些不同的，所以此时发版需要每个分组的机器各自发版一台，按上述流程进行观察，无问题后，对其他机器进行批量发版。配置文件的少配，配错经常发生，对线上操作一定要保持敬畏之心。 14.服务隔离核心服务需要分组部署提供服务，两种场景可以使用，服务是公共服务，其中一个业务方特别关键，公司黄金链路种的一环，另一个是多个业务方，其中一个量级特别大，其他的加起来也就一丢丢，此时可以进行服务隔离。 类似于dubbo类型的服务可以分不同别名来供不同业务方使用来做到服务隔离 http类型的服务可以根据业务情况部署多套，举个例子：nginx A转发到服务器1和2，nginx B转发到服务器3和4，不用业务方调用不同的请求链接来进行服务隔离 15.远程rpc调用超时时间及重试控制为什么要进行超时设置呢？ 由于下游服务响应过慢、线程死锁、线上BUG等一系列原因导致我们作为调用方调用之后一直没有响应。从而最终导致用户请求长时间得不到响应，同时长时间占有线程资源甚至会拖垮整个服务影响其他正常请求。所以我们应该进行超时设置，这是微服务中快速失败的一个基本手段。 第二个问题，我们该如何设置呢？ 通常有两个层面的超时：服务级超时、接口级别的超时。服务级别的超时时间一般以最慢接口的耗时时间为基准。而且大部分框架都只会有服务级的超时设置，很少有接口级的超时设置。接口级的超时设置，没有必要所有接口都设置，仅针对流量极大、性能要求较高的接口进行单独设置就行。 第三个问题，服务超时时我们该如何处理呢？ 基于这个这个问题，我们需要从上下游、流量、业务场景这三个维度进行考量。 超时的处理手段一般有两种：重试、异常快速返回。写场景通常不要重试（会有幂等相关问题，不同接口幂等实现方式不同）。流量较大的接口一般不适合重试，直接返回异常就好，最极端的场景下容易导致流量翻n倍，导致后面的链路奔溃。 具体超时时间和重试次数建议根据下游接口的压测TP999来进行设置。 eg：a服务依赖b（读）、c（读）、d（写）服务，b服务tp999为15ms，c服务tp999为100ms，d服务tp999为20ms，a服务要求tp999必须在200ms以内，那么我们可以这样设置： b服务（超时时间20ms，重试一次） C服务（超时时间120ms，不重试） d服务（超时时间30ms，不重试） b极端情况20*2+c极端120+d极端30=190ms&lt;200ms 16.幂等处理接口的幂等一般采用两层防护措施进行防重设计。 首先一般接口会加上调用请求流水号字段 1.Redis的分布式锁来解决同类型请求的并发（相同用户并发请求，看情况，有的业务也可以不需要，因为毕竟有数据看兜底） 2.1分布式锁中按照请求流水号查询数据，有记录则直接返回现有数据状态及数据，无记录继续往下执行业务逻辑 2.2分布式锁中执行业务逻辑，然后保存业务数据，通过抓取保存时数据库报唯一键值冲突错来达到幂等处理，返回数据现有情况 3.DB层，请求唯一流水号设置唯一索引，保证DB底层只有一条数据入库。 通常情况幂等操作会按照上述逻辑进行，相同流水号怎么请求，返回的都是当前时间点该条数据的真实情况，但是也有做的简单的情况，按照流水号查询，有数据，抛异常，直接返回上游错误码，但是需要注意如果用这种方式，一方面需要保证该接口幂等错误码唯一，不要其他校验失败也返回这个错误码，另一方面要暴露查询接口，让上游收到该错误码后可以继续进行查询来得到最新的状态等信息，一般建议用上面的方案，你好我好大家好，大家写代码都挺舒服对吧。 三、数据治理1.慢查询治理公司内部有慢sql相关自动报警最好，如果没有，尽量每隔一段时间主动找dba看看自己负责系统相关库表中是否有慢sql，然后在根据不同场景来决定优化业务流程或者修改增加索引。 2.数据备份与恢复核心服务数据库主从同步还是需要有的，主库出问题的时候可以很快的进行主从切换，另一方面变相的做了数据备份，如果一些非实时查询逻辑也可以走从库。 3.冷热数据分离冷热数据分离可以有两个层面的分离，一个是把非常热点的数据放到缓存中，查询的时候热点数据从缓存中查询，非热点数据查询数据库；另一个层面冷热可以指比如近一年的数据相对经常查询，放到一张业务表中，一年前的数据在另一张表中，这样在查询正常业务的时候表中数据尽可能的少，性能相对就提高了，需要查询历史数据时查询另外的表或者库。 4.历史数据归档跟冷热数据分离有些异曲同工之处，比如银行流水查询，可以在App内查询近一两年的数据，查询历史数据需要去营业厅查询，相当于业务库中只保留近一两年的数据，再往前之前的数据会做归档，放到比如ES或者hbase等大数据存储介质中 四、变更管控1.发布、资源变更每次上线前，准备本次上线checklist，包含数据库变更，数据库初始化数据，配置文件，发版先后顺序，相关数据库、redis、mq等资源申请等 2.数据变更数据库变更表结构，人工插入、修改、变更线上数据，需要经过评估。 eg：修改数据是否会对线上流程造成影响，增加索引时库中多少数据，大体需要多长时间等等 3.流量灰度线上有10台服务正在提供服务，发版的时候直接一股脑全都发版么，如果代码比如有些配置有问题，线上机器就全部完蛋了。 可以两个方面处理，一个是在启动10台服务，流量0，保证新的项目起来后，逐步10%、20%。。。100%把流量切过来，亦或是10台里面找一台先发布，并在启动后进行观察，是否有问题，在逐步启动其他机器。 4.A/B Testing使用老版本，一部分用户开始用新版本，如果用户对新版本没有什么意见，那么逐步扩大范围，把所有用户都迁移到新版本上面来 某些功能上线前后区别很大，此时可以后端两套逻辑，基于用户唯一id取模，保证相同的用户每次进来看到的都是相同的，不要每次进来一会新版，一会老版。 AB很大程度上是基于用户维度做一些事情，而流量灰度则不care这些，整体流量百分比来的，可能用户一会走到新代码里，一会老代码里 五、容量评估1.全链路压测压测指的基于当前负责的系统为入口，而全链路指的是基于页面访问的接口作为入口。 类似于618，业务会根据往年流量来预估本次的流量，技术侧也同样会基于往年每次的增量来进行预估，看是否可以包住业务的预估数据，每个接口都会梳理进4-5次大促的流量峰值数据，以次进行推算，最终决定出来本年度全链路压测量级、每个接口的压测量级。之后压测量级同步下游，最外层压测接口比如要求tps10w/s，下游各接口根据实际情况进行数值调整，没有循环调用的可以定位11-12w/s，有循环调用的下游需要进行相应的翻倍操作，作为底层服务的需要沟通各上游调用方，汇总得出系统本次需要支持的tps量级。 接下来就是各业务系统压测各自接口，都达标后，进行多轮全链路压测（军演），还会在线上进行破坏性压测，量级达到目标后继续往上走，直到下游有服务扛不住为止，对系统设计、代码编写都有非常大的考验，下游系统挂了，如何尽可能的不影响自身业务，并且等待下游恢复后如何重试这段时间内的请求等等～ 2.容量规划服务上线申请多少台服务器？多少台服务器可以应对本次大促流量峰值？ eg：比如一个项目中只有一个普通的没有使用多线程的接口，接口tp999在200ms，可以预估一秒一个线程处理5个请求，容器（如tomcat）配置200线程，那么单机可以支撑1000tps，要求1w tps，最少需要10台机器，为了一些极端情况可以先申请12-15台进行压测，看服务器各项指标情况，在达标的时候cpu60-80%为宜，这样天然的微峰值留有一些余地。 使用多线程的接口，需要对线程进行编排，调整线程池的各项参数使性能达到最优，单服务器能够支持的并发肯定是低于上面这种情况，具体能够支持多少，需要压测来进行统计。 但是现实情况往往一个项目中有若干个接口，此时流量特别小的接口可以忽略，比如tps低于5的，之后统计各接口需要各自达到什么量级的目标，之后进行混压，要求各接口都要达标的情况下看需要多少机器。 3.弹性能力建设此处是类似于devops公司发版平台需要考虑的一些问题，配合流量预测和感知，发现目前服务承载不了请求了，动态对服务进行扩容 六、风险感知1.资源监控监控服务器，容器jvm，mysql，redis等相关资源的监控信息 eg 服务器内存、网络、io，mysql读/写tps、慢sql、内存、容量、连接数、网络流量等～ 配上相关阈值报警，开发、运维能够及时知道相关资源问题 2.业务监控针对业务数据制作实时数据报表、T+1数据报表，每日关注核心业务报表（统计图、统计表），观察同环比，是否有异常，及时发现问题 3.安全监控运维团队针对网络攻击（eg：DDoS攻击）进行监控并进行防御及报警 4.拨测项目中提供类似于/health的接口，接口内无任何逻辑即可，拨测平台定时（eg：每分钟）调用后只需判定http状态码200即可基本断定服务还活着，没有hang住并且有节点在提供服务。 5.系统巡检资源类的有问题基本都会有相关报警，有些代码报错等等之类的也可以通过自定义报警实现，相对于来说比较不容易发现的是业务数据类的问题，可以通过系统巡检机制保证。 此处尽量是建立在业务监控报表已经完毕的基础上，每日安排值班同学查看业务监控报表，查看业务数据量是否处于不合理的区间或者是流量激增以及流量极具减少，及时发现并分析原因，同步业务同学。 6.钉钉业务报警代码中关键的节点，需要增加报警机制，可以使用公司内部的报警控件亦或钉钉报警，在线上出现一些问题的时候，能够自动发出报警来通知开发、运维同学来及时解决问题。 7.tps/tp999/max监控监控平台监控每一个接口的tps，tp99，tp999，max，根据tp999可以进行接口优化，max抖动可以进一进行调优，看是机器问题，抑或是gc的stw问题、网络问题、跨机房问题等等。 TP90就是满足90%的网络请求所需要的最低耗时(90%的请求耗时情况)。 TP99就是满足99%的网络请求所需要的最低耗时。TP99=10ms，标识这段时间99%的请求都在10毫秒以内。 TP999就是满足999‰的网络请求所需要的最低耗时。 MAX就是这段时间内耗时最大的，比如MAX=1000ms，表示这段时间最耗时的一次请求是1s，max高表示偶有一次请求，耗时很大。","categories":[{"name":"11.方法论","slug":"11-方法论","permalink":"https://zhangxin66666.github.io/categories/11-%E6%96%B9%E6%B3%95%E8%AE%BA/"}],"tags":[{"name":"方法论","slug":"方法论","permalink":"https://zhangxin66666.github.io/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"}]},{"title":"行页索引底层结构","slug":"7.mysql/行页索引底层结构","date":"2022-01-04T16:00:00.000Z","updated":"2024-08-22T03:23:14.709Z","comments":true,"path":"2022/01/05/7.mysql/行页索引底层结构/","link":"","permalink":"https://zhangxin66666.github.io/2022/01/05/7.mysql/%E8%A1%8C%E9%A1%B5%E7%B4%A2%E5%BC%95%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/","excerpt":"","text":"前言首先明确下什么是索引呢？假设我一张数据库的表存了10亿条数据，如果要查找出其中的10条数据，如果逐条遍历匹配的话，效率上肯定是无法容忍的。所以为了提高数据查询的效率，就需要对数据进行一些格式化的存储，来方便我们更快的查找，这就是索引。 索引其实是对数据按照某种格式进行存储的文件。就InnoDB来讲，索引文件里面会有很多的基本单元【页】，为什么要有页呢？ 如果我们在查询数据的时候直接交互磁盘，效率显然又会很慢，所以真正处理数据的过程其实是在内存中，这样就需要把磁盘的数据加载到内存，如果是写操作，可能还要将内存的数据再次刷新到磁盘。如果内存与磁盘的数据交互过程是基于一条条记录来进行的，显然又会很慢，所以InnoDB采取的方式是将数据划分为若干个页，以页来作为内存和磁盘交互的基本单位，默认大小为16KB。 换句话讲，其实就是内存一次拉到磁盘的数据最少是16KB，内存写入磁盘的数据一次最少也是16KB。 上面解释清楚了索引和页，MySQL的InnoDB存储引擎是基于页来进行内存和磁盘的数据交互的。所以我们的数据或者叫记录，其实是以【行】的格式存储在页里面的，可以简单的理解成页里面的一行对应一条记录。 当然索引文件里面肯定不光只有页，还会有其余的东西，页里面也不光只有行格式，也会有额外的信息，这个下面我们会详细分析，至此我们仅仅需要明确一下索引的概念和层级关系。 明确了这个层级关系之后，接下来我们来从最基础的行格式来进行分析。 一、行我们平时都是以记录为单位向表中插入数据的，这些记录在磁盘上的存储形式被称为行格式或者记录格式，截至目前，一共有4种行格式。分别是 compact redundant dynamic compressed，MySQL5.7默认的行格式为dynamic。 1. 如何指定行格式首先来看下，我们如何来指定行格式呢？ 123CREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称 比如我们创建一张表来指定行格式： 123456create table record_format( c1 varchar(10), c2 varchar(10) not null, c3 char(10), c4 varchar(10))charset=ascii row_format=compact; 2.compact 行格式一条完整的行格式可以被分为两个部分：记录额外信息的部分&amp;记录真实数据的部分。 2.1 额外的信息先来看额外的信息，其实包含三部分：变长字段的长度列表，NULL值列表和记录头信息。 2.1.1 变长字段长度列表MySQL支持很多的变长字段，我们就以最经典的varchar来进行举例，变长字段的数据存储多少字节其实是不固定的，所以在存储真实的数据的时候，要记录一下真实数据的字节数，这样的话，一个变长字段列实际上就占用了两部分的空间来存储：【真实数据】&amp;【真实数据占用字节数】 注意：对于一个列varchar(100)，我们实际上存储一个10字节的数据，当在内存中为这个列的数据分配内存空间的时候，实际上会分配100字节，但是这个列的数据在磁盘上，实际上只会分配10字节。 在Compact行格式中，会把所有的变长字段占用的真实长度全部逆序存储在记录的开头位置，形成一个变长字段长度列表。 比如我们刚才创建的那张表，我们来分析一下： c1,c2,c4三个列都是变长字段，所以这三个列的值的长度其实都需要保存到变长字段长度列表，因为这张表的字符集的ASCII，所以每个字符实际只占用1字节来进行编码： 列名 储存内容 内容长度(十进制表示) 内容长度(十六进制表示) C1 ‘aaaa’ 4 0x04 C2 ‘bbb’ 3 0x03 C4 ‘d’ 1 0x01 因为这些长度是按照逆序来存放的，所以最终变长字段长度列表的字节串用十六进制表示的效果就是【010304】。 因为我们演示的这条记录中，c1,c2,c4列中的字符串都比较短，所以真实的数据占用的字节数就比较小，真实数据的长度用一个字节就可以表示，但是如果变长列的内容占用字节数比较多，可能就需要用2个字节来表示。对此InnoDB的规定是： 【W】：某个字符集中表示一个字符最多需要使用的字节数 【M】：当前列类型最多能存储的字符数(比如varchar(100),M=100),如果换算成字节数就是W*M 【L】：真实占用的字节数 如果M*W&lt;=255,那么使用1字节来表示字符串实际用到的字节数。 InnoDB在读记录的变长字段长度列表的时候会先去查看表结构，判断用几个字节去存储的。 如果M*W&gt;=255,这个时候再次分为两种情况： 如果L&lt;=127，那就用1个字节表示 否则就用2个字节表示 如果某个变长字段允许存储的最大字节数大于255的时候，怎么区分他正在读取的字节是一个单独的字段长度还是半个字段长度呢？ InnoDB用该字节的第一个二进制为作为标志位，0：单独的字段长度，1：半个字段长度。 对于一些占用字节数特别多的字段，单个页都无法存储的时候，InnoDB会把一部分数据放到所谓的溢出页，在变长字段长度列表中只会记录当前页的字段长度，所以用两个字节也可以存的下。 此外，变长字段的长度列表中只存储真实数据值为非NULL的列占用的长度，真实数据为NULL的列的长度是不存储的。 也并不是所有的记录都会有变长字段长度列表，假如表中的列要是没有变长字段，或者记录中的变长字段值都是NULL，那就没有变长字段长度列表了。 2.1.2 NULL值列表如果一条记录有多个字段的真实值为NULL，不统一管理的话就会比较占用空间，所以抽取出来了NULL值列表。 当然如果这个表的所有字段都是NOT NULL约束的，就不会有NULL值列表。 看一下处理过程： 首先统计出表中允许存储NULL的字段 如果表中没有NULL字段的列，那就没必要再往下了，否则将每个允许存储NULL的列对应的一个二进制位按照列的顺序逆序排列。1：NULL，0：不是NULL。 MySQL规定NULL值必须用整数个字节的位表示，如果使用的二进制位个数不是整数个字节，则在字节的高位补0。 以此类推，如果一个表中有9个字段允许为NULL，那么这个记录的NULL值列表部分就需要2个字节来表示。 这个时候再来看我们上面创建的表中的记录。 2.1.3 记录头信息由五个固定的字节组成，换算成二进制就是40位，每一部分代表不同的信息。 名称 大小(bit) 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型 0 ：普通记录，1：B+树非页节点记录，2：最小记录，3：最大记录 next_record 16 下一条记录的相对位置 除了表中显式定义的列，MySQL会往我们的表中放一些隐藏列。 列名 是否必须 占用空间 描述 row_id 否 6字节 行ID，唯一标识一条记录 transaction_id 是 6字节 事务ID roll_pointer 是 7字节 回滚指针 【row_id】：这个玩意，跟主键的选择有关，如果我们显式定义了表的主键，就不会有它，如果我们没显式定义主键，那么会去选择一个unique的列作为主键，如果unique的列也没有，那么就会生成一个row_id列作为隐藏的主键。 【transaction_id】&amp;【roll_pointer】和一致性非锁读(MVCC)有关,后面遇到的时候我会在分析介绍。 在完善下我们开头创建的那张表的记录形象。 至此，其实就剩下我们显式插入数据库的真实记录了，但是还有一个特殊的类型需要说明一下。 2.2.1 CHAR 也是变长的？在Compact行格式下只会把变长类型的列的长度逆序记录到变长字段长度列表，但是这其实和我们的字符集有关系，上面我们创建的表显式指定为ASCII字符集，这个时候一个字符只会用一个字节表示，但是假如我们指定的是其它字符集，比如utf8，这个时候一个字符用几个字节表示就不确定了，所以CHAR列的真实字节长度也会被记录到变长字段长度列表。 另外，变长字符集的CHAR(M)类型的列要求至少占用M个字节，而VARCHAR(M)就没有这个要求。 对于使用utf8字符集的CHAR(10)的列来说，该列存储的数据字节长度的范围是10～30个字节。即使我们向该列中存储一个空字符串也会占用10个字节，这是怕将来更新该列的值的字节长度大于原有值的字节长度而小于10个字节时，可以在该记录处直接更新，而不是在存储空间中重新分配一个新的记录空间，导致原有的记录空间成为所谓的碎片。 3. 行溢出上面提到了，如果一条记录的真实字节数太大，就会导致行溢出，把超出的一部分数据存储到其他行或者页。 3.1 varchar(M)最多能存储的数据varchar(M)的列最多可以占用65535个字节。其中M代表该类型最多存储的字符数量。 实际上，MySQL对一条记录占用的最大存储空间是有限制的，除了BLOB，TEXT类型的列之外，其他所有的列(不包含隐藏列和记录头信息)占用的字节长度加起来不能超过65535个字节。这个65535个字节除了列本身的数据之外，还包括一些其他的数据，比如说我们为了存储一个varchar列，其实还需要占用3部分空间。 真实数据 真实数据占用的字节长度 NULL值标识，如果该列有NOT_NULL属性则可以没有这部分存储空间 如果该varchar类型的列没有NOT NULL属性那最多只能存储65532个字节的数据，因为真实数据的长度可能占用2个字节，NULL值标识需要占用1个字节。 如果VARCHAR类型的列有NOT NULL属性，那最多只能存储65533个字节的数据，因为真实数据的长度可能占用2个字节，不需要NULL值标识。 如果VARCHAR(M)类型的列使用的不是ascii字符集，那会怎么样呢？ 如果VARCHAR(M)类型的列使用的不是ascii字符集，那M的最大取值取决于该字符集表示一个字符最多需要的字节数。在列的值允许为NULL的情况下，gbk字符集表示一个字符最多需要2个字节，那在该字符集下，M的最大取值就是32766（也就是：65532/2），也就是说最多能存储32766个字符；utf8字符集表示一个字符最多需要3个字节，那在该字符集下，M的最大取值就是21844，就是说最多能存储21844（也就是：65532/3）个字符。 上述所言在列的值允许为NULL的情况下，gbk字符集下M的最大取值就是32766，utf8字符集下M的最大取值就是21844，这都是在表中只有一个字段的情况下说的，一定要记住一个行中的所有列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过65535个字节！ 3.2 记录中的数据太多产生溢出MySQL中磁盘和内存交互的基本单位是页，也就是说MySQL是以页为基本单位来管理存储空间的，我们的记录都会被分配到某个页中存储。而一个页的大小一般是16KB，也就是16384字节，而一个VARCHAR(M)类型的列就最多可以存储65532个字节，这样就可能造成一个页存放不了一条记录的尴尬情况。 在Compact和Redundant行格式中，对于占用存储空间非常大的列，在记录的真实数据处只会存储该列的一部分数据，把剩余的数据分散存储在几个其他的页中，然后记录的真实数据处用20个字节存储指向这些页的地址（当然这20个字节中还包括这些分散在其他页面中的数据的占用的字节数），从而可以找到剩余数据所在的页。 从图中可以看出来，对于Compact和Redundant行格式来说，如果某一列中的数据非常多的话，在本记录的真实数据处只会存储该列的前768个字节的数据和一个指向其他页的地址，然后把剩下的数据存放到其他页中，这个过程也叫做行溢出，存储超出768字节的那些页面也被称为溢出页。画一个简图就是这样： 不只是 VARCHAR(M)类型的列，其他的 TEXT、BLOB 类型的列在存储数据非常多的时候也会发生行溢出。 3.3 行溢出的临界点发生行溢出的临界点是什么呢？也就是说在列存储多少字节的数据时就会发生行溢出？ MySQL中规定一个页中至少存放两行记录，至于为什么这么规定我们之后再说，现在看一下这个规定造成的影响。我们往表中插入亮条记录，每条记录最少插入多少字节的数据才会行溢出呢？ 分析一下页空间是如何利用的 每个页除了存放我们的记录以外，也需要存储一些额外的信息，乱七八糟的额外信息加起来需要132个字节的空间（现在只要知道这个数字就好了），其他的空间都可以被用来存储记录。 每个记录需要的额外信息是27字节。这27个字节包括下边这些部分： 内容 大小(字节) 真实数据的长度 2 列是否是NULL值 1 头信息 5 row_id 6 transaction_id 6 roll_pointer 7 因为表中具体有多少列不确定，所以没法确定具体的临界点，只需要知道插入的字段数据长度很大就会导致行溢出的现象。 4.Dynamic &amp; Compressed 行格式这俩行格式和Compact行格式挺像，只不过在处理行溢出数据时有点儿分歧，它们不会在记录的真实数据处存储字段真实数据的前768个字节，而是把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址，就像这样： Compressed行格式和Dynamic不同的一点是，Compressed行格式会采用压缩算法对页面进行压缩，以节省空间。 至此，行格式就分析的差不多了，接下来我们来看页的存储结构。 二、页InnoDB为了不同的目的设计了许多种页，比如存放表空间头部信息的页，存放 Insert Buffer信息的页，存放Innode信息的页，存放undo日志信息的页等等。 本节分析存放表中记录的页，官方称为索引页，为了分析方便，我们暂且叫做数据页。 系统变量innodb_page_size表明了InnoDB存储引擎中的页大小，默认值是16384字节，也就是16kb。 该变量只能在第一次初始化MySQL数据目录时指定，之后就再也不能更改了。 数据页代表的这块16kb的存储空间被划分为多个部分，不同部分有不同的功能。 从图中可以看出，一个InnoDB数据页的存储空间大致被划分为了7个部分，有的部分占用的字节数是确定的，有的占用的字节数不是确定的。 名称 中文名 占用空间大小（字节） 简单描述 File Header 文件头部 38 页的一些通用信息 Page Header 页面头部 56 数据页专有的一些信息 Infifmum + Supremum 最小记录和最大记录 26 两个虚拟的行记录 User Records 用户记录 不确定 实际存储的行记录内容 Free Space 空闲空间 不确定 页中尚未使用的空间 Page Directory 页面目录 不确定 页中某些记录的相对位置 File Trailer 文件尾部 8 校验页是否完整 1. 记录在页中的存储我们先来创建一张表 1234567mysql&gt; create table page_demo( -&gt; c1 int , -&gt; c2 int , -&gt; c3 varchar(10000), -&gt; primary key(c1) -&gt; ) charset=ascii row_format=Compact;Query OK, 0 rows affected (0.03 sec) 因为我们指定了主键，所以存储实际数据的列里面不会有隐藏的row_id,我们来看一下他的行格式。 再次回顾下记录头中5个字节表示的数据。 名称 大小(bit) 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型 0 ：普通记录，1：B+树非页节点记录，2：最小记录，3：最大记录 next_record 16 下一条记录的相对位置 针对当前这个表的行格式简化图： 接下来我们往表中插入几条数据： 1INSERT INTO page_demo VALUES(1, 100, &#x27;aaaa&#x27;), (2, 200, &#x27;bbbb&#x27;), (3, 300, &#x27;cccc&#x27;), (4, 400, &#x27;dddd&#x27;); 为了分析这些记录在页的User Records 部分中是怎么表示的，把记录头信息和实际的列数据都用十进制表示出来了（其实是一堆二进制位），所以这些记录的示意图就是： 分析一下头信息中的每个属性是什么意思。 1.1 delete_mask标记当前记录是否被删除，占用1个二进制位，0：未删除，1：删除。 被删除的记录不会立即从磁盘上删除，因为删除他们之后吧其他的记录在磁盘上重新排列需要性能消耗，所以只是打一个删除标记，所有被删掉的数据会组成一个垃圾链表，在这个链表中的记录占用的空间成为可重用空间，之后如果有新的记录插入到表中，可能会把这些删除的记录覆盖掉。 将delete_mask 设置为1 和 将被删除的记录加入到垃圾链表中其实是两个阶段。 1.2 min_rec_maskB+树的每层非叶子节点中的最小记录都会添加该标记，如果这个字段的值是0，意味着不是B+树的非叶子节点中的最小记录。 1.3 n_owned1.4 heap_no这个属性表示当前记录在本页中的位置，我们插入的四条记录在本页中的位置分别是 2，3，4 ，5 。为什么不见 0 和 1 的记录呢？ 这是因为InnoDB自动给每个页里边加了两个记录，由于这两个记录并不是我们自己插入的，所以有时候也称为虚拟记录。这两个伪记录一个代表最小记录，一个代表最大记录。 记录是如何比较大小的？对于一条完整的记录来说，比较记录大小就是比较主键的大小。比方说我们插入的4行记录的主键值分别为1，2，3，4，这也就意味着这四条记录的大小从大到小递增。 但是不管我们往页中插入了多少自己的记录，InnoDB都规定他们定义的两条伪记录分别为最小记录和最大记录。这两条记录的构造十分简单，都是由5字节大小的记录头信息和8字节大小的一个固定的部分组成的。 由于这两条记录不是我们自己定义的记录，所以他们并不存放在页的User Records部分，他们被单独放在一个称为Infimum+Supremum的部分。 从图中我们可以看出来，最小记录和最大记录的heap_no值分别是0 和 1 ， 也就是说他们的位置最靠前。 1.5 record_type这个属性表示当前记录的类型。0：普通记录，1：B+树非叶子节点记录，2：最小记录，3：最大记录。 我们自己插入的记录是普通记录 0 ， 而最大记录和最小记录record_type 分别为 2 和 3。 1.6 next_record表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量。这其实是一条链表，可以通过一条记录找到他的下一条记录，但是下一条记录指的并不是按照我们插入顺序的下一条记录，而是按照主键值由小到大的顺序的下一条记录。而且规定 infimum记录 的下一条记录就是本页主键值最小的用户记录，而本页中主键最大的用户记录的下一条记录就是supremum记录。 如果从中删除一条记录，这个链表也是会跟着变化的，假如现在删除第二条记录 1delete from page_demo where c1 =2 ; 删除第二条记录以后： 发生的变化： 第二条记录并没有从存储空间中移除，而是把该记录的delete_mask设置为1 第二条记录的next_records值变成了0，意味着该记录没有下一条记录了 第一条记录的next record指向了第三条记录 最大记录的 n_owned 值从5 变成了4 所以，不论我们怎么对页中的记录做增删改查操作，InnoDB始终会维护一条记录的单链表，链表中各个节点是按照主键值由小到大的顺序连接起来的。 next_records 为啥要指向记录头信息和真实数据之间的位置呢？为啥不干脆指向整条记录的开头位置，也就是记录的额外信息开头的位置呢？ 因为这个位置刚刚好，向左读取就是记录头信息，向右读取就是真实数据。我们前边还说过变长字段长度列表，null值列表中的信息都是逆序存放的，这样可以使记录中位置靠前的字段和他们对应的字段长度信息在内存中的距离更近，可能会提高高速缓存的命中率。 因为主键值为2的记录已经被我们删除了，但是存储空间并没有回收，如果再次把这条记录插入到表中，会发生什么？ 1INSERT INTO page_demo VALUES(2, 200, &#x27;bbbb&#x27;); 从图中可以看到，InnoDB并没有因为新记录的插入而为他申请新的存储空间，而是直接复用了原来删除的记录的存储空间。 2. Page Directory（页目录）如果我们想根据主键值查找页中某条记录该咋办？ 1select * from page_demo where c1 = 3; 将所有正常的记录(包括两条隐藏记录但是不包括已经标记为删除的记录)划分为几组 每个组的最后一条记录（也就是组内最大的那条记录）的头信息中的n_owned属性表示该组拥有多少条记录 将每个组的最后一条记录的地址偏移量单独提取出来按照顺序存储到靠近页的尾部的地方，这个地方就是所谓的【Page Directory】,也就是页目录。页目录中的这些地址偏移量被称为槽，所以页目录就是由槽组成的 比方说刚才创建的表中正常的记录由6条，InnoDB会把他们分成两组，第一组中只有一条最小记录，第二组中是剩余的5条记录。 现在页目录部分中有两个槽，也就意味着我们的记录被分成了两个组，槽1中的值为112，代表最大记录的地址偏移量；槽0的值为99，代表最小记录的地址偏移量。 注意最大和最小记录的头信息的n_owned属性： 最小记录中的n_owned值为1，这就代表着以最小记录结尾的这个分组中只有1条记录，也就是最小记录本身 最大记录中的n_owned值为5，这就代表着以最大记录结尾的这个分组中只有5条记录，包括最大记录本身还有我们自己插入的4条记录 【99】&amp;【112】这样的地址偏移量很不直观，我们用箭头指向的方式替代数字。 InnoDB对每个分组中的记录条数是有规定的：对于最小记录所在的分组只能有 1 条记录，最大记录所在的分组拥有的记录条数只能在 1~8 条之间，剩下的分组中记录的条数范围只能在是 4~8 条之间。所以分组是按照下边的步骤进行的： 初始情况下一个数据页里只有最小记录和最大记录两条记录，它们分属于两个分组。 之后每插入一条记录，都会从页目录中找到主键值比本记录的主键值大并且差值最小的槽，然后把该槽对应的记录的n_owned值加1，表示本组内又添加了一条记录，直到该组中的记录数等于8个。 在一个组中的记录数等于8个后再插入一条记录时，会将组中的记录拆分成两个组，一个组中4条记录，另一个5条记录。这个过程会在页目录中新增一个槽来记录这个新增分组中最大的那条记录的偏移量。 由于现在page_demo表中的记录太少，无法演示添加了页目录之后加快查找速度的过程，所以再往page_demo表中添加一些记录： 1INSERT INTO page_demo VALUES(5, 500, &#x27;eeee&#x27;), (6, 600, &#x27;ffff&#x27;), (7, 700, &#x27;gggg&#x27;), (8, 800, &#x27;hhhh&#x27;), (9, 900, &#x27;iiii&#x27;), (10, 1000, &#x27;jjjj&#x27;), (11, 1100, &#x27;kkkk&#x27;), (12, 1200, &#x27;llll&#x27;), (13, 1300, &#x27;mmmm&#x27;), (14, 1400, &#x27;nnnn&#x27;), (15, 1500, &#x27;oooo&#x27;), (16, 1600, &#x27;pppp&#x27;); 现在看怎么从这个页目录中查找记录。因为各个槽代表的记录的主键值都是从小到大排序的，所以我们可以使用所谓的二分法来进行快速查找。5个槽的编号分别是：0、1、2、3、4，所以初始情况下最低的槽就是low=0，最高的槽就是high=4。比方说我们想找主键值为6的记录，过程是这样的： 计算中间槽的位置：(0+4)/2=2，所以查看槽2对应记录的主键值为8，又因为8 &gt; 6，所以设置high=2，low保持不变。 重新计算中间槽的位置：(0+2)/2=1，所以查看槽1对应的主键值为4，又因为4 &lt; 6，所以设置low=1，high保持不变。 因为high - low的值为1，所以确定主键值为6的记录在槽2对应的组中。此刻我们需要找到槽2中主键值最小的那条记录，然后沿着单向链表遍历槽2中的记录。但是我们前边又说过，每个槽对应的记录都是该组中主键值最大的记录，这里槽2对应的记录是主键值为8的记录，怎么定位一个组中最小的记录呢？别忘了各个槽都是挨着的，我们可以很轻易的拿到槽1对应的记录（主键值为4），该条记录的下一条记录就是槽2中主键值最小的记录，该记录的主键值为5。所以我们可以从这条主键值为5的记录出发，遍历槽2中的各条记录，直到找到主键值为6的那条记录即可。由于一个组中包含的记录条数只能是1~8条，所以遍历一个组中的记录的代价是很小的。 所以在一个数据页中查找指定主键值的记录的过程分为两步： 通过二分法确定该记录所在的槽，并找到该槽所在分组中主键值最小的那条记录。 通过记录的next_record属性遍历该槽所在的组中的各个记录。 3.Page Header（页面头部）为了能得到一个数据页中存储的记录的状态信息，比如本页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等等，特意在页中定义了一个叫Page Header的部分，它是页结构的第二部分，这个部分占用固定的56个字节，专门存储各种状态信息。 名称 占用空间大小（字节） 描述 PAGE_N_DIR_SLOTS 2 在页目录中的槽数量 PAGE_HEAP_TOP 2 还未使用的空间最小地址，也就是说从该地址之后就是Free Space PAGE_N_HEAP 2 本页中的记录的数量（包括最小和最大记录以及标记为删除的记录） PAGE_FREE 2 第一个已经标记为删除的记录地址（各个已删除的记录通过next_record也会组成一个单链表，这个单链表中的记录可以被重新利用） PAGE_GARBAGE 2 已删除记录占用的字节数 PAGE_LAST_INSERT 2 最后插入记录的位置 PAGE_DIRECTION 2 记录插入的方向 PAGE_N_DIRECTION 2 一个方向连续插入的记录数量 PAGE_N_RECS 2 该页中记录的数量（不包括最小和最大记录以及被标记为删除的记录） PAGE_MAX_TRX_ID 8 修改当前页的最大事务ID，该值仅在二级索引中定义 PAGE_LEVEL 2 当前页在B+树中所处的层级 PAGE_INDEX_ID 8 索引ID，表示当前页属于哪个索引 PAGE_BTR_SEG_LEAF 10 B+树叶子段的头部信息，仅在B+树的Root页定义 PAGE_BTR_SEG_TOP 10 B+树非叶子段的头部信息，仅在B+树的Root页定义 4.File Header（文件头部）File Header针对各种类型的页都通用，也就是说不同类型的页都会以File Header作为第一个组成部分，它描述了一些针对各种页都通用的一些信息，比方说这个页的编号是多少，它的上一个页、下一个页是谁，这个部分占用固定的38个字节。 名称 占用空间大小（字节） 描述 FIL_PAGE_SPACE_OR_CHKSUM 4 页的校验和（checksum值） FIL_PAGE_OFFSET 4 页号 FIL_PAGE_PREV 4 上一个页的页号 FIL_PAGE_NEXT 4 下一个页的页号 FIL_PAGE_LSN 8 页面被最后修改时对应的日志序列位置（英文名是：Log Sequence Number） FIL_PAGE_TYPE 2 该页的类型 FIL_PAGE_FILE_FLUSH_LSN 8 仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值 FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4 页属于哪个表空间 InnoDB为了不同的目的而把页分为不同的类型，我们上边介绍的其实都是存储记录的数据页，其实还有很多别的类型的页，具体如下表： 类型名称 十六进制 描述 FIL_PAGE_TYPE_ALLOCATED 0x0000 最新分配，还没使用 FIL_PAGE_UNDO_LOG 0x0002 Undo日志页 FIL_PAGE_INODE 0x0003 段信息节点 FIL_PAGE_IBUF_FREE_LIST 0x0004 Insert Buffer空闲列表 FIL_PAGE_IBUF_BITMAP 0x0005 Insert Buffer位图 FIL_PAGE_TYPE_SYS 0x0006 系统页 FIL_PAGE_TYPE_TRX_SYS 0x0007 事务系统数据 FIL_PAGE_TYPE_FSP_HDR 0x0008 表空间头部信息 FIL_PAGE_TYPE_XDES 0x0009 扩展描述页 FIL_PAGE_TYPE_BLOB 0x000A 溢出页 FIL_PAGE_INDEX 0x45BF 索引页，也就是我们所说的数据页 我们存放记录的数据页的类型其实是FIL_PAGE_INDEX，也就是所谓的索引页。 有时候我们存放某种类型的数据占用的空间非常大（比方说一张表中可以有成千上万条记录），InnoDB可能不可以一次性为这么多数据分配一个非常大的存储空间，如果分散到多个不连续的页中存储的话需要把这些页关联起来，FIL_PAGE_PREV和FIL_PAGE_NEXT就分别代表本页的上一个和下一个页的页号。这样通过建立一个双向链表把许许多多的页就都串联起来了，而无需这些页在物理上真正连着。需要注意的是，并不是所有类型的页都有上一个和下一个页的属性，不过我们现在分析的数据页（也就是类型为FIL_PAGE_INDEX的页）是有这两个属性的，所以所有的数据页其实是一个双链表。 5.File Trailer(文件尾部)如果页中的数据在内存中被修改了，那么在修改后的某个时间需要把数据同步到磁盘中。但是在同步了一半的时候中断电了咋办？ 为了检测一个页是否完整，在每个页的尾部都加了一个File Trailer部分，这个部分由8个字节组成，可以分成2个小部分： 前四个字节代表校验和 后四个字节代表页面被最后修改时对应的日志序列位置 这个File Trailer &amp; File Header 类似，都是所有类型的页通用的。 至此，整个数据页的结构我们也基本上分析完了，现在在回头看一下开头我们那张恐怖的图，是不是感觉清晰很多了呢？接下来，我们来分析索引的结构。 三、索引1.索引结构1.1 聚簇索引上边介绍的B+树本身就是一个目录，或者说本身就是一个索引。它有两个特点： 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表。 B+树的叶子节点存储的是完整的用户记录。 所谓完整的用户记录，就是指这个记录中存储了所有列的值（包括隐藏列）。 我们把具有这两种特性的B+树称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这种聚簇索引并不需要我们在MySQL语句中显式的使用INDEX语句去创建，InnoDB存储引擎会自动的为我们创建聚簇索引。另外，在InnoDB存储引擎中，聚簇索引就是数据的存储方式（所有的用户记录都存储在了叶子节点），也就是所谓的索引即数据，数据即索引。 1.2 二级索引聚簇索引只能在搜索条件是主键值时才能发挥作用，因为B+树中的数据都是按照主键进行排序的。那如果我们想以别的列作为搜索条件怎么办？ 我们可以多建几棵B+树，不同的B+树中的数据采用不同的排序规则。比方说我们用c2列的大小作为数据页、页中记录的排序规则，再建一棵B+树，效果如下图所示： 这个B+树与上边介绍的聚簇索引有几处不同： 使用记录c2列的大小进行记录和页的排序，这包括三个方面的含义： a页内的记录是按照c2列的大小顺序排成一个单向链表。 b各个存放用户记录的页也是根据页中记录的c2列大小顺序排成一个双向链表。 c存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的c2列大小顺序排成一个双向链表。 B+树的叶子节点存储的并不是完整的用户记录，而只是c2列+主键这两个列的值。 目录项记录中不再是主键+页号的搭配，而变成了c2列+页号的搭配。 所以如果我们现在想通过c2列的值查找某些记录的话就可以使用我们刚刚建好的这个B+树了。以查找c2列的值为4的记录为例，查找过程如下： 1确定目录项记录页 根据根页面，也就是页44，可以快速定位到目录项记录所在的页为页42（因为2 &lt; 4 &lt; 9）。 2通过目录项记录页确定用户记录真实所在的页。 在页42中可以快速定位到实际存储用户记录的页，但是由于c2列并没有唯一性约束，所以c2列值为4的记录可能分布在多个数据页中，又因为2 &lt; 4 ≤ 4，所以确定实际存储用户记录的页在页34和页35中。 3在真实存储用户记录的页中定位到具体的记录. 到页34和页35中定位到具体的记录。 4但是这个B+树的叶子节点中的记录只存储了c2和c1（也就是主键）两个列，所以我们必须再根据主键值去聚簇索引中再查找一遍完整的用户记录。 我们根据这个以c2列大小排序的B+树只能确定我们要查找记录的主键值，所以如果我们想根据c2列的值查找到完整的用户记录的话，仍然需要到聚簇索引中再查一遍，这个过程也被称为回表。也就是根据c2列的值查询一条完整的用户记录需要使用到2棵B+树！！！ 为什么我们还需要一次回表操作呢？直接把完整的用户记录放到叶子节点不就好了么？ 如果把完整的用户记录放到叶子节点是可以不用回表，相当于每建立一棵B+树都需要把所有的用户记录再都拷贝一遍，这就有点太浪费存储空间了。因为这种按照非主键列建立的B+树需要一次回表操作才可以定位到完整的用户记录，所以这种B+树也被称为二级索引（英文名secondary index），或者辅助索引。由于我们使用的是c2列的大小作为B+树的排序规则，所以我们也称这个B+树为为c2列建立的索引。 假设我们的查询结果是十条，那就是要进行10次回表，那这样的话，效率不是又慢了？ 在MySQL5.6对这种情况进行了优化，如果发现查询结果会导致多次回表，那么就会进行IO合并，拿到所有的主键再去进行回表。 1.3 联合索引我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+树按照c2和c3列的大小进行排序，这个包含两层含义： ●先把各个记录和页按照c2列进行排序。 ●在记录的c2列相同的情况下，采用c3列进行排序 为c2和c3列建立的索引的示意图如下： 四、总结：一次通过聚簇索引定位数据的过程大家熟悉的三个版本索引结构图如下 普通索引 二级索引 组合索引索引 相对完整些的三种索引结构见（三、索引） 下图为聚簇索引完整版结构 一次按照主键id查询id=5的数据的全流程 1.确定目录项记录页，找到页50 2.二分查找确定所属槽位1，然后遍历槽位1里的链表得知下次该继续查询页55 3.二分查找确定所属槽位1，然后遍历槽位1里的链表得知下次该继续查询页61 4.二分查找确定所属槽位2，然后遍历槽位2里的链表得到id为4的数据","categories":[{"name":"7.mysql","slug":"7-mysql","permalink":"https://zhangxin66666.github.io/categories/7-mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"}]},{"title":"mysql三大日志","slug":"7.mysql/mysql三大日志","date":"2021-12-26T16:00:00.000Z","updated":"2024-08-22T03:23:14.706Z","comments":true,"path":"2021/12/27/7.mysql/mysql三大日志/","link":"","permalink":"https://zhangxin66666.github.io/2021/12/27/7.mysql/mysql%E4%B8%89%E5%A4%A7%E6%97%A5%E5%BF%97/","excerpt":"","text":"日志是 mysql 数据库的重要组成部分，记录着数据库运行期间各种状态信息。mysql日志主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。 作为开发，我们重点需要关注的是二进制日志( binlog )和事务日志(包括redo log 和 undo log )，本文接下来会详细介绍这三种日志。 binlogbinlog 用于记录数据库执行的写入性操作(不包括查询)信息，以二进制的形式保存在磁盘中。binlog 是 mysql的逻辑日志，并且由 Server 层进行记录，使用任何存储引擎的 mysql 数据库都会记录 binlog 日志。 逻辑日志：可以简单理解为记录的就是sql语句 。 物理日志：mysql 数据最终是保存在数据页中的，物理日志记录的就是数据页变更 。 binlog 是通过追加的方式进行写入的，可以通过max_binlog_size 参数设置每个 binlog文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志。 binlog使用场景在实际应用中， binlog 的主要使用场景有两个，分别是 主从复制 和 数据恢复 。 主从复制 ：在 Master 端开启 binlog ，然后将 binlog发送到各个 Slave 端， Slave 端重放 binlog 从而达到主从数据一致。 数据恢复 ：通过使用 mysqlbinlog 工具来恢复数据。 binlog刷盘时机对于 InnoDB 存储引擎而言，只有在事务提交时才会记录biglog ，此时记录还在内存中，那么 biglog是什么时候刷到磁盘中的呢？ mysql 通过 sync_binlog 参数控制 biglog 的刷盘时机，取值范围是 0-N： 0：不去强制要求，由系统自行判断何时写入磁盘； 1：每次 commit 的时候都要将 binlog 写入磁盘； N：每N个事务，才会将 binlog 写入磁盘。 从上面可以看出， sync_binlog 最安全的是设置是 1 ，这也是MySQL 5.7.7之后版本的默认值。但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。 binlog日志格式binlog 日志有三种格式，分别为 STATMENT 、 ROW 和 MIXED。 在 MySQL 5.7.7 之前，默认的格式是 STATEMENT ， MySQL 5.7.7 之后，默认值是 ROW。日志格式通过 binlog-format 指定。 STATMENT：基于SQL 语句的复制( statement-based replication, SBR )，每一条会修改数据的sql语句会记录到binlog 中 。 优点：不需要记录每一行的变化，减少了 binlog 日志量，节约了 IO , 从而提高了性能； 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate() 、 slepp() 等 。 ROW：基于行的复制(row-based replication, RBR )，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了 。 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题 ； 缺点：会产生大量的日志，尤其是alter table 的时候会让日志暴涨 MIXED：基于STATMENT 和 ROW 两种模式的混合复制(mixed-based replication, MBR )，一般的复制使用STATEMENT 模式保存 binlog ，对于 STATEMENT 模式无法复制的操作使用 ROW 模式保存 binlog redo log为什么需要redo log我们都知道，事务的四大特性里面有一个是 持久性 ，具体来说就是只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态 。 那么 mysql是如何保证一致性的呢？ 最简单的做法是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。但是这么做会有严重的性能问题，主要体现在两个方面： 因为 Innodb 是以 页 为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！ 一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差！ 因此 mysql 设计了 redo log ， 具体来说就是只记录事务对数据页做了哪些****修改，这样就能完美地解决性能问题了(相对而言文件更小并且是顺序IO)。 redo log基本概念redo log 包括两部分：一个是内存中的日志缓冲( redo log buffer )，另一个是磁盘上的日志文件( redo logfile)。 mysql 每执行一条 DML 语句，先将记录写入 redo log buffer，后续某个时间点再一次性将多个操作记录写到 redo log file。这种 先写日志，再写磁盘 的技术就是 MySQL里经常说到的 WAL(Write-Ahead Logging) 技术。 在计算机操作系统中，用户空间( user space )下的缓冲区数据一般情况下是无法直接写入磁盘的，中间必须经过操作系统内核空间( kernel space )缓冲区( OS Buffer )。 因此， redo log buffer 写入 redo logfile 实际上是先写入 OS Buffer ，然后再通过系统调用 fsync() 将其刷到 redo log file中，过程如下： mysql 支持三种将 redo log buffer 写入 redo log file 的时机，可以通过 innodb_flush_log_at_trx_commit 参数配置，各参数值含义如下： redo log记录形式前面说过， redo log 实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此 redo log实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。 如下图： 同时我们很容易得知， 在innodb中，既有redo log 需要刷盘，还有 数据页 也需要刷盘， redo log存在的意义主要就是降低对 数据页 刷盘的要求。 在上图中， write pos 表示 redo log 当前记录的 LSN (逻辑序列号)位置， check point 表示 数据页更改记录 刷盘后对应 redo log 所处的 LSN(逻辑序列号)位置。 write pos 到 check point 之间的部分是 redo log 空着的部分，用于记录新的记录；check point 到 write pos 之间是 redo log 待落盘的数据页更改记录。当 write pos追上check point 时，会先推动 check point 向前移动，空出位置再记录新的日志。 启动 innodb 的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。因为 redo log记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志(如 binlog )要快很多。 重启innodb 时，首先会检查磁盘中数据页的 LSN ，如果数据页的LSN 小于日志中的 LSN ，则会从 checkpoint 开始恢复。 还有一种情况，在宕机前正处于checkpoint 的刷盘过程，且数据页的刷盘进度超过了日志页的刷盘进度，此时会出现数据页中记录的 LSN 大于日志中的 LSN，这时超出日志进度的部分将不会重做，因为这本身就表示已经做过的事情，无需再重做。 redo log与binlog区别 由 binlog 和 redo log 的区别可知：binlog 日志只用于归档，只依靠 binlog 是没有 crash-safe 能力的。 但只有 redo log 也不行，因为 redo log 是 InnoDB特有的，且日志上的记录落盘后会被覆盖掉。因此需要 binlog和 redo log二者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失。 undo log数据库事务四大特性中有一个是 原子性 ，具体来说就是 原子性是指对数据库的一系列操作，要么全部成功，要么全部失败，不可能出现部分成功的情况。 实际上， 原子性 底层就是通过 undo log 实现的。undo log主要记录了数据的逻辑变化，比如一条 INSERT 语句，对应一条DELETE 的 undo log ，对于每个 UPDATE 语句，对应一条相反的 UPDATE 的 undo log ，这样在发生错误时，就能回滚到事务之前的数据状态。 同时， undo log 也是 MVCC(多版本并发控制)实现的关键。 Undo log是InnoDB MVCC事务特性的重要组成部分。当我们对记录做了变更操作时就会产生Undo记录，Undo记录默认被记录到系统表空间(ibdata)中，但从5.6开始，也可以使用独立的Undo 表空间。 Undo记录中存储的是老版本数据，当一个旧的事务需要读取数据时，为了能读取到老版本的数据，需要顺着undo链找到满足其可见性的记录。当版本链很长时，通常可以认为这是个比较耗时的操作。 Undo Log 格式 在InnoDB引擎中，undo log分为： insert undo log： insert undo log是指在insert操作中产生的undo log，因为insert操作的记录，只对事务本身可见，对其他事务不可见（这是事务隔离性的要求），故该undo log可以在事务提交后直接删除，不需要进行purge操作。 update undo log： update undo log记录的是delete和update操作产生的undo log。该undo log可能需要提供MVCC机制，因此不能在事务提交时就进行删除，提交时放入undo log链表，等待purge线程进行最后的删除。下面是两种undo log的结构图。","categories":[{"name":"7.mysql","slug":"7-mysql","permalink":"https://zhangxin66666.github.io/categories/7-mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"}]},{"title":"undolog","slug":"7.mysql/undo日志","date":"2021-12-26T16:00:00.000Z","updated":"2024-08-22T03:23:14.708Z","comments":true,"path":"2021/12/27/7.mysql/undo日志/","link":"","permalink":"https://zhangxin66666.github.io/2021/12/27/7.mysql/undo%E6%97%A5%E5%BF%97/","excerpt":"","text":"1.事务回滚的需求我们说过事务需要保证原子性，也就是事务中的操作要么全部完成，要么什么也不做。但是偏偏有时候事务执行到一半会出现一些情况，比如： 情况一：事务执行过程中可能遇到各种错误，比如服务器本身的错误，操作系统错误，甚至是突然断电导致的错误。 情况二：程序员可以在事务执行过程中手动输入ROLLBACK语句结束当前的事务的执行。 这两种情况都会导致事务执行到一半就结束，但是事务执行过程中可能已经修改了很多东西，为了保证事务的原子性，我们需要把东西改回原先的样子，这个过程就称之为回滚（英文名：rollback），这样就可以造成一个假象：这个事务看起来什么都没做，所以符合原子性要求。 每当我们要对一条记录做改动时（这里的改动可以指INSERT、DELETE、UPDATE），都需要留一手 —— 把回滚时所需的东西都给记下来。比方说： 你插入一条记录时，至少要把这条记录的主键值记下来，之后回滚的时候只需要把这个主键值对应的记录删掉就好了。 你删除了一条记录，至少要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了。 你修改了一条记录，至少要把修改这条记录前的旧值都记录下来，这样之后回滚时再把这条记录更新为旧值就好了。 数据库把这些为了回滚而记录的这些东西称之为撤销日志，英文名为undo log，我们也可以土洋结合，称之为undo日志。这里需要注意的一点是，由于查询操作（SELECT）并不会修改任何用户记录，所以在查询操作执行时，并不需要记录相应的undo日志。在真实的InnoDB中，undo日志其实并不像我们上边所说的那么简单，不同类型的操作产生的undo日志的格式也是不同的，不过先暂时把这些具体细节放一放，我们先回过头来看看事务id。 2.事务id2.1给事务分配id的时机一个事务可以是一个只读事务，或者是一个读写事务： 我们可以通过START TRANSACTION READ ONLY语句开启一个只读事务。在只读事务中不可以对普通的表（其他事务也能访问到的表）进行增、删、改操作，但可以对临时表做增、删、改操作。 我们可以通过START TRANSACTION READ WRITE语句开启一个读写事务，或者使用BEGIN、START TRANSACTION语句开启的事务默认也算是读写事务。在读写事务中可以对表执行增删改查操作。 如果某个事务执行过程中对某个表执行了增、删、改操作，那么InnoDB存储引擎就会给它分配一个独一无二的事务id，分配方式如下： 对于只读事务来说，只有在它第一次对某个用户创建的临时表执行增、删、改操作时才会为这个事务分配一个事务id，否则的话是不分配事务id的。 对某个查询语句执行EXPLAIN分析它的查询计划时，有时候在Extra列会看到Using temporary的提示，这个表明在执行该查询语句时会用到内部临时表。这个所谓的内部临时表和我们手动用CREATE TEMPORARY TABLE创建的用户临时表并不一样，在事务回滚时并不需要把执行SELECT语句过程中用到的内部临时表也回滚，在执行SELECT语句用到内部临时表时并不会为它分配事务id。 对于读写事务来说，只有在它第一次对某个表（包括用户创建的临时表）执行增、删、改操作时才会为这个事务分配一个事务id，否则的话也是不分配事务id的。有的时候虽然我们开启了一个读写事务，但是在这个事务中全是查询语句，并没有执行增、删、改的语句，那也就意味着这个事务并不会被分配一个事务id。 只有在事务对表中的记录做改动时才会为这个事务分配一个唯一的**事务id**。 上边描述的事务id分配策略是针对MySQL 5.7来说的，前边的版本的分配方式可能不同。 2.2事务id是怎么生成的这个事务id本质上就是一个数字，它的分配策略和对隐藏列row_id（当用户没有为表创建主键和UNIQUE键时InnoDB自动创建的列）的分配策略大抵相同，具体策略如下： 服务器会在内存中维护一个全局变量，每当需要为某个事务分配一个事务id时，就会把该变量的值当作事务id分配给该事务，并且把该变量自增1。 每当这个变量的值为256的倍数时，就会将该变量的值刷新到系统表空间的页号为5的页面中一个称之为Max Trx ID的属性处，这个属性占用8个字节的存储空间。 当系统下一次重新启动时，会将上边提到的Max Trx ID属性加载到内存中，将该值加上256之后赋值给我们前边提到的全局变量（因为在上次关机时该全局变量的值可能大于Max Trx ID属性值）。 这样就可以保证整个系统中分配的事务id值是一个递增的数字。先被分配id的事务得到的是较小的事务id，后被分配id的事务得到的是较大的事务id。 2.3trx_id隐藏列聚簇索引的记录除了会保存完整的用户数据以外，而且还会自动添加名为trx_id、roll_pointer的隐藏列，如果用户没有在表中定义主键以及UNIQUE键，还会自动添加一个名为row_id的隐藏列。所以一条记录在页面中的真实结构看起来就是这样的： 其中的trx_id列其实还蛮好理解的，就是某个对这个聚簇索引记录做改动的语句所在的事务对应的事务id而已（此处的改动可以是INSERT、DELETE、UPDATE操作）。至于roll_pointer隐藏列我们后边分析。 3.undo日志的格式为了实现事务的原子性，InnoDB存储引擎在实际进行增、删、改一条记录时，都需要先把对应的undo日志记下来。一般每对一条记录做一次改动，就对应着一条undo日志，但在某些更新记录的操作中，也可能会对应着2条undo日志。一个事务在执行过程中可能新增、删除、更新若干条记录，也就是说需要记录很多条对应的undo日志，这些undo日志会被从0开始编号，也就是说根据生成的顺序分别被称为第0号undo日志、第1号undo日志、…、第n号undo日志等，这个编号也被称之为undo no。 这些undo日志是被记录到类型为FIL_PAGE_UNDO_LOG的页面中。这些页面可以从系统表空间中分配，也可以从一种专门存放undo日志的表空间，也就是所谓的undo tablespace中分配。先来看看不同操作都会产生什么样子的undo日志吧～我们先来创建一个名为undo_demo的表： 1234567CREATE TABLE undo_demo ( id INT NOT NULL, key1 VARCHAR(100), col VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1))Engine=InnoDB CHARSET=utf8; 这个表中有3个列，其中id列是主键，我们为key1列建立了一个二级索引，col列是一个普通的列。每个表都会被分配一个唯一的table id，我们可以通过系统数据库information_schema中的innodb_sys_tables表来查看某个表对应的table id是什么，现在我们查看一下undo_demo对应的table id是多少： 1234567mysql&gt; SELECT * FROM information_schema.innodb_sys_tables WHERE name = &#x27;yhd/undo_demo&#x27;;+----------+---------------------+------+--------+-------+-------------+------------+---------------+------------+| TABLE_ID | NAME | FLAG | N_COLS | SPACE | FILE_FORMAT | ROW_FORMAT | ZIP_PAGE_SIZE | SPACE_TYPE |+----------+---------------------+------+--------+-------+-------------+------------+---------------+------------+| 138 | yhd/undo_demo | 33 | 6 | 482 | Barracuda | Dynamic | 0 | Single |+----------+---------------------+------+--------+-------+-------------+------------+---------------+------------+1 row in set (0.01 sec) 从查询结果可以看出，undo_demo表对应的table id为138。 3.1INSERT操作对应的undo日志当我们向表中插入一条记录时会有乐观插入和悲观插入的区分，但是不管怎么插入，最终导致的结果就是这条记录被放到了一个数据页中。如果希望回滚这个插入操作，那么把这条记录删除就好了，也就是说在写对应的undo日志时，主要是把这条记录的主键信息记上。所以InnoDB设计了一个类型为TRX_UNDO_INSERT_REC的undo日志，它的完整结构如下图所示： 根据示意图我们强调几点： undo no在一个事务中是从0开始递增的，也就是说只要事务没提交，每生成一条undo日志，那么该条日志的undo no就增1。 如果记录中的主键只包含一个列，那么在类型为TRX_UNDO_INSERT_REC的undo日志中只需要把该列占用的存储空间大小和真实值记录下来，如果记录中的主键包含多个列，那么每个列占用的存储空间大小和对应的真实值都需要记录下来（图中的len就代表列占用的存储空间大小，value就代表列的真实值）。 当我们向某个表中插入一条记录时，实际上需要向聚簇索引和所有的二级索引都插入一条记录。不过记录undo日志时，我们只需要考虑向聚簇索引插入记录时的情况就好了，因为其实聚簇索引记录和二级索引记录是一一对应的，我们在回滚插入操作时，只需要知道这条记录的主键信息，然后根据主键信息做对应的删除操作，做删除操作时就会顺带着把所有二级索引中相应的记录也删除掉。后边说到的DELETE操作和UPDATE操作对应的undo日志也都是针对聚簇索引记录而言的。 现在我们向undo_demo中插入两条记录： 12345BEGIN; # 显式开启一个事务，假设该事务的id为100# 插入两条记录INSERT INTO undo_demo(id, key1, col) VALUES (1, &#x27;AWM&#x27;, &#x27;狙击枪&#x27;), (2, &#x27;M416&#x27;, &#x27;步枪&#x27;); 因为记录的主键只包含一个id列，所以我们在对应的undo日志中只需要将待插入记录的id列占用的存储空间长度（id列的类型为INT，INT类型占用的存储空间长度为4个字节）和真实值记录下来。本例中插入了两条记录，所以会产生两条类型为TRX_UNDO_INSERT_REC的undo日志: 第一条undo日志的undo no为0，记录主键占用的存储空间长度为4，真实值为1。画一个示意图就是这样： 第二条undo日志的undo no为1，记录主键占用的存储空间长度为4，真实值为2。画一个示意图就是这样（与第一条undo日志对比，undo no和主键各列信息有不同）： 为了最大限度的节省undo日志占用的存储空间，和我们前边说过的redo日志类似，InnoDB会给undo日志中的某些属性进行压缩处理。 ①roll_pointer隐藏列的含义roll_pointer本质上就是一个指向记录对应的undo日志的一个指针。比方说我们上边向undo_demo表里插入了2条记录，每条记录都有与其对应的一条undo日志。记录被存储到了类型为FIL_PAGE_INDEX的页面中（就是我们前边一直所说的数据页），undo日志被存放到了类型为FIL_PAGE_UNDO_LOG的页面中。效果如图所示： **roll_pointer**本质就是一个指针，指向记录对应的undo日志。 3.2 DELETE操作对应的undo日志插入到页面中的记录会根据记录头信息中的next_record属性组成一个单向链表，我们把这个链表称之为正常记录链表；被删除的记录其实也会根据记录头信息中的next_record属性组成一个链表，只不过这个链表中的记录占用的存储空间可以被重新利用，所以也称这个链表为垃圾链表。Page Header部分有一个称之为PAGE_FREE的属性，它指向由被删除记录组成的垃圾链表中的头节点。我们先画一个图，假设此刻某个页面中的记录分布情况是这样的（这个不是undo_demo表中的记录，只是我们随便举的一个例子）： 为了突出主题，在这个简化版的示意图中，我们只把记录的delete_mask标志位展示了出来。从图中可以看出，正常记录链表中包含了3条正常记录，垃圾链表里包含了2条已删除记录，在垃圾链表中的这些记录占用的存储空间可以被重新利用。页面的Page Header部分的PAGE_FREE属性的值代表指向垃圾链表头节点的指针。假设现在我们准备使用DELETE语句把正常记录链表中的最后一条记录给删除掉，其实这个删除的过程需要经历两个阶段： 阶段一：仅仅将记录的delete_mask标识位设置为1，其他的不做修改（其实会修改记录的trx_id、roll_pointer这些隐藏列的值）。设计InnoDB的大叔把这个阶段称之为delete mark。把这个过程画下来就是这样：可以看到，正常记录链表中的最后一条记录的delete_mask值被设置为1，但是并没有被加入到垃圾链表。也就是此时记录处于一个中间状态。在删除语句所在的事务提交之前，被删除的记录一直都处于这种所谓的中间状态。 为啥会有这种奇怪的中间状态呢？其实主要是为了实现一个称之为MVCC的功能。 阶段二：当该删除语句所在的事务提交之后，会有专门的线程后来真正的把记录删除掉。所谓真正的删除就是把该记录从正常记录链表中移除，并且加入到垃圾链表中，然后还要调整一些页面的其他信息，比如页面中的用户记录数量PAGE_N_RECS、上次插入记录的位置PAGE_LAST_INSERT、垃圾链表头节点的指针PAGE_FREE、页面中可重用的字节数量PAGE_GARBAGE、还有页目录的一些信息等等。InnoDB把这个阶段称之为purge。把阶段二执行完了，这条记录就算是真正的被删除掉了。这条已删除记录占用的存储空间也可以被重新利用了。画下来就是这样：将被删除记录加入到垃圾链表时，实际上加入到链表的头节点处，会跟着修改PAGE_FREE属性的值。 页面的Page Header部分有一个PAGE_GARBAGE属性，该属性记录着当前页面中可重用存储空间占用的总字节数。每当有已删除记录被加入到垃圾链表后，都会把这个PAGE_GARBAGE属性的值加上该已删除记录占用的存储空间大小。PAGE_FREE指向垃圾链表的头节点，之后每当新插入记录时，首先判断PAGE_FREE指向的头节点代表的已删除记录占用的存储空间是否足够容纳这条新插入的记录，如果不可以容纳，就直接向页面中申请新的空间来存储这条记录（并不会尝试遍历整个垃圾链表，找到一个可以容纳新记录的节点）。如果可以容纳，那么直接重用这条已删除记录的存储空间，并且把PAGE_FREE指向垃圾链表中的下一条已删除记录。但是这里有一个问题，如果新插入的那条记录占用的存储空间大小小于垃圾链表的头节点占用的存储空间大小，那就意味头节点对应的记录占用的存储空间里有一部分空间用不到，这部分空间就被称之为碎片空间。那这些碎片空间岂不是永远都用不到了么？其实也不是，这些碎片空间占用的存储空间大小会被统计到PAGE_GARBAGE属性中，这些碎片空间在整个页面快使用完前并不会被重新利用，不过当页面快满时，如果再插入一条记录，此时页面中并不能分配一条完整记录的空间，这时候会首先看一看PAGE_GARBAGE的空间和剩余可利用的空间加起来是不是可以容纳下这条记录，如果可以的话，InnoDB会尝试重新组织页内的记录，重新组织的过程就是先开辟一个临时页面，把页面内的记录依次插入一遍，因为依次插入时并不会产生碎片，之后再把临时页面的内容复制到本页面，这样就可以把那些碎片空间都解放出来（很显然重新组织页面内的记录比较耗费性能）。 从上边的描述中我们也可以看出来，在删除语句所在的事务提交之前，只会经历阶段一，也就是delete mark阶段（提交之后我们就不用回滚了，所以只需考虑对删除操作的阶段一做的影响进行回滚）。InnoDB为此设计了一种称之为TRX_UNDO_DEL_MARK_REC类型的undo日志，它的完整结构如下图所示： 在对一条记录进行delete mark操作前，需要把该记录的旧的trx_id和roll_pointer隐藏列的值都给记到对应的undo日志中来，就是我们图中显示的old trx_id和old roll_pointer属性。这样有一个好处，那就是可以通过undo日志的old roll_pointer找到记录在修改之前对应的undo日志。比方说在一个事务中，我们先插入了一条记录，然后又执行对该记录的删除操作，这个过程的示意图就是这样：从图中可以看出来，执行完delete mark操作后，它对应的undo日志和INSERT操作对应的undo日志就串成了一个链表。这个链表就称之为版本链。 与类型为TRX_UNDO_INSERT_REC的undo日志不同，类型为TRX_UNDO_DEL_MARK_REC的undo日志还多了一个索引列各列信息的内容，也就是说如果某个列被包含在某个索引中，那么它的相关信息就应该被记录到这个索引列各列信息部分，所谓的相关信息包括该列在记录中的位置（用pos表示），该列占用的存储空间大小（用len表示），该列实际值（用value表示）。所以索引列各列信息存储的内容实质上就是&lt;pos, len, value&gt;的一个列表。这部分信息主要是用在事务提交后，对该中间状态记录做真正删除的阶段二，也就是purge阶段中使用的。 现在继续在上边那个事务id为100的事务中删除一条记录，比如我们把id为1的那条记录删除掉： 12345678BEGIN; # 显式开启一个事务，假设该事务的id为100# 插入两条记录INSERT INTO undo_demo(id, key1, col) VALUES (1, &#x27;AWM&#x27;, &#x27;狙击枪&#x27;), (2, &#x27;M416&#x27;, &#x27;步枪&#x27;); # 删除一条记录 DELETE FROM undo_demo WHERE id = 1; 这个delete mark操作对应的undo日志的结构就是这样： 对照着这个图，我们得注意下边几点： 因为这条undo日志是id为100的事务中产生的第3条undo日志，所以它对应的undo no就是2。 在对记录做delete mark操作时，记录的trx_id隐藏列的值是100（也就是说对该记录最近的一次修改就发生在本事务中），所以把100填入old trx_id属性中。然后把记录的roll_pointer隐藏列的值取出来，填入old roll_pointer属性中，这样就可以通过old roll_pointer属性值找到最近一次对该记录做改动时产生的undo日志。 由于undo_demo表中有2个索引：一个是聚簇索引，一个是二级索引idx_key1。只要是包含在索引中的列，那么这个列在记录中的位置（pos），占用存储空间大小（len）和实际值（value）就需要存储到undo日志中。 对于主键来说，只包含一个id列，存储到undo日志中的相关信息分别是： pos：id列是主键，也就是在记录的第一个列，它对应的pos值为0。pos占用1个字节来存储。 len：id列的类型为INT，占用4个字节，所以len的值为4。len占用1个字节来存储。 value：在被删除的记录中id列的值为1，也就是value的值为1。value占用4个字节来存储。 画一个图演示一下就是这样： 所以对于id列来说，最终存储的结果就是&lt;0, 4, 1&gt;，存储这些信息占用的存储空间大小为1 + 1 + 4 = 6个字节。 对于idx_key1来说，只包含一个key1列，存储到undo日志中的相关信息分别是： pos：key1列是排在id列、trx_id列、roll_pointer列之后的，它对应的pos值为3。pos占用1个字节来存储。 len：key1列的类型为VARCHAR(100)，使用utf8字符集，被删除的记录实际存储的内容是AWM，所以一共占用3个字节，也就是所以len的值为3。len占用1个字节来存储。 value：在被删除的记录中key1列的值为AWM，也就是value的值为AWM。value占用3个字节来存储。 画一个图演示一下就是这样： 所以对于key1列来说，最终存储的结果就是&lt;3, 3, &#39;AWM&#39;&gt;，存储这些信息占用的存储空间大小为1 + 1 + 3 = 5个字节。从上边的叙述中可以看到，&lt;0, 4, 1&gt;和&lt;3, 3, &#39;AWM&#39;&gt;共占用11个字节。然后index_col_info len本身占用2个字节，所以加起来一共占用13个字节，把数字13就填到了index_col_info len的属性中。 3.3 UPDATE操作对应的undo日志在执行UPDATE语句时，InnoDB对更新主键和不更新主键这两种情况有截然不同的处理方案。 ①不更新主键的情况在不更新主键的情况下，又可以细分为被更新的列占用的存储空间不发生变化和发生变化的情况。 就地更新（in-place update）更新记录时，对于被更新的每个列来说，如果更新后的列和更新前的列占用的存储空间都一样大，那么就可以进行就地更新，也就是直接在原记录的基础上修改对应列的值。 先删除掉旧记录，再插入新记录在不更新主键的情况下，如果有任何一个被更新的列更新前和更新后占用的存储空间大小不一致，那么就需要先把这条旧的记录从聚簇索引页面中删除掉，然后再根据更新后列的值创建一条新的记录插入到页面中。注意，这里所说的删除并不是delete mark操作，而是真正的删除掉，也就是把这条记录从正常记录链表中移除并加入到垃圾链表中，并且修改页面中相应的统计信息（比如PAGE_FREE、PAGE_GARBAGE等这些信息）。不过这里做真正删除操作的线程并不是在DELETE语句中做purge操作时使用的另外专门的线程，而是由用户线程同步执行真正的删除操作，真正删除之后紧接着就要根据各个列更新后的值创建的新记录插入。这里如果新创建的记录占用的存储空间大小不超过旧记录占用的空间，那么可以直接重用被加入到垃圾链表中的旧记录所占用的存储空间，否则的话需要在页面中新申请一段空间以供新记录使用，如果本页面内已经没有可用的空间的话，那就需要进行页面分裂操作，然后再插入新记录。 针对UPDATE不更新主键的情况（包括上边所说的就地更新和先删除旧记录再插入新记录），InnoDB设计了一种类型为TRX_UNDO_UPD_EXIST_REC的undo日志`，它的完整结构如下： 其实大部分属性和我们介绍过的TRX_UNDO_DEL_MARK_REC类型的undo日志是类似的，不过还是要注意这么几点： n_updated属性表示本条UPDATE语句执行后将有几个列被更新，后边跟着的&lt;pos, old_len, old_value&gt;分别表示被更新列在记录中的位置、更新前该列占用的存储空间大小、更新前该列的真实值。 如果在UPDATE语句中更新的列包含索引列，那么也会添加索引列各列信息这个部分，否则的话是不会添加这个部分的。 现在继续在上边那个事务id为100的事务中更新一条记录，比如我们把id为2的那条记录更新一下： 12345678910111213BEGIN; # 显式开启一个事务，假设该事务的id为100# 插入两条记录INSERT INTO undo_demo(id, key1, col) VALUES (1, &#x27;AWM&#x27;, &#x27;狙击枪&#x27;), (2, &#x27;M416&#x27;, &#x27;步枪&#x27;); # 删除一条记录 DELETE FROM undo_demo WHERE id = 1; # 更新一条记录UPDATE undo_demo SET key1 = &#x27;M249&#x27;, col = &#x27;机枪&#x27; WHERE id = 2; 这个UPDATE语句更新的列大小都没有改动，所以可以采用就地更新的方式来执行，在真正改动页面记录时，会先记录一条类型为TRX_UNDO_UPD_EXIST_REC的undo日志，长这样： 对照着这个图我们注意一下这几个地方： 因为这条undo日志是id为100的事务中产生的第4条undo日志，所以它对应的undo no就是3。 这条日志的roll_pointer指向undo no为1的那条日志，也就是插入主键值为2的记录时产生的那条undo日志，也就是最近一次对该记录做改动时产生的undo日志。 由于本条UPDATE语句中更新了索引列key1的值，所以需要记录一下索引列各列信息部分，也就是把主键和key1列更新前的信息填入。 ②更新主键的情况在聚簇索引中，记录是按照主键值的大小连成了一个单向链表的，如果我们更新了某条记录的主键值，意味着这条记录在聚簇索引中的位置将会发生改变，比如你将记录的主键值从1更新为10000，如果还有非常多的记录的主键值分布在1 ~ 10000之间的话，那么这两条记录在聚簇索引中就有可能离得非常远，甚至中间隔了好多个页面。针对UPDATE语句中更新了记录主键值的这种情况，InnoDB在聚簇索引中分了两步处理： 将旧记录进行delete mark操作高能注意：这里是delete mark操作！这里是delete mark操作！这里是delete mark操作！也就是说在UPDATE语句所在的事务提交前，对旧记录只做一个delete mark操作，在事务提交后才由专门的线程做purge操作，把它加入到垃圾链表中。 之所以只对旧记录做delete mark操作，是因为别的事务同时也可能访问这条记录，如果把它真正的删除加入到垃圾链表后，别的事务就访问不到了。这个功能就是所谓的MVCC。 根据更新后各列的值创建一条新记录，并将其插入到聚簇索引中（需重新定位插入的位置）。由于更新后的记录主键值发生了改变，所以需要重新从聚簇索引中定位这条记录所在的位置，然后把它插进去。 针对UPDATE语句更新记录主键值的这种情况，在对该记录进行delete mark操作前，会记录一条类型为TRX_UNDO_DEL_MARK_REC的undo日志；之后插入新记录时，会记录一条类型为TRX_UNDO_INSERT_REC的undo日志，也就是说每对一条记录的主键值做改动时，会记录2条undo日志。 4.通用链表结构在写入undo日志的过程中会使用到多个链表，很多链表都有同样的节点结构，如图所示： 在某个表空间内，我们可以通过一个页的页号和在页内的偏移量来唯一定位一个节点的位置，这两个信息也就相当于指向这个节点的一个指针。所以： Pre Node Page Number和Pre Node Offset的组合就是指向前一个节点的指针 Next Node Page Number和Next Node Offset的组合就是指向后一个节点的指针。 整个List Node占用12个字节的存储空间。 为了更好的管理链表，InnoDB还提出了一个基节点的结构，里边存储了这个链表的头节点、尾节点以及链表长度信息，基节点的结构示意图如下： 其中： List Length表明该链表一共有多少节点。 First Node Page Number和First Node Offset的组合就是指向链表头节点的指针。 Last Node Page Number和Last Node Offset的组合就是指向链表尾节点的指针。 整个List Base Node占用16个字节的存储空间。 所以使用List Base Node和List Node这两个结构组成的链表的示意图就是这样： 5.FIL_PAGE_UNDO_LOG页面表空间其实是由许许多多的页面构成的，页面默认大小为16KB。这些页面有不同的类型，比如类型为FIL_PAGE_INDEX的页面用于存储聚簇索引以及二级索引，类型为FIL_PAGE_TYPE_FSP_HDR的页面用于存储表空间头部信息的，还有其他各种类型的页面，其中有一种称之为FIL_PAGE_UNDO_LOG类型的页面是专门用来存储undo日志的，这种类型的页面的通用结构如下图所示（以默认的16KB大小为例）： 我们就简称为Undo页面，上图中的File Header和File Trailer是各种页面都有的通用结构。Undo Page Header是Undo页面所特有的，我们来看一下它的结构： 其中各个属性的意思如下： TRX_UNDO_PAGE_TYPE：本页面准备存储什么种类的undo日志。前边介绍了好几种类型的undo日志，它们可以被分为两个大类： TRX_UNDO_INSERT（使用十进制1表示）：类型为TRX_UNDO_INSERT_REC的undo日志属于此大类，一般由INSERT语句产生，或者在UPDATE语句中有更新主键的情况也会产生此类型的undo日志。 TRX_UNDO_UPDATE（使用十进制2表示），除了类型为TRX_UNDO_INSERT_REC的undo日志，其他类型的undo日志都属于这个大类，比如我们前边说的TRX_UNDO_DEL_MARK_REC、TRX_UNDO_UPD_EXIST_REC啥的，一般由DELETE、UPDATE语句产生的undo日志属于这个大类。 这个TRX_UNDO_PAGE_TYPE属性可选的值就是上边的两个，用来标记本页面用于存储哪个大类的undo日志，不同大类的undo日志不能混着存储，比如一个Undo页面的TRX_UNDO_PAGE_TYPE属性值为TRX_UNDO_INSERT，那么这个页面就只能存储类型为TRX_UNDO_INSERT_REC的undo日志，其他类型的undo日志就不能放到这个页面中了。 之所以把undo日志分成两个大类，是因为类型为TRX_UNDO_INSERT_REC的undo日志在事务提交后可以直接删除掉，而其他类型的undo日志还需要为所谓的MVCC服务，不能直接删除掉，对它们的处理需要区别对待。 TRX_UNDO_PAGE_START：表示在当前页面中是从什么位置开始存储undo日志的，或者说表示第一条undo日志在本页面中的起始偏移量。 TRX_UNDO_PAGE_FREE：与上边的TRX_UNDO_PAGE_START对应，表示当前页面中存储的最后一条undo日志结束时的偏移量，或者说从这个位置开始，可以继续写入新的undo日志。假设现在向页面中写入了3条undo日志，那么TRX_UNDO_PAGE_START和TRX_UNDO_PAGE_FREE的示意图就是这样：当然，在最初一条undo日志也没写入的情况下，TRX_UNDO_PAGE_START和TRX_UNDO_PAGE_FREE的值是相同的。 TRX_UNDO_PAGE_NODE：代表一个List Node结构（链表的普通节点，我们上边刚说的）。 6.Undo页面链表6.1单个事务中的Undo页面链表因为一个事务可能包含多个语句，而且一个语句可能对若干条记录进行改动，而对每条记录进行改动前，都需要记录1条或2条的undo日志，所以在一个事务执行过程中可能产生很多undo日志，这些日志可能一个页面放不下，需要放到多个页面中，这些页面就通过我们上边介绍的TRX_UNDO_PAGE_NODE属性连成了链表： 我特意把链表中的第一个Undo页面给标了出来，称它为first undo page，其余的Undo页面称之为normal undo page，这是因为在first undo page中除了记录Undo Page Header之外，还会记录其他的一些管理信息。 在一个事务执行过程中，可能混着执行INSERT、DELETE、UPDATE语句，也就意味着会产生不同类型的undo日志。但是，同一个Undo页面要么只存储TRX_UNDO_INSERT大类的undo日志，要么只存储TRX_UNDO_UPDATE大类的undo日志，反正不能混着存，所以在一个事务执行过程中就可能需要2个Undo页面的链表，一个称之为insert undo链表，另一个称之为update undo链表，画个示意图就是这样： 另外，InnoDB规定对普通表和临时表的记录改动时产生的undo日志要分别记录（我们稍后阐释为啥这么做），所以在一个事务中最多有4个以Undo页面为节点组成的链表： 当然，并不是在事务一开始就会为这个事务分配这4个链表，具体分配策略如下： 刚刚开启事务时，一个Undo页面链表也不分配。 当事务执行过程中向普通表中插入记录或者执行更新记录主键的操作之后，就会为其分配一个普通表的insert undo链表。 当事务执行过程中删除或者更新了普通表中的记录之后，就会为其分配一个普通表的update undo链表。 当事务执行过程中向临时表中插入记录或者执行更新记录主键的操作之后，就会为其分配一个临时表的insert undo链表。 当事务执行过程中删除或者更新了临时表中的记录之后，就会为其分配一个临时表的update undo链表。 总结一句就是：按需分配，啥时候需要啥时候再分配，不需要就不分配。 6.2多个事务中的Undo页面链表为了尽可能提高undo日志的写入效率，不同事务执行过程中产生的undo日志需要被写入到不同的Undo页面链表中。比方说现在有事务id分别为1、2的两个事务，我们分别称之为trx 1和trx 2，假设在这两个事务执行过程中： trx 1对普通表做了DELETE操作，对临时表做了INSERT和UPDATE操作。InnoDB会为trx 1分配3个链表，分别是： 针对普通表的update undo链表 针对临时表的insert undo链表 针对临时表的update undo链表。 trx 2对普通表做了INSERT、UPDATE和DELETE操作，没有对临时表做改动。InnoDB会为trx 2分配2个链表，分别是： 针对普通表的insert undo链表 针对普通表的update undo链表。 综上所述，在trx 1和trx 2执行过程中，InnoDB共需为这两个事务分配5个Undo页面链表，画个图就是这样： 如果有更多的事务，那就意味着可能会产生更多的Undo页面链表。 7.undo日志具体写入过程7.1段（Segment）的概念段是一个逻辑上的概念，本质上是由若干个零散页面和若干个完整的区组成的。比如一个B+树索引被划分成两个段，一个叶子节点段，一个非叶子节点段，这样叶子节点就可以被尽可能的存到一起，非叶子节点被尽可能的存到一起。每一个段对应一个INODE Entry结构，这个INODE Entry结构描述了这个段的各种信息，比如段的ID，段内的各种链表基节点，零散页面的页号有哪些等信息。我为了定位一个INODE Entry，InnoDB设计了一个Segment Header的结构： 整个Segment Header占用10个字节大小，各个属性的意思如下： Space ID of the INODE Entry：INODE Entry结构所在的表空间ID。 Page Number of the INODE Entry：INODE Entry结构所在的页面页号。 Byte Offset of the INODE Ent：INODE Entry结构在该页面中的偏移量 知道了表空间ID、页号、页内偏移量，就可以唯一定位一个INODE Entry的地址。 7.2Undo Log Segment HeaderInnoDB规定，每一个Undo页面链表都对应着一个段，称之为Undo Log Segment。也就是说链表中的页面都是从这个段里边申请的，所以他们在Undo页面链表的第一个页面，也就是上边提到的first undo page中设计了一个称之为Undo Log Segment Header的部分，这个部分中包含了该链表对应的段的segment header信息以及其他的一些关于这个段的信息，所以Undo页面链表的第一个页面其实长这样： 可以看到这个Undo链表的第一个页面比普通页面多了个Undo Log Segment Header，我们来看一下它的结构： 其中各个属性的意思如下： TRX_UNDO_STATE：本Undo页面链表处在什么状态。一个Undo Log Segment可能处在的状态包括： TRX_UNDO_ACTIVE：活跃状态，也就是一个活跃的事务正在往这个段里边写入undo日志。 TRX_UNDO_CACHED：被缓存的状态。处在该状态的Undo页面链表等待着之后被其他事务重用。 TRX_UNDO_TO_FREE：对于insert undo链表来说，如果在它对应的事务提交之后，该链表不能被重用，那么就会处于这种状态。 TRX_UNDO_TO_PURGE：对于update undo链表来说，如果在它对应的事务提交之后，该链表不能被重用，那么就会处于这种状态。 TRX_UNDO_PREPARED：包含处于PREPARE阶段的事务产生的undo日志。 TRX_UNDO_LAST_LOG：本Undo页面链表中最后一个Undo Log Header的位置。 TRX_UNDO_FSEG_HEADER：本Undo页面链表对应的段的Segment Header信息。 TRX_UNDO_PAGE_LIST：Undo页面链表的基节点。 Undo页面的Undo Page Header部分有一个12字节大小的TRX_UNDO_PAGE_NODE属性，这个属性代表一个List Node结构。每一个Undo页面都包含Undo Page Header结构，这些页面就可以通过这个属性连成一个链表。这个TRX_UNDO_PAGE_LIST属性代表着这个链表的基节点，当然这个基节点只存在于Undo页面链表的第一个页面，也就是first undo page中。 7.3Undo Log Header一个事务在向Undo页面中写入undo日志时的方式是十分简单暴力的，就是直接往里怼，写完一条紧接着写另一条，各条undo日志之间是亲密无间的。写完一个Undo页面后，再从段里申请一个新页面，然后把这个页面插入到Undo页面链表中，继续往这个新申请的页面中写。InnoDB认为同一个事务向一个Undo页面链表中写入的undo日志算是一个组，比方说我们上边介绍的trx 1由于会分配3个Undo页面链表，也就会写入3个组的undo日志；trx 2由于会分配2个Undo页面链表，也就会写入2个组的undo日志。在每写入一组undo日志时，都会在这组undo日志前先记录一下关于这个组的一些属性，InnoDB把存储这些属性的地方称之为Undo Log Header。所以Undo页面链表的第一个页面在真正写入undo日志前，其实都会被填充Undo Page Header、Undo Log Segment Header、Undo Log Header这3个部分，如图所示： 这个Undo Log Header具体的结构如下： 我们先大致看一下它们都是啥意思： TRX_UNDO_TRX_ID：生成本组undo日志的事务id。 TRX_UNDO_TRX_NO：事务提交后生成的一个需要序号，使用此序号来标记事务的提交顺序（先提交的此序号小，后提交的此序号大）。 TRX_UNDO_DEL_MARKS：标记本组undo日志中是否包含由于Delete mark操作产生的undo日志。 TRX_UNDO_LOG_START：表示本组undo日志中第一条undo日志的在页面中的偏移量。 TRX_UNDO_XID_EXISTS：本组undo日志是否包含XID信息。 TRX_UNDO_DICT_TRANS：标记本组undo日志是不是由DDL语句产生的。 TRX_UNDO_TABLE_ID：如果TRX_UNDO_DICT_TRANS为真，那么本属性表示DDL语句操作的表的table id。 TRX_UNDO_NEXT_LOG：下一组的undo日志在页面中开始的偏移量。 TRX_UNDO_PREV_LOG：上一组的undo日志在页面中开始的偏移量。 一般来说一个Undo页面链表只存储一个事务执行过程中产生的一组undo日志，但是在某些情况下，可能会在一个事务提交之后，之后开启的事务重复利用这个Undo页面链表，这样就会导致一个Undo页面中可能存放多组Undo日志，TRX_UNDO_NEXT_LOG和TRX_UNDO_PREV_LOG就是用来标记下一组和上一组undo日志在页面中的偏移量的。 TRX_UNDO_HISTORY_NODE：一个12字节的List Node结构，代表一个称之为History链表的节点。 7.4小结对于没有被重用的Undo页面链表来说，链表的第一个页面，也就是first undo page在真正写入undo日志前，会填充Undo Page Header、Undo Log Segment Header、Undo Log Header这3个部分，之后才开始正式写入undo日志。对于其他的页面来说，也就是normal undo page在真正写入undo日志前，只会填充Undo Page Header。链表的List Base Node存放到first undo page的Undo Log Segment Header部分，List Node信息存放到每一个Undo页面的undo Page Header部分，所以画一个Undo页面链表的示意图就是这样： 8.重用Undo页面为了能提高并发执行的多个事务写入undo日志的性能，InnoDB决定为每个事务单独分配相应的Undo页面链表（最多可能单独分配4个链表）。但是这样也造成了一些问题，比如其实大部分事务执行过程中可能只修改了一条或几条记录，针对某个Undo页面链表只产生了非常少的undo日志，这些undo日志可能只占用一点存储空间，每开启一个事务就新创建一个Undo页面链表（虽然这个链表中只有一个页面）来存储这么一点undo日志岂不是太浪费了么？的确是挺浪费，于是InnoDB决定在事务提交后在某些情况下重用该事务的Undo页面链表。一个Undo页面链表是否可以被重用的条件很简单： 该链表中只包含一个Undo页面。如果一个事务执行过程中产生了非常多的undo日志，那么它可能申请非常多的页面加入到Undo页面链表中。在该事物提交后，如果将整个链表中的页面都重用，那就意味着即使新的事务并没有向该Undo页面链表中写入很多undo日志，那该链表中也得维护非常多的页面，那些用不到的页面也不能被别的事务所使用，这样就造成了另一种浪费。所以InnoDB规定，只有在Undo页面链表中只包含一个Undo页面时，该链表才可以被下一个事务所重用。 该Undo页面已经使用的空间小于整个页面空间的3/4。 Undo页面链表按照存储的undo日志所属的大类可以被分为insert undo链表和update undo链表两种，这两种链表在被重用时的策略也是不同的，我们分别看一下： insert undo链表insert undo链表中只存储类型为TRX_UNDO_INSERT_REC的undo日志，这种类型的undo日志在事务提交之后就没用了，就可以被清除掉。所以在某个事务提交后，重用这个事务的insert undo链表（这个链表中只有一个页面）时，可以直接把之前事务写入的一组undo日志覆盖掉，从头开始写入新事务的一组undo日志，如下图所示：如图所示，假设有一个事务使用的insert undo链表，到事务提交时，只向insert undo链表中插入了3条undo日志，这个insert undo链表只申请了一个Undo页面。假设此刻该页面已使用的空间小于整个页面大小的3/4，那么下一个事务就可以重用这个insert undo链表（链表中只有一个页面)。假设此时有一个新事务重用了该insert undo链表，那么可以直接把旧的一组undo日志覆盖掉，写入一组新的undo日志。 在重用Undo页面链表写入新的一组undo日志时，不仅会写入新的Undo Log Header，还会适当调整Undo Page Header、Undo Log Segment Header、Undo Log Header中的一些属性，比如TRX_UNDO_PAGE_START、TRX_UNDO_PAGE_FREE等等。 update undo链表在一个事务提交后，它的update undo链表中的undo日志也不能立即删除掉（这些日志用于MVCC）。所以如果之后的事务想重用update undo链表时，就不能覆盖之前事务写入的undo日志。这样就相当于在同一个Undo页面中写入了多组的undo日志，效果看起来就是这样： 9.回滚段9.1回滚段的概念一个事务在执行过程中最多可以分配4个Undo页面链表，在同一时刻不同事务拥有的Undo页面链表是不一样的，所以在同一时刻系统里其实可以有许许多多个Undo页面链表存在。为了更好的管理这些链表，InnoDB又设计了一个称之为Rollback Segment Header的页面，在这个页面中存放了各个Undo页面链表的frist undo page的页号，这些页号称之为undo slot。可以这样理解，每个Undo页面链表都相当于是一个班，这个链表的first undo page就相当于这个班的班长，找到了这个班的班长，就可以找到班里的其他同学（其他同学相当于normal undo page）。有时候学校需要向这些班级传达一下精神，就需要把班长都召集在会议室，这个Rollback Segment Header就相当于是一个会议室。 我们看一下这个称之为Rollback Segment Header的页面长啥样（以默认的16KB为例）： InnoDB规定，每一个Rollback Segment Header页面都对应着一个段，这个段就称为Rollback Segment，翻译过来就是回滚段。与之前介绍的各种段不同的是，这个Rollback Segment里其实只有一个页面。 了解了Rollback Segment的含义之后，我们再来看看这个称之为Rollback Segment Header的页面的各个部分的含义都是啥意思： TRX_RSEG_MAX_SIZE：本Rollback Segment中管理的所有Undo页面链表中的Undo页面数量之和的最大值。换句话说，本Rollback Segment中所有Undo页面链表中的Undo页面数量之和不能超过TRX_RSEG_MAX_SIZE代表的值。该属性的值默认为无限大，也就是我们想写多少Undo页面都可以。 无限大其实也只是个夸张的说法，4个字节能表示最大的数也就是0xFFFFFFFF，但是0xFFFFFFFF这个数有特殊用途，所以实际上TRX_RSEG_MAX_SIZE的值为0xFFFFFFFE。 TRX_RSEG_HISTORY_SIZE：History链表占用的页面数量。 TRX_RSEG_HISTORY：History链表的基节点。 TRX_RSEG_FSEG_HEADER：本Rollback Segment对应的10字节大小的Segment Header结构，通过它可以找到本段对应的INODE Entry。 TRX_RSEG_UNDO_SLOTS：各个Undo页面链表的first undo page的页号集合，也就是undo slot集合。一个页号占用4个字节，对于16KB大小的页面来说，这个TRX_RSEG_UNDO_SLOTS部分共存储了1024个undo slot，所以共需1024 × 4 = 4096个字节。 9.2 从回滚段中申请Undo页面链表初始情况下，由于未向任何事务分配任何Undo页面链表，所以对于一个Rollback Segment Header页面来说，它的各个undo slot都被设置成了一个特殊的值：FIL_NULL（对应的十六进制就是0xFFFFFFFF），表示该undo slot不指向任何页面。 随着时间的流逝，开始有事务需要分配Undo页面链表了，就从回滚段的第一个undo slot开始，看看该undo slot的值是不是FIL_NULL： 如果是FIL_NULL，那么在表空间中新创建一个段（也就是Undo Log Segment），然后从段里申请一个页面作为Undo页面链表的first undo page，然后把该undo slot的值设置为刚刚申请的这个页面的页号，这样也就意味着这个undo slot被分配给了这个事务。 如果不是FIL_NULL，说明该undo slot已经指向了一个undo链表，也就是说这个undo slot已经被别的事务占用了，那就跳到下一个undo slot，判断该undo slot的值是不是FIL_NULL，重复上边的步骤。 一个Rollback Segment Header页面中包含1024个undo slot，如果这1024个undo slot的值都不为FIL_NULL，这就意味着这1024个undo slot都已经被分配给了某个事务，此时由于新事务无法再获得新的Undo页面链表，就会回滚这个事务并且给用户报错： 1Too many active concurrent transactions 用户看到这个错误，可以选择重新执行这个事务（可能重新执行时有别的事务提交了，该事务就可以被分配Undo页面链表了）。 当一个事务提交时，它所占用的undo slot有两种命运： 如果该undo slot指向的Undo页面链表符合被重用的条件（就是我们上边说的Undo页面链表只占用一个页面并且已使用空间小于整个页面的3/4）。该undo slot就处于被缓存的状态，InnoDB规定这时该Undo页面链表的TRX_UNDO_STATE属性（该属性在first undo page的Undo Log Segment Header部分）会被设置为TRX_UNDO_CACHED。 被缓存的undo slot都会被加入到一个链表，根据对应的Undo页面`链表的类型不同，也会被加入到不同的链表： 如果对应的Undo页面链表是insert undo链表，则该undo slot会被加入insert undo cached链表。 如果对应的Undo页面链表是update undo链表，则该undo slot会被加入update undo cached链表。 一个回滚段就对应着上述两个cached链表，如果有新事务要分配undo slot时，先从对应的cached链表中找。如果没有被缓存的undo slot，才会到回滚段的Rollback Segment Header页面中再去找。 如果该undo slot指向的Undo页面链表不符合被重用的条件，那么针对该undo slot对应的Undo页面链表类型不同，也会有不同的处理： 如果对应的Undo页面链表是insert undo链表，则该Undo页面链表的TRX_UNDO_STATE属性会被设置为TRX_UNDO_TO_FREE，之后该Undo页面链表对应的段会被释放掉（也就意味着段中的页面可以被挪作他用），然后把该undo slot的值设置为FIL_NULL。 如果对应的Undo页面链表是update undo链表，则该Undo页面链表的TRX_UNDO_STATE属性会被设置为TRX_UNDO_TO_PRUGE，则会将该undo slot的值设置为FIL_NULL，然后将本次事务写入的一组undo日志放到所谓的History链表中（需要注意的是，这里并不会将Undo页面链表对应的段给释放掉，因为这些undo日志还有用呢～）。 9.3多个回滚段一个事务执行过程中最多分配4个Undo页面链表，而一个回滚段里只有1024个undo slot，很显然undo slot的数量有点少。即使假设一个读写事务执行过程中只分配1个Undo页面链表，那1024个undo slot也只能支持1024个读写事务同时执行，再多了就崩溃了 在InnoDB的早期发展阶段的确只有一个回滚段，但是InnoDB后来意识到了这个问题，所以InnoDB一口气定义了128个回滚段，也就相当于有了128 × 1024 = 131072个undo slot。假设一个读写事务执行过程中只分配1个Undo页面链表，那么就可以同时支持131072个读写事务并发执行。 只读事务并不需要分配Undo页面链表，MySQL 5.7中所有刚开启的事务默认都是只读事务，只有在事务执行过程中对记录做了某些改动时才会被升级为读写事务。 每个回滚段都对应着一个Rollback Segment Header页面，有128个回滚段，自然就要有128个Rollback Segment Header页面，这些页面的地址需要找个地方存一下！于是InnoDB在系统表空间的第5号页面的某个区域包含了128个8字节大小的格子： 每个8字节的格子的构造就像这样： 如果所示，每个8字节的格子其实由两部分组成： 4字节大小的Space ID，代表一个表空间的ID。 4字节大小的Page number，代表一个页号。 也就是说每个8字节大小的格子相当于一个指针，指向某个表空间中的某个页面，这些页面就是Rollback Segment Header。这里需要注意的一点事，要定位一个Rollback Segment Header还需要知道对应的表空间ID，这也就意味着不同的回滚段可能分布在不同的表空间中。 所以通过上边的叙述我们可以大致清楚，在系统表空间的第5号页面中存储了128个Rollback Segment Header页面地址，每个Rollback Segment Header就相当于一个回滚段。在Rollback Segment Header页面中，又包含1024个undo slot，每个undo slot都对应一个Undo页面链表。我们画个示意图： 9.4回滚段的分类我们把这128个回滚段给编一下号，最开始的回滚段称之为第0号回滚段，之后依次递增，最后一个回滚段就称之为第127号回滚段。这128个回滚段可以被分成两大类： 第0号、第33～127号回滚段属于一类。其中第0号回滚段必须在系统表空间中（就是说第0号回滚段对应的Rollback Segment Header页面必须在系统表空间中），第33～127号回滚段既可以在系统表空间中，也可以在自己配置的undo表空间中。如果一个事务在执行过程中由于对普通表的记录做了改动需要分配Undo页面链表时，必须从这一类的段中分配相应的undo slot。 第1～32号回滚段属于一类。这些回滚段必须在临时表空间（对应着数据目录中的ibtmp1文件）中。如果一个事务在执行过程中由于对临时表的记录做了改动需要分配Undo页面链表时，必须从这一类的段中分配相应的undo slot。 也就是说如果一个事务在执行过程中既对普通表的记录做了改动，又对临时表的记录做了改动，那么需要为这个记录分配2个回滚段，再分别到这两个回滚段中分配对应的undo slot。 为啥要把针对普通表和临时表来划分不同种类的回滚段呢？这个还得从Undo页面本身说起，我们说Undo页面其实是类型为FIL_PAGE_UNDO_LOG的页面的简称，说到底它也是一个普通的页面。我们前边说过，在修改页面之前一定要先把对应的redo日志写上，这样在系统奔溃重启时才能恢复到奔溃前的状态。我们向Undo页面写入undo日志本身也是一个写页面的过程，InnoDB为此还设计了许多种redo日志的类型，比方说MLOG_UNDO_HDR_CREATE、MLOG_UNDO_INSERT、MLOG_UNDO_INIT等等等等，也就是说我们对Undo页面做的任何改动都会记录相应类型的redo日志。但是对于临时表来说，因为修改临时表而产生的undo日志只需要在系统运行过程中有效，如果系统奔溃了，那么在重启时也不需要恢复这些undo日志所在的页面，所以在写针对临时表的Undo页面时，并不需要记录相应的redo日志。总结一下针对普通表和临时表划分不同种类的回滚段的原因：在修改针对普通表的回滚段中的Undo页面时，需要记录对应的redo日志，而修改针对临时表的回滚段中的Undo页面时，不需要记录对应的redo日志。 实际上在MySQL 5.7.21这个版本中，如果我们仅仅对普通表的记录做了改动，那么只会为该事务分配针对普通表的回滚段，不分配针对临时表的回滚段。但是如果我们仅仅对临时表的记录做了改动，那么既会为该事务分配针对普通表的回滚段，又会为其分配针对临时表的回滚段（不过分配了回滚段并不会立即分配undo slot，只有在真正需要Undo页面链表时才会去分配回滚段中的undo slot）。 9.5为事务分配Undo页面链表详细过程接下来以事务对普通表的记录做改动为例，梳理一下事务执行过程中分配Undo页面链表时的完整过程： 事务在执行过程中对普通表的记录首次做改动之前，首先会到系统表空间的第5号页面中分配一个回滚段（其实就是获取一个Rollback Segment Header页面的地址）。一旦某个回滚段被分配给了这个事务，那么之后该事务中再对普通表的记录做改动时，就不会重复分配了。使用round-robin（循环使用）方式来分配回滚段。比如当前事务分配了第0号回滚段，那么下一个事务就要分配第33号回滚段，下下个事务就要分配第34号回滚段，简单一点的说就是这些回滚段被轮着分配给不同的事务。 在分配到回滚段后，首先看一下这个回滚段的两个cached链表有没有已经缓存了的undo slot，比如如果事务做的是INSERT操作，就去回滚段对应的insert undo cached链表中看看有没有缓存的undo slot；如果事务做的是DELETE操作，就去回滚段对应的update undo cached链表中看看有没有缓存的undo slot。如果有缓存的undo slot，那么就把这个缓存的undo slot分配给该事务。 如果没有缓存的undo slot可供分配，那么就要到Rollback Segment Header页面中找一个可用的undo slot分配给当前事务。从Rollback Segment Header页面中分配可用的undo slot的方式我们上边也说过了，就是从第0个undo slot开始，如果该undo slot的值为FIL_NULL，意味着这个undo slot是空闲的，就把这个undo slot分配给当前事务，否则查看第1个undo slot是否满足条件，依次类推，直到最后一个undo slot。如果这1024个undo slot都没有值为FIL_NULL的情况，就直接报错（一般不会出现这种情况）。 找到可用的undo slot后，如果该undo slot是从cached链表中获取的，那么它对应的Undo Log Segment已经分配了，否则的话需要重新分配一个Undo Log Segment，然后从该Undo Log Segment中申请一个页面作为Undo页面链表的first undo page。 然后事务就可以把undo日志写入到上边申请的Undo页面链表了！ 对临时表的记录做改动的步骤和上述的一样。不过需要再次强调一次，如果一个事务在执行过程中既对普通表的记录做了改动，又对临时表的记录做了改动，那么需要为这个记录分配2个回滚段。并发执行的不同事务其实也可以被分配相同的回滚段，只要分配不同的undo slot就可以了。 10.回滚段相关配置10.1配置回滚段数量系统中一共有128个回滚段，其实这只是默认值，我们可以通过启动参数innodb_rollback_segments来配置回滚段的数量，可配置的范围是1~128。但是这个参数并不会影响针对临时表的回滚段数量，针对临时表的回滚段数量一直是32，也就是说： 如果我们把innodb_rollback_segments的值设置为1，那么只会有1个针对普通表的可用回滚段，但是仍然有32个针对临时表的可用回滚段。 如果我们把innodb_rollback_segments的值设置为2～33之间的数，效果和将其设置为1是一样的。 如果我们把innodb_rollback_segments设置为大于33的数，那么针对普通表的可用回滚段数量就是该值减去32。 10.2 配置undo表空间默认情况下，针对普通表设立的回滚段（第0号以及第33~127号回滚段）都是被分配到系统表空间的。其中的第0号回滚段是一直在系统表空间的，但是第33~127号回滚段可以通过配置放到自定义的undo表空间中。但是这种配置只能在系统初始化（创建数据目录时）的时候使用，一旦初始化完成，之后就不能再次更改了。我们看一下相关启动参数： 通过innodb_undo_directory指定undo表空间所在的目录，如果没有指定该参数，则默认undo表空间所在的目录就是数据目录。 通过innodb_undo_tablespaces定义undo表空间的数量。该参数的默认值为0，表明不创建任何undo表空间。第33~127号回滚段可以平均分布到不同的undo表空间中。 如果我们在系统初始化的时候指定了创建了undo表空间，那么系统表空间中的第0号回滚段将处于不可用状态。 比如我们在系统初始化时指定的innodb_rollback_segments为35，innodb_undo_tablespaces为2，这样就会将第33、34号回滚段分别分布到一个undo表空间中。 设立undo表空间的一个好处就是在undo表空间中的文件大到一定程度时，可以自动的将该undo表空间截断（truncate）成一个小文件。而系统表空间的大小只能不断的增大，却不能截断。 11.总结为了保证事务的原子性设计，InnoDB引入了undo日志。undo日志记载了回滚一个段所需的必要内容。 在事务对表中的记录进行改动的时候，才会为这个事务分配一个唯一的ID。事务ID值是一个递增的数字。先被分配ID的事务得到的是较小的事务ID，后被分配ID的事务得到的是较大的事务ID。未被分配事务ID的事务ID默认是0。聚簇索引记录中有一个trx_id隐藏列，他代表对这个聚簇索引隐藏记录进行改动的语句所在的事务对应的事务ID。 InnoDB针对不同的场景设计了不同类型的undo日志。 类型为FIL_PAGE_UNDO_LOG的页面是专门用来存储undo日志的，简称为undo页面。 在一个事务执行过程中，最多分配四个undo页面链表： 针对普通表的insert undo链表 针对普通表的update undo链表 针对临时表的insert undo链表 针对临时表的update undo链表 只有在真正用到这些链表的时候才会去创建他们。 每个undo页面链表都对应一个undo log segment。undo页面链表的第一个页面中有一个名为undo log segment header 的部分，专门用来存储关于这个段的一些信息。 同一个事务向一个undo页面链表中写入的undo日志算是一个组，每个组都以一个undo log header部分开头。 一个undo页面链表如果可以被重用，需要符合两个条件： 该链表只包含一个undo页面 该undo页面已经使用的空间小于整个页面空间的3/4 每一个Rollback segmrnt header 页面都对应一个回滚段，每个回滚段包含1024个undo slot，一个undo slot代表一个undo页面链表的第一个页面的页号。目前，InnoDB最多支持128个回滚段，其中第0号，第33127号回滚段是针对普通表设计的，第132号回滚段是针对临时表设计的。 我们可以选择将undo日志记录到专门的undo表空间中，在undo表空间中的文件大到一定程度时，可以自动将该undo表空间截断为小文件。","categories":[{"name":"7.mysql","slug":"7-mysql","permalink":"https://zhangxin66666.github.io/categories/7-mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"}]},{"title":"redolog","slug":"7.mysql/redo日志","date":"2021-12-26T16:00:00.000Z","updated":"2024-08-22T03:23:14.707Z","comments":true,"path":"2021/12/27/7.mysql/redo日志/","link":"","permalink":"https://zhangxin66666.github.io/2021/12/27/7.mysql/redo%E6%97%A5%E5%BF%97/","excerpt":"","text":"一，什么是redo日志InnoDB存储引擎是以页为单位来管理存储空间的，我们的CRUD操作其实都是在访问页面。在真正访问页面之前，需要把磁盘中的页加载到内存的BufferPool，之后才能访问，但是因为事务要保持持久性，如果我们仅仅在内存的缓冲池修改了页面，假设事务提交后突然发生故障，导致内存的数据都消失了，那么这个已经提交的事务在数据库做的更改就丢失了。 如何保证持久性呢？可以在事务提交完成之前，把事务修改的所有页面都刷新到磁盘。不过这样做存在一些问题： 刷新一个完整的数据页过于浪费 随机IO效率比较低 事实上仅仅是为了保证事务的持久性，没有必要每次提交事务的时候就把该事务在内存修改过的全部页面刷新到磁盘，只需要把修改的内容记录一下就好，这样在事务提交的时候，就会把这个记录刷新到磁盘。即使系统因为崩溃而重启只需要按照记录的内容重新更新数据页即可恢复数据，上述记录修改的内容就叫做重做日志（redo log）。 相比于在事务提交的时候将所有修改过的内存中的页面刷新到磁盘，重做日志有以下好处： redo日志占用空间小：在存储表空间ID，页号，偏移量以及需要更新的值时，需要的存储空间很小。 redo日志是顺序写入磁盘的：在执行事务过程中，每执行一条语句，就可能产生若干条redo日志，这些日志是按照产生的顺序写入磁盘的，也就是使用顺序IO。 二，redo日志格式重做日志本质上仅仅是记录了一下事务对数据库进行了哪些修改。针对事务对数据库的不同修改场景MySQL定义了很多种重做日志，但是大部分类型的重做日志都有以下的通用结构。 type 重做日志的类型 space ID 表空间ID page number 页号 Data 日志的具体内容 1. 简单的redo日志类型行格式里面有一个隐藏列叫做row_id。为row_id进行赋值的方式如下： 服务器会在内存中维护一个全局变量，每当像某个包含row_id隐藏列的表插入一条记录的时候，就会把这个全局变量的值当做新记录row_id的值，并且把这个全局变量自增1。 每当这个全局变量的值是256的整数倍的时候，就会把这个变量的值刷新到系统表空间页号为7的页面中一个叫做Max Row ID的属性中。 当系统启动的时候，会将Max Row ID属性加载到内存，并把这个值加上256之后赋值给前面提到的全局变量。 这个Max Row ID占用的存储空间是8字节。当某个事务向某个包含row_id的表插入一条记录并且该记录分配的row_id值为256的整数倍的时候，就会像系统表空间页号为7的页面的相应偏移量处写入8字节的值。但是这个写入操作实际上是在内存缓冲区完成的，我们需要把这次修改以redo日志的形式记录下来，这样在事务提交之后，即使系统崩溃，也可以将该页面恢复成崩溃前的状态。在这种对页面的修改特别简单的时候，重做日志仅仅需要记录一下在某个页面的某个偏移量处修改了几个字节的值，具体修改后的内容是什么就可以了。这也叫做物理日志。 offset表示页面中的偏移量。如果写入的是字节序列类型的重做日志，还需要有一个len属性记录实际写入的长度。 2.复杂的redo日志类型有时候执行一条语句会修改非常多的页面，包括系统数据页面和用户数据页面（用户数据指的就是聚簇索引和二级索引对应的B+树） 这时我们如果使用简单的物理redo日志来记录这些修改时，可以有两种解决方案： 方案一：在每个修改的地方都记录一条redo日志。也就是有多少个修改的记录，就写多少条物理redo日志。这样子记录redo日志的缺点是显而易见的，因为被修改的地方是在太多了，可能记录的redo日志占用的空间都比整个页面占用的空间都多。 方案二：将整个页面的第一个被修改的字节到最后一个修改的字节之间所有的数据当成是一条物理redo日志中的具体数据。第一个被修改的字节到最后一个修改的字节之间仍然有许多没有修改过的数据，我们把这些没有修改的数据也加入到redo日志中太浪费了。 正因为上述两种使用物理redo日志的方式来记录某个页面中做了哪些修改比较浪费，InnoDB提出了一些新的redo日志类型。 这些类型的redo日志既包含物理层面的意思，也包含逻辑层面的意思，具体指： 物理层面看，这些日志都指明了对哪个表空间的哪个页进行了修改。 逻辑层面看，在系统崩溃重启时，并不能直接根据这些日志里的记载，将页面内的某个偏移量处恢复成某个数据，而是需要调用一些事先准备好的函数，执行完这些函数后才可以将页面恢复成系统崩溃前的样子。 这个类型为MLOG_COMP_REC_INSERT的redo日志并没有记录PAGE_N_DIR_SLOTS的值修改为了什么，PAGE_HEAP_TOP的值修改为了什么，PAGE_N_HEAP的值修改为了什么等等这些信息，而只是把在本页面中插入一条记录所有必备的要素记了下来，之后系统崩溃重启时，服务器会调用相关向某个页面插入一条记录的那个函数，而redo日志中的那些数据就可以被当成是调用这个函数所需的参数，在调用完该函数后，页面中的PAGE_N_DIR_SLOTS、PAGE_HEAP_TOP、PAGE_N_HEAP等等的值也就都被恢复到系统崩溃前的样子了。这就是所谓的逻辑日志的意思。 日志格式说了一堆核心其实就是：重做日志会把事务执行过程中对数据库所做的所有修改都记录下来，在之后系统因为崩溃而重启后可以把事务所做的任何修改都恢复过来。 为了节省重做日志占用的空间大小，InnoDB还对重做日志中的某些数据进行了压缩处理，比如表空间ID&amp;page number 一般占用4字节来存储，但是经过压缩之后占用的空间就更小了。 三，Mini-Transcation1.以组的形式写入redo日志语句在执行过程中可能修改若干个页面。比如我们前边说的一条INSERT语句可能修改系统表空间页号为7的页面的Max Row ID属性（当然也可能更新别的系统页面，只不过我们没有都列举出来而已），还会更新聚簇索引和二级索引对应B+树中的页面。由于对这些页面的更改都发生在Buffer Pool中，所以在修改完页面之后，需要记录一下相应的redo日志。在执行语句的过程中产生的redo日志被InnoDB人为的划分成了若干个不可分割的组，比如： 更新Max Row ID属性时产生的redo日志是不可分割的。 向聚簇索引对应B+树的页面中插入一条记录时产生的redo日志是不可分割的。 向某个二级索引对应B+树的页面中插入一条记录时产生的redo日志是不可分割的。 还有其他的一些对页面的访问操作时产生的redo日志是不可分割的。。。 怎么理解这个不可分割的意思呢？我们以向某个索引对应的B+树插入一条记录为例，在向B+树中插入这条记录之前，需要先定位到这条记录应该被插入到哪个叶子节点代表的数据页中，定位到具体的数据页之后，有两种可能的情况： 情况一：该数据页的剩余的空闲空间充足，足够容纳这一条待插入记录，那么事情很简单，直接把记录插入到这个数据页中，记录一条类型为MLOG_COMP_REC_INSERT的redo日志就好了，我们把这种情况称之为乐观插入。假如某个索引对应的B+树长这样：现在我们要插入一条键值为10的记录，很显然需要被插入到页b中，由于页b现在有足够的空间容纳一条记录，所以直接将该记录插入到页b中就好了，就像这样： 情况二：该数据页剩余的空闲空间不足，那么事情就悲剧了，我们前边说过，遇到这种情况要进行所谓的页分裂操作，也就是新建一个叶子节点，然后把原先数据页中的一部分记录复制到这个新的数据页中，然后再把记录插入进去，把这个叶子节点插入到叶子节点链表中，最后还要在内节点中添加一条目录项记录指向这个新创建的页面。很显然，这个过程要对多个页面进行修改，也就意味着会产生多条redo日志，我们把这种情况称之为悲观插入。假如某个索引对应的B+树长这样：现在我们要插入一条键值为10的记录，很显然需要被插入到页b中，但是从图中也可以看出来，此时页b已经塞满了记录，没有更多的空闲空间来容纳这条新记录了，所以我们需要进行页面的分裂操作，就像这样：如果作为内节点的页a的剩余空闲空间也不足以容纳增加一条目录项记录，那需要继续做内节点页a的分裂操作，也就意味着会修改更多的页面，从而产生更多的redo日志。另外，对于悲观插入来说，由于需要新申请数据页，还需要改动一些系统页面，比方说要修改各种段、区的统计信息信息，各种链表的统计信息（比如什么FREE链表、FSP_FREE_FRAG链表等，我们在介绍表空间那一篇中介绍过的各种东西)，反正总共需要记录的redo日志有二、三十条。 其实不光是悲观插入一条记录会生成许多条redo日志，InnoDB为了其他的一些功能，在乐观插入时也可能产生多条redo日志。 InnoDB认为向某个索引对应的B+树中插入一条记录的这个过程必须是原子的，不能说插了一半之后就停止了。比方说在悲观插入过程中，新的页面已经分配好了，数据也复制过去了，新的记录也插入到页面中了，可是没有向内节点中插入一条目录项记录，这个插入过程就是不完整的，这样会形成一棵不正确的B+树。我们知道redo日志是为了在系统崩溃重启时恢复崩溃前的状态，如果在悲观插入的过程中只记录了一部分redo日志，那么在系统崩溃重启时会将索引对应的B+树恢复成一种不正确的状态，这是InnoDB所不能忍受的。所以他们规定在执行这些需要保证原子性的操作时必须以组的形式来记录的redo日志，在进行系统崩溃重启恢复时，针对某个组中的redo日志，要么把全部的日志都恢复掉，要么一条也不恢复。怎么做到的呢？这得分情况讨论： 有的需要保证原子性的操作会生成多条redo日志，比如向某个索引对应的B+树中进行一次悲观插入就需要生成许多条redo日志。如何把这些redo日志划分到一个组里边儿呢？InnoDB做了一个很简单的操作，就是在该组中的最后一条redo日志后边加上一条特殊类型的redo日志，该类型名称为MLOG_MULTI_REC_END，type字段对应的十进制数字为31，该类型的redo日志结构很简单，只有一个type字段：所以某个需要保证原子性的操作产生的一系列redo日志必须要以一个类型为MLOG_MULTI_REC_END结尾，就像这样：这样在系统崩溃重启进行恢复时，只有当解析到类型为MLOG_MULTI_REC_END的redo日志，才认为解析到了一组完整的redo日志，才会进行恢复。否则的话直接放弃前边解析到的redo日志。 有的需要保证原子性的操作只生成一条redo日志，比如更新Max Row ID属性的操作就只会生成一条redo日志。其实在一条日志后边跟一个类型为MLOG_MULTI_REC_END的redo日志也是可以的，InnoDB不想浪费一个比特位。虽然redo日志的类型比较多，但撑死了也就是几十种，是小于127这个数字的，也就是说我们用7个比特位就足以包括所有的redo日志类型，而type字段其实是占用1个字节的，也就是说我们可以省出来一个比特位用来表示该需要保证原子性的操作只产生单一的一条redo日志，示意图如下：如果type字段的第一个比特位为1，代表该需要保证原子性的操作只产生了单一的一条redo日志，否则表示该需要保证原子性的操作产生了一系列的redo日志。 2.Mini-TransactionMySQL把对底层页面中的一次原子访问的过程称之为一个Mini-Transaction，简称mtr，比如上边所说的修改一次Max Row ID的值算是一个Mini-Transaction，向某个索引对应的B+树中插入一条记录的过程也算是一个Mini-Transaction。一个所谓的mtr可以包含一组redo日志，在进行崩溃恢复时这一组redo日志作为一个不可分割的整体。 一个事务可以包含若干条语句，每一条语句其实是由若干个mtr组成，每一个mtr又可以包含若干条redo日志，画个图表示它们的关系就是这样： 四，redo日志的写入过程1.redo log blockInnoDB为了更好的进行系统崩溃恢复，他们把通过mtr生成的redo日志都放在了大小为512字节的页中。为了和表空间中的页做区别，我们这里把用来存储redo日志的页称为block。一个redo log block的示意图如下： 真正的redo日志都是存储到占用496字节大小的log block body中，图中的log block header和log block trailer存储的是一些管理信息。 其中log block header的几个属性的意思分别如下： LOG_BLOCK_HDR_NO：每一个block都有一个大于0的唯一标号，本属性就表示该标号值。 LOG_BLOCK_HDR_DATA_LEN：表示block中已经使用了多少字节，初始值为12（因为log block body从第12个字节处开始）。随着往block中写入的redo日志越来也多，本属性值也跟着增长。如果log block body已经被全部写满，那么本属性的值被设置为512。 LOG_BLOCK_FIRST_REC_GROUP：一条redo日志也可以称之为一条redo日志记录（redo log record），一个mtr会生产多条redo日志记录，这些redo日志记录被称之为一个redo日志记录组（redo log record group）。LOG_BLOCK_FIRST_REC_GROUP就代表该block中第一个mtr生成的redo日志记录组的偏移量（其实也就是这个block里第一个mtr生成的第一条redo日志的偏移量）。 LOG_BLOCK_CHECKPOINT_NO：表示所谓的checkpoint的序号，checkpoint是我们后续内容的重点，现在先不用清楚它的意思，稍安勿躁。 log block trailer中属性的意思如下： LOG_BLOCK_CHECKSUM：表示block的校验值，用于正确性校验，我们暂时不关心它。 2.redo 日志缓冲区InnoDB为了解决磁盘速度过慢的问题而引入了Buffer Pool。同理，写入redo日志时也不能直接直接写到磁盘上，实际上在服务器启动时就向操作系统申请了一大片称之为redo log buffer的连续内存空间，翻译成中文就是redo日志缓冲区，也可以简称为log buffer。这片内存空间被划分成若干个连续的redo log block，就像这样： 我们可以通过启动参数innodb_log_buffer_size来指定log buffer的大小，在MySQL 5.7.21这个版本中，该启动参数的默认值为16MB。 3.redo log 日志写入log buffer向log buffer中写入redo日志的过程是顺序的，也就是先往前边的block中写，当该block的空闲空间用完之后再往下一个block中写。当我们想往log buffer中写入redo日志时，第一个遇到的问题就是应该写在哪个block的哪个偏移量处，所以InnoDB特意提供了一个称之为buf_free的全局变量，该变量指明后续写入的redo日志应该写入到log buffer中的哪个位置，如图所示： 一个mtr执行过程中可能产生若干条redo日志，这些redo日志是一个不可分割的组，所以其实并不是每生成一条redo日志，就将其插入到log buffer中，而是每个mtr运行过程中产生的日志先暂时存到一个地方，当该mtr结束的时候，将过程中产生的一组redo日志再全部复制到log buffer中。我们现在假设有两个名为T1、T2的事务，每个事务都包含2个mtr，我们给这几个mtr命名一下： 事务T1的两个mtr分别称为mtr_T1_1和mtr_T1_2。 事务T2的两个mtr分别称为mtr_T2_1和mtr_T2_2。 每个mtr都会产生一组redo日志，不同的事务可能是并发执行的，所以T1、T2之间的mtr可能是交替执行的。每当一个mtr执行完成时，伴随该mtr生成的一组redo日志就需要被复制到log buffer中，也就是说不同事务的mtr可能是交替写入log buffer的，我们画个示意图（为了美观，我们把一个mtr中产生的所有的redo日志当作一个整体来画）： 从示意图中我们可以看出来，不同的mtr产生的一组redo日志占用的存储空间可能不一样，有的mtr产生的redo日志量很少，比如mtr_t1_1、mtr_t2_1就被放到同一个block中存储，有的mtr产生的redo日志量非常大，比如mtr_t1_2产生的redo日志甚至占用了3个block来存储。 五，redo 日志文件1.redo日志刷盘时机mtr运行过程中产生的一组redo日志在mtr结束时会被复制到log buffer`中，在一些情况下它们会被刷新到磁盘里，比如： log buffer空间不足时log buffer的大小是有限的（通过系统变量innodb_log_buffer_size指定），如果不停的往这个有限大小的log buffer里塞入日志，很快它就会被填满。InnoDB认为如果当前写入log buffer的redo日志量已经占满了log buffer总容量的大约一半左右，就需要把这些日志刷新到磁盘上。 事务提交时之所以使用redo日志主要是因为它占用的空间少，还是顺序写，在事务提交时可以不把修改过的Buffer Pool页面刷新到磁盘，但是为了保证持久性，必须要把修改这些页面对应的redo日志刷新到磁盘。 将某个脏页刷新到磁盘前，会保证先将该脏页对应的 redo 日志刷新到磁盘中（再一次 强调，redo 日志是顺序刷新的，所以在将某个脏页对应的 redo 日志从 redo log buffer 刷新到磁盘时，也会保证将在其之前产生的 redo 日志也刷新到磁盘）。 后台线程不停的刷后台有一个线程，大约每秒都会刷新一次log buffer中的redo日志到磁盘。 正常关闭服务器时 做所谓的checkpoint时 其他的一些情况… 2.redo日志文件组MySQL的数据目录（使用SHOW VARIABLES LIKE &#39;datadir&#39;查看）下默认有两个名为ib_logfile0和ib_logfile1的文件，log buffer中的日志默认情况下就是刷新到这两个磁盘文件中。如果我们对默认的redo日志文件不满意，可以通过下边几个启动参数来调节： innodb_log_group_home_dir该参数指定了redo日志文件所在的目录，默认值就是当前的数据目录。 innodb_log_file_size该参数指定了每个redo日志文件的大小，在MySQL 5.7.21这个版本中的默认值为48MB， innodb_log_files_in_group该参数指定redo日志文件的个数，默认值为2，最大值为100。 磁盘上的redo日志文件不只一个，而是以一个日志文件组的形式出现的。这些文件以ib_logfile[数字]（数字可以是0、1、2…）的形式进行命名。在将redo日志写入日志文件组时，是从ib_logfile0开始写，如果ib_logfile0写满了，就接着ib_logfile1写，同理，ib_logfile1写满了就去写ib_logfile2，依此类推。如果写到最后一个文件该咋办？那就重新转到ib_logfile0继续写，所以整个过程如下图所示： 总共的redo日志文件大小其实就是：innodb_log_file_size × innodb_log_files_in_group。 如果采用循环使用的方式向redo日志文件组里写数据的话，那岂不是要追尾，也就是后写入的redo日志覆盖掉前边写的redo日志？当然可能了！所以InnoDB提出了checkpoint的概念。 3.redo日志文件格式log buffer本质上是一片连续的内存空间，被划分成了若干个512字节大小的block。将log buffer中的redo日志刷新到磁盘的本质就是把block的镜像写入日志文件中，所以redo日志文件其实也是由若干个512字节大小的block组成。 redo日志文件组中的每个文件大小都一样，格式也一样，都是由两部分组成： 前2048个字节，也就是前4个block是用来存储一些管理信息的。 从第2048字节往后是用来存储log buffer中的block镜像的。 所以我们前边所说的循环使用redo日志文件，其实是从每个日志文件的第2048个字节开始算，画个示意图就是这样： 普通block的格式我们在了解log buffer的时候都说过了，就是log block header、log block body、log block trialer这三个部分。这里需要介绍一下每个redo日志文件前2048个字节，也就是前4个特殊block的格式都是什么作用。 从图中可以看出来，这4个block分别是： log file header：描述该redo日志文件的一些整体属性各个属性的具体释义如下： 属性名 长度（单位：字节） 描述 LOG_HEADER_FORMAT 4 redo日志的版本，在MySQL 5.7.21中该值永远为1 LOG_HEADER_PAD1 4 做字节填充用的，没什么实际意义，忽略～ LOG_HEADER_START_LSN 8 标记本redo日志文件开始的LSN值，也就是文件偏移量为2048字节初对应的LSN值。 LOG_HEADER_CREATOR 32 一个字符串，标记本redo日志文件的创建者是谁。正常运行时该值为MySQL的版本号，比如：&quot;MySQL 5.7.21&quot;，使用mysqlbackup命令创建的redo日志文件的该值为&quot;ibbackup&quot;和创建时间。 LOG_BLOCK_CHECKSUM 4 本block的校验值，所有block都有，我们不关心 checkpoint1：记录关于checkpoint的一些属性，看一下它的结构：各个属性的具体释义如下： 属性名 长度（单位：字节） 描述 LOG_CHECKPOINT_NO 8 服务器做checkpoint的编号，每做一次checkpoint，该值就加1。 LOG_CHECKPOINT_LSN 8 服务器做checkpoint结束时对应的LSN值，系统崩溃恢复时将从该值开始。 LOG_CHECKPOINT_OFFSET 8 上个属性中的LSN值在redo日志文件组中的偏移量 LOG_CHECKPOINT_LOG_BUF_SIZE 8 服务器在做checkpoint操作时对应的log buffer的大小 LOG_BLOCK_CHECKSUM 4 本block的校验值，所有block都有，我们不关心 第三个block未使用，忽略 checkpoint2：结构和checkpoint1一样。 六，Log Sequence Number自系统开始运行，就不断的在修改页面，也就意味着会不断的生成redo日志。redo日志的量在不断的递增。InnoDB为记录已经写入的redo日志量，设计了一个称之为Log Sequence Number的全局变量，翻译过来就是：日志序列号，简称lsn。InnoDB规定初始的lsn值为8704（也就是一条redo日志也没写入时，lsn的值为8704）。 在向log buffer中写入redo日志时不是一条一条写入的，而是以一个mtr生成的一组redo日志为单位进行写入的。而且实际上是把日志内容写在了log block body处。但是在统计lsn的增长量时，是按照实际写入的日志量加上占用的log block header和log block trailer来计算的。我们来看一个例子： 系统第一次启动后初始化log buffer时，buf_free（就是标记下一条redo日志应该写入到log buffer的位置的变量）就会指向第一个block的偏移量为12字节（log block header的大小）的地方，那么lsn值也会跟着增加12： 如果某个mtr产生的一组redo日志占用的存储空间比较小，也就是待插入的block剩余空闲空间能容纳这个mtr提交的日志时，lsn增长的量就是该mtr生成的redo日志占用的字节数，就像这样：我们假设上图中mtr_1产生的redo日志量为200字节，那么lsn就要在8716的基础上增加200，变为8916。 如果某个mtr产生的一组redo日志占用的存储空间比较大，也就是待插入的block剩余空闲空间不足以容纳这个mtr提交的日志时，lsn增长的量就是该mtr生成的redo日志占用的字节数加上额外占用的log block header和log block trailer的字节数，就像这样：我们假设上图中mtr_2产生的redo日志量为1000字节，为了将mtr_2产生的redo日志写入log buffer，我们不得不额外多分配两个block，所以lsn的值需要在8916的基础上增加1000 + 12×2 + 4 × 2 = 1032。 从上边的描述中可以看出来，每一组由mtr生成的redo日志都有一个唯一的LSN值与其对应，LSN值越小，说明redo日志产生的越早。 1.flushed_to_disk_lsnredo日志是首先写到log buffer中，之后才会被刷新到磁盘上的redo日志文件。所以InnoDB提出了一个称之为buf_next_to_write的全局变量，标记当前log buffer中已经有哪些日志被刷新到磁盘中了。画个图表示就是这样： lsn是表示当前系统中写入的redo日志量，这包括了写到log buffer而没有刷新到磁盘的日志，相应的，InnoDB提出了一个表示刷新到磁盘中的redo日志量的全局变量，称之为flushed_to_disk_lsn。系统第一次启动时，该变量的值和初始的lsn值是相同的，都是8704。随着系统的运行，redo日志被不断写入log buffer，但是并不会立即刷新到磁盘，lsn的值就和flushed_to_disk_lsn的值拉开了差距。我们推理一下： 系统第一次启动后，向log buffer中写入了mtr_1、mtr_2、mtr_3这三个mtr产生的redo日志，假设这三个mtr开始和结束时对应的lsn值分别是： mtr_1：8716 ～ 8916 mtr_2：8916 ～ 9948 mtr_3：9948 ～ 10000 此时的lsn已经增长到了10000，但是由于没有刷新操作，所以此时flushed_to_disk_lsn的值仍为8704，如图： 随后进行将log buffer中的block刷新到redo日志文件的操作，假设将mtr_1和mtr_2的日志刷新到磁盘，那么flushed_to_disk_lsn就应该增长mtr_1和mtr_2写入的日志量，所以flushed_to_disk_lsn的值增长到了9948，如图： 综上所述，当有新的redo日志写入到log buffer时，首先lsn的值会增长，但flushed_to_disk_lsn不变，随后随着不断有log buffer中的日志被刷新到磁盘上，flushed_to_disk_lsn的值也跟着增长。如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中了。 应用程序向磁盘写入文件时其实是先写到操作系统的缓冲区中去，如果某个写入操作要等到操作系统确认已经写到磁盘时才返回，那需要调用一下操作系统提供的fsync函数。其实只有当系统执行了fsync函数后，flushed_to_disk_lsn的值才会跟着增长，当仅仅把log buffer中的日志写入到操作系统缓冲区却没有显式的刷新到磁盘时，另外的一个称之为write_lsn的值跟着增长。 2.lsn值和redo日志文件偏移量的对应关系因为lsn的值是代表系统写入的redo日志量的一个总和，一个mtr中产生多少日志，lsn的值就增加多少（当然有时候要加上log block header和log block trailer的大小），这样mtr产生的日志写到磁盘中时，很容易计算某一个lsn值在redo日志文件组中的偏移量，如图： 初始时的LSN值是8704，对应文件偏移量2048，之后每个mtr向磁盘中写入多少字节日志，lsn的值就增长多少。 3.flush链表中的LSN一个mtr代表一次对底层页面的原子访问，在访问过程中可能会产生一组不可分割的redo日志，在mtr结束时，会把这一组redo日志写入到log buffer中。除此之外，在mtr结束时还有一件非常重要的事情要做，就是把在mtr执行过程中可能修改过的页面加入到Buffer Pool的flush链表。 当第一次修改某个缓存在Buffer Pool中的页面时，就会把这个页面对应的控制块插入到flush链表的头部，之后再修改该页面时由于它已经在flush链表中了，就不再次插入了。也就是说flush链表中的脏页是按照页面的第一次修改时间从大到小进行排序的。在这个过程中会在缓存页对应的控制块中记录两个关于页面何时修改的属性： oldest_modification：如果某个页面被加载到Buffer Pool后进行第一次修改，那么就将修改该页面的mtr开始时对应的lsn值写入这个属性。 newest_modification：每修改一次页面，都会将修改该页面的mtr结束时对应的lsn值写入这个属性。也就是说该属性表示页面最近一次修改后对应的系统lsn值。 接着上边flushed_to_disk_lsn的例子看一下： 假设mtr_1执行过程中修改了页a，那么在mtr_1执行结束时，就会将页a对应的控制块加入到flush链表的头部。并且将mtr_1开始时对应的lsn，也就是8716写入页a对应的控制块的oldest_modification属性中，把mtr_1结束时对应的lsn，也就是8916写入页a对应的控制块的newest_modification属性中。画个图表示一下（oldest_modification缩写成了o_m，newest_modification缩写成了n_m）： 接着假设mtr_2执行过程中又修改了页b和页c两个页面，那么在mtr_2执行结束时，就会将页b和页c对应的控制块都加入到flush链表的头部。并且将mtr_2开始时对应的lsn，也就是8916写入页b和页c对应的控制块的oldest_modification属性中，把mtr_2结束时对应的lsn，也就是9948写入页b和页c对应的控制块的newest_modification属性中。画个图表示一下：从图中可以看出来，每次新插入到flush链表中的节点都是被放在了头部，也就是说flush链表中前边的脏页修改的时间比较晚，后边的脏页修改时间比较早。 接着假设mtr_3执行过程中修改了页b和页d，不过页b之前已经被修改过了，所以它对应的控制块已经被插入到了flush链表，所以在mtr_3执行结束时，只需要将页d对应的控制块都加入到flush链表的头部即可。所以需要将mtr_3开始时对应的lsn，也就是9948写入页d对应的控制块的oldest_modification属性中，把mtr_3结束时对应的lsn，也就是10000写入页d对应的控制块的newest_modification属性中。另外，由于页b在mtr_3执行过程中又发生了一次修改，所以需要更新页b对应的控制块中newest_modification的值为10000。画个图表示一下： 总结一下上边说的，就是：flush链表中的脏页按照修改发生的时间顺序进行排序，也就是按照oldest_modification代表的LSN值进行排序，被多次更新的页面不会重复插入到flush链表中，但是会更新newest_modification属性的值。 七，checkpointredo日志文件组容量是有限的，我们不得不选择循环使用redo日志文件组中的文件，但是这会造成最后写的redo日志与最开始写的redo日志追尾，这时应该想到：redo日志只是为了系统崩溃后恢复脏页用的，如果对应的脏页已经刷新到了磁盘，也就是说即使现在系统崩溃，那么在重启后也用不着使用redo日志恢复该页面了，所以该redo日志也就没有存在的必要了，那么它占用的磁盘空间就可以被后续的redo日志所重用。也就是说：判断某些redo日志占用的磁盘空间是否可以覆盖的依据就是它对应的脏页是否已经刷新到磁盘里。我们看一下前边的那个例子： 如图，虽然mtr_1和mtr_2生成的redo日志都已经被写到了磁盘上，但是它们修改的脏页仍然留在Buffer Pool中，所以它们生成的redo日志在磁盘上的空间是不可以被覆盖的。之后随着系统的运行，如果页a被刷新到了磁盘，那么它对应的控制块就会从flush链表中移除，就像这样子： 这样mtr_1生成的redo日志就没有用了，它们占用的磁盘空间就可以被覆盖掉了。InnoDB提出了一个全局变量checkpoint_lsn来代表当前系统中可以被覆盖的redo日志总量是多少，这个变量初始值也是8704。 比方说现在页a被刷新到了磁盘，mtr_1生成的redo日志就可以被覆盖了，所以我们可以进行一个增加checkpoint_lsn的操作，我们把这个过程称之为做一次checkpoint。做一次checkpoint其实可以分为两个步骤： 步骤一：计算一下当前系统中可以被覆盖的redo日志对应的lsn值最大是多少。redo日志可以被覆盖，意味着它对应的脏页被刷到了磁盘，只要我们计算出当前系统中被最早修改的脏页对应的oldest_modification值，那凡是在系统lsn值小于该节点的oldest_modification值时产生的redo日志都是可以被覆盖掉的，我们就把该脏页的oldest_modification赋值给checkpoint_lsn。比方说当前系统中页a已经被刷新到磁盘，那么flush链表的尾节点就是页c，该节点就是当前系统中最早修改的脏页了，它的oldest_modification值为8916，我们就把8916赋值给checkpoint_lsn（也就是说在redo日志对应的lsn值小于8916时就可以被覆盖掉）。 步骤二：将checkpoint_lsn和对应的redo日志文件组偏移量以及此次checkpint的编号写到日志文件的管理信息（就是checkpoint1或者checkpoint2）中。InnoDB维护了一个目前系统做了多少次checkpoint的变量checkpoint_no，每做一次checkpoint，该变量的值就加1。我们前边说过计算一个lsn值对应的redo日志文件组偏移量是很容易的，所以可以计算得到该checkpoint_lsn在redo日志文件组中对应的偏移量checkpoint_offset，然后把这三个值都写到redo日志文件组的管理信息中。我们说过，每一个redo日志文件都有2048个字节的管理信息，但是上述关于checkpoint的信息只会被写到日志文件组的第一个日志文件的管理信息中。不过我们是存储到checkpoint1中还是checkpoint2中呢？InnoDB规定，当checkpoint_no的值是偶数时，就写到checkpoint1中，是奇数时，就写到checkpoint2中。 记录完checkpoint的信息之后，redo日志文件组中各个lsn值的关系就像这样： 1.批量从flush链表中刷出脏页一般情况下都是后台的线程在对LRU链表和flush链表进行刷脏操作，这主要因为刷脏操作比较慢，不想影响用户线程处理请求。但是如果当前系统修改页面的操作十分频繁，这样就导致写日志操作十分频繁，系统lsn值增长过快。如果后台的刷脏操作不能将脏页刷出，那么系统无法及时做checkpoint，可能就需要用户线程同步的从flush链表中把那些最早修改的脏页（oldest_modification最小的脏页）刷新到磁盘，这样这些脏页对应的redo日志就没用了，然后就可以去做checkpoint了。 2.查看系统中的各种LSN值我们可以使用SHOW ENGINE INNODB STATUS命令查看当前InnoDB存储引擎中的各种LSN值的情况，比如： 12345678910111213mysql&gt; SHOW ENGINE INNODB STATUS\\G(...省略前边的许多状态)LOG---Log sequence number 124476971Log flushed up to 124099769Pages flushed up to 124052503Last checkpoint at 1240524940 pending log flushes, 0 pending chkp writes24 log i/o&#x27;s done, 2.00 log i/o&#x27;s/second----------------------(...省略后边的许多状态) 其中： Log sequence number：代表系统中的lsn值，也就是当前系统已经写入的redo日志量，包括写入log buffer中的日志。 Log flushed up to：代表flushed_to_disk_lsn的值，也就是当前系统已经写入磁盘的redo日志量。 Pages flushed up to：代表flush链表中被最早修改的那个页面对应的oldest_modification属性值。 Last checkpoint at：当前系统的checkpoint_lsn值。 3.innodb_flush_log_at_trx_commit的用法为了保证事务的持久性，用户线程在事务提交时需要将该事务执行过程中产生的所有redo日志都刷新到磁盘上。这一条要求太狠了，会很明显的降低数据库性能。如果对事务的持久性要求不是那么强烈的话，可以选择修改一个称为innodb_flush_log_at_trx_commit的系统变量的值，该变量有3个可选的值： 0：当该系统变量值为0时，表示在事务提交时不立即向磁盘中同步redo日志，这个任务是交给后台线程做的。这样很明显会加快请求处理速度，但是如果事务提交后服务器挂了，后台线程没有及时将redo日志刷新到磁盘，那么该事务对页面的修改会丢失。 1：当该系统变量值为1时，表示在事务提交时需要将redo日志同步到磁盘，可以保证事务的持久性。1也是innodb_flush_log_at_trx_commit的默认值。 2：当该系统变量值为2时，表示在事务提交时需要将redo日志写到操作系统的缓冲区中，但并不需要保证将日志真正的刷新到磁盘。这种情况下如果数据库挂了，操作系统没挂的话，事务的持久性还是可以保证的，但是操作系统也挂了的话，那就不能保证持久性了。 八，崩溃恢复在服务器不挂的情况下，redo日志不仅没用，反而让性能变得更差。但是万一数据库挂了，我们就可以在重启时根据redo日志中的记录就可以将页面恢复到系统崩溃前的状态。我们接下来大致看一下恢复过程。 1.确定恢复的起点checkpoint_lsn之前的redo日志都可以被覆盖，也就是说这些redo日志对应的脏页都已经被刷新到磁盘中了，既然它们已经被刷盘，我们就没必要恢复它们了。对于checkpoint_lsn之后的redo日志，它们对应的脏页可能没被刷盘，也可能被刷盘了，我们不能确定，所以需要从checkpoint_lsn开始读取redo日志来恢复页面。 当然，redo日志文件组的第一个文件的管理信息中有两个block都存储了checkpoint_lsn的信息，我们当然是要选取最近发生的那次checkpoint的信息。衡量checkpoint发生时间早晚的信息就是所谓的checkpoint_no，只要把checkpoint1和checkpoint2这两个block中的checkpoint_no值读出来比一下大小，哪个的checkpoint_no值更大，说明哪个block存储的就是最近的一次checkpoint信息。这样我们就能拿到最近发生的checkpoint对应的checkpoint_lsn值以及它在redo日志文件组中的偏移量checkpoint_offset。 2.确定恢复的终点redo日志恢复的起点确定了，那终点是哪个呢？这个还得从block的结构说起。在写redo日志的时候都是顺序写的，写满了一个block之后会再往下一个block中写。 普通block的log block header部分有一个称之为LOG_BLOCK_HDR_DATA_LEN的属性，该属性值记录了当前block里使用了多少字节的空间。对于被填满的block来说，该值永远为512。如果该属性的值不为512，那么就是它了，它就是此次崩溃恢复中需要扫描的最后一个block。 3.怎么恢复确定了需要扫描哪些redo日志进行崩溃恢复之后，接下来就是怎么进行恢复了。假设现在的redo日志文件中有5条redo日志，如图： 由于redo 0在checkpoint_lsn后前边，恢复时可以不管它。现在可以按照redo日志的顺序依次扫描checkpoint_lsn之后的各条redo日志，按照日志中记载的内容将对应的页面恢复出来。这样没什么问题，不过InnoDB还是想了一些办法加快这个恢复的过程： 使用哈希表根据redo日志的space ID和page number属性计算出散列值，把space ID和page number相同的redo日志放到哈希表的同一个槽里，如果有多个space ID和page number都相同的redo日志，那么它们之间使用链表连接起来，按照生成的先后顺序链接起来的，如图所示：之后就可以遍历哈希表，因为对同一个页面进行修改的redo日志都放在了一个槽里，所以可以一次性将一个页面修复好（避免了很多读取页面的随机IO)，这样可以加快恢复速度。另外需要注意一点的是，同一个页面的redo日志是按照生成时间顺序进行排序的，所以恢复的时候也是按照这个顺序进行恢复，如果不按照生成时间顺序进行排序的话，那么可能出现错误。比如原先的修改操作是先插入一条记录，再删除该条记录，如果恢复时不按照这个顺序来，就可能变成先删除一条记录，再插入一条记录，这显然是错误的。 跳过已经刷新到磁盘的页面checkpoint_lsn之前的redo日志对应的脏页确定都已经刷到磁盘了，但是checkpoint_lsn之后的redo日志我们不能确定是否已经刷到磁盘，主要是因为在最近做的一次checkpoint后，可能后台线程又不断的从LRU链表和flush链表中将一些脏页刷出Buffer Pool。这些在checkpoint_lsn之后的redo日志，如果它们对应的脏页在崩溃发生时已经刷新到磁盘，那在恢复时也就没有必要根据redo日志的内容修改该页面了。那在恢复时怎么知道某个redo日志对应的脏页是否在崩溃发生时已经刷新到磁盘了呢？这还得从页面的结构说起，每个页面都有一个称之为File Header的部分，在File Header里有一个称之为FIL_PAGE_LSN的属性，该属性记载了最近一次修改页面时对应的lsn值（其实就是页面控制块中的newest_modification值）。如果在做了某次checkpoint之后有脏页被刷新到磁盘中，那么该页对应的FIL_PAGE_LSN代表的lsn值肯定大于checkpoint_lsn的值，凡是符合这种情况的页面就不需要重复执行lsn值小于FIL_PAGE_LSN的redo日志了，所以更进一步提升了崩溃恢复的速度。 九，LOG_BLOCK_HDR_NO是如何计算的对于实际存储redo日志的普通的log block来说，在log block header处有一个称之为LOG_BLOCK_HDR_NO的属性，我们说这个属性代表一个唯一的标号。这个属性是初次使用该block时分配的，跟当时的系统lsn值有关。使用下边的公式计算该block的LOG_BLOCK_HDR_NO值： 1((lsn / 512) &amp; 0x3FFFFFFFUL) + 1 从图中可以看出，0x3FFFFFFFUL对应的二进制数的前2位为0，后30位的值都为1。一个二进制位与0做与运算（&amp;）的结果肯定是0，一个二进制位与1做与运算（&amp;）的结果就是原值。让一个数和0x3FFFFFFFUL做与运算的意思就是要将该值的前2个比特位的值置为0，这样该值就肯定小于或等于0x3FFFFFFFUL了。这也就说明了，不论lsn多大，((lsn / 512) &amp; 0x3FFFFFFFUL)的值肯定在0``0x3FFFFFFFUL之间，再加1的话肯定在1``0x40000000UL之间。而0x40000000UL这个值就代表着1GB。也就是说系统最多能产生不重复的LOG_BLOCK_HDR_NO值只有1GB个。InnoDB规定redo日志文件组中包含的所有文件大小总和不得超过512GB，一个block大小是512字节，也就是说redo日志文件组中包含的block块最多为1GB个，所以有1GB个不重复的编号值也就够用了。 另外，LOG_BLOCK_HDR_NO值的第一个比特位比较特殊，称之为flush bit，如果该值为1，代表着本block是在某次将log buffer中的block刷新到磁盘的操作中的第一个被刷入的block。 十，double write1.脏页刷盘风险关于IO的最小单位： 数据库IO的最小单位是16K（MySQL默认，oracle是8K） 文件系统IO的最小单位是4K（也有1K的） 磁盘IO的最小单位是512字节 因此，存在IO写入导致page损坏的风险： 2.doublewrite：两次写提高innodb的可靠性，用来解决部分写失败(partial page write页断裂)。 2.1 Double write解决了什么问题一个数据页的大小是16K，假设在把内存中的脏页写到数据库的时候，写了2K突然掉电，也就是说前2K数据是新的，后14K是旧的，那么磁盘数据库这个数据页就是不完整的，是一个坏掉的数据页。redo只能加上旧、校检完整的数据页恢复一个脏块，不能修复坏掉的数据页，所以这个数据就丢失了，可能会造成数据不一致，所以需要double write。 2.2使用情景当数据库正在从内存想磁盘写一个数据页是，数据库宕机，从而导致这个页只写了部分数据，这就是部分写失效，它会导致数据丢失。这时是无法通过重做日志恢复的，因为重做日志记录的是对页的物理修改，如果页本身已经损坏，重做日志也无能为力。 2.3 double write工作流程 doublewrite由两部分组成，一部分为内存中的doublewrite buffer，其大小为2MB，另一部分是磁盘上共享表空间(ibdata x)中连续的128个页，即2个区(extent)，大小也是2M。 当一系列机制触发数据缓冲池中的脏页刷新时，并不直接写入磁盘数据文件中，而是先拷贝至内存中的doublewrite buffer中； 接着从两次写缓冲区分两次写入磁盘共享表空间中(连续存储，顺序写，性能很高)，每次写1MB； 待第二步完成后，再将doublewrite buffer中的脏页数据写入实际的各个表空间文件(离散写)；(脏页数据固化后，即进行标记对应doublewrite数据可覆盖) 2.4 doublewrite的崩溃恢复如果操作系统在将页写入磁盘的过程中发生崩溃，在恢复过程中，innodb存储引擎可以从共享表空间的doublewrite中找到该页的一个最近的副本，将其复制到表空间文件，再应用redo log，就完成了恢复过程。因为有副本所以也不担心表空间中数据页是否损坏。 Q：为什么*log write*不需要*doublewrite*的支持？ A：因为*redolog*写入的单位就是512字节，也就是磁盘IO的最小单位，所以无所谓数据损坏。 3.doublewrite的副作用3.1 double write带来的写负载 double write是一个buffer, 但其实它是开在物理文件上的一个buffer, 其实也就是file, 所以它会导致系统有更多的fsync操作, 而硬盘的fsync性能是很慢的, 所以它会降低mysql的整体性能。 但是，doublewrite buffer写入磁盘共享表空间这个过程是连续存储，是顺序写，性能非常高，(约占写的10%)，牺牲一点写性能来保证数据页的完整还是很有必要的。 3.2 监控double write工作负载12345678mysql&gt; show global status like &#x27;%dblwr%&#x27;;+----------------------------+-------+| Variable_name | Value |+----------------------------+-------+| Innodb_dblwr_pages_written | 7 || Innodb_dblwr_writes | 3 |+----------------------------+-------+2 rows in set (0.00 sec) 关注点：Innodb_dblwr_pages_written / Innodb_dblwr_writes 开启doublewrite后，每次脏页刷新必须要先写doublewrite，而doublewrite存在于磁盘上的是两个连续的区，每个区由连续的页组成，一般情况下一个区最多有64个页，所以一次IO写入应该可以最多写64个页。 而根据以上系统Innodb_dblwr_pages_written与Innodb_dblwr_writes的比例来看，大概在3左右，远远还没到64(如果约等于64，那么说明系统的写压力非常大，有大量的脏页要往磁盘上写)，所以从这个角度也可以看出，系统写入压力并不高。 3.3 关闭double write适合的场景 海量DML 不惧怕数据损坏和丢失 系统写负载成为主要负载 1234567mysql&gt; show variables like &#x27;%double%&#x27;;+--------------------+-------+| Variable_name | Value |+--------------------+-------+| innodb_doublewrite | ON |+--------------------+-------+1 row in set (0.04 sec) 作为InnoDB的一个关键特性，doublewrite功能默认是开启的，但是在上述特殊的一些场景也可以视情况关闭，来提高数据库写性能。静态参数，配置文件修改，重启数据库。 3.4 为什么没有把double write里面的数据写到data page里面呢？ double write里面的数据是连续的，如果直接写到data page里面，而data page的页又是离散的，写入会很慢。 double write里面的数据没有办法被及时的覆盖掉，导致double write的压力很大；短时间内可能会出现double write溢出的情况。 十一，总结redo日志记录了事务执行过程中都修改了哪些内容。 事务提交时只将执行过程中产生的redo日志刷新到磁盘，而不是将所有修改过的页面都刷新到磁盘。这样做有两个好处： redo日志占用的空间非常小 redo日志是顺序写入磁盘的 一条redo日志由下面几部分组成。 type：这条redo日志的类型 space ID:表空间ID page number :页号 data：这条redo日志的具体内容 redo日志的类型有简单和复杂之分。简单类型的redo日志是纯粹的物理日志，复杂类型的redo日志兼有物理日志和逻辑日志的特性。 一个MTR可以包含一组redo日志。在进行崩溃恢复时，这一组redo日志作为一个不可分割的整体来处理。 redo日志存放在大小为512字节的block中。每一个block被分为3部分： log block header log block body log block trailer redo日志缓冲区是一片连续的内存空间，由若干个block组成；可以通过启动选项innodb_log_buffer_size 来调整他的大小。 redo日志文件组由若干个日志文件组成，这些redo日志文件是被循环使用的。redo日志文件组中每个文件的大小都一样，格式也一样，都是由两部分组成的： 前2048字节用来存储一些管理信息 从第2048字节往后的字节用来存储log buffer中的block镜像 lsn指已经写入的redo日志量，flushed_to_disk_lsn指刷新到磁盘中的redo日志量，flush链表中的脏页按照修改发生的时间顺序进行排序，也就是按照oldest_modification代表的lsn值进行排序。被多次更新的页面不会重复插入到flush链表，但是会更新newest_modification属性的值。checkpoint_lsn表示当前系统中可以被覆盖的redo日志总量是多少。 redo日志占用的磁盘空间在他对应的脏页已经被刷新到磁盘后即可被覆盖。执行一次checkpoint的意思就是增加checkpoint_lsn的值，然后把相关信息放到日志文件的管理信息中。 innodb_flush_log_at_trx_commit系统变量控制着在事务提交时是否将该事务运行过程中产生的redo刷新到磁盘。 在崩溃恢复过程中，从redo日志文件组第一个文件的管理信息中取出最近发生的那次checkpoint信息，然后从checkpoint_lsn在日志文件组中对应的偏移量开始，一直扫描日志文件中的block，直到某个block的LOG_BLOCK_HDR_DATA_LEN值不等于512为止。再恢复过程中，使用hash表可加快恢复过程，并且会跳过已经刷新到磁盘的页面。","categories":[{"name":"7.mysql","slug":"7-mysql","permalink":"https://zhangxin66666.github.io/categories/7-mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"}]}],"categories":[{"name":"5.缓存","slug":"5-缓存","permalink":"https://zhangxin66666.github.io/categories/5-%E7%BC%93%E5%AD%98/"},{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/categories/%E9%9D%A2%E8%AF%95/"},{"name":"1.基础知识","slug":"1-基础知识","permalink":"https://zhangxin66666.github.io/categories/1-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"7.mysql","slug":"7-mysql","permalink":"https://zhangxin66666.github.io/categories/7-mysql/"},{"name":"11.方法论","slug":"11-方法论","permalink":"https://zhangxin66666.github.io/categories/11-%E6%96%B9%E6%B3%95%E8%AE%BA/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://zhangxin66666.github.io/tags/redis/"},{"name":"JVM","slug":"JVM","permalink":"https://zhangxin66666.github.io/tags/JVM/"},{"name":"集合源码分析","slug":"集合源码分析","permalink":"https://zhangxin66666.github.io/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"},{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"算法","slug":"算法","permalink":"https://zhangxin66666.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"方法论","slug":"方法论","permalink":"https://zhangxin66666.github.io/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"}]}