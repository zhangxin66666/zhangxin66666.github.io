{"meta":{"title":"金子爸爸の家","subtitle":"热爱生活，喜欢软件","description":"欢迎来到金子爸爸博客の家","author":"金子爸爸","url":"https://zhangxin66666.github.io","root":"/"},"pages":[{"title":"","date":"2024-08-22T03:23:14.738Z","updated":"2024-08-22T03:23:14.738Z","comments":true,"path":"js/chocolate.js","permalink":"https://zhangxin66666.github.io/js/chocolate.js","excerpt":"","text":"// 友情链接页面 头像找不到时 替换图片 if (location.href.indexOf(\"link\") !== -1) { var imgObj = document.getElementsByTagName(\"img\"); for (i = 0; i < imgObj.length; i++) { imgObj[i].onerror = function() { this.src = \"https://cdn.jsdelivr.net/gh/tzy13755126023/BLOG_SOURCE/theme_f/friend_404.gif\" } } } $(function() { // 气泡 function bubble() { $('#page-header').circleMagic({ radius: 10, density: .2, color: 'rgba(255,255,255,.4)', clearOffset: 0.99 }); }! function(p) { p.fn.circleMagic = function(t) { var o, a, n, r, e = !0, i = [], d = p.extend({ color: \"rgba(255,0,0,.5)\", radius: 10, density: .3, clearOffset: .2 }, t), l = this[0]; function c() { e = !(document.body.scrollTop > a) } function s() { o = l.clientWidth, a = l.clientHeight, l.height = a + \"px\", n.width = o, n.height = a } function h() { if (e) for (var t in r.clearRect(0, 0, o, a), i) i[t].draw(); requestAnimationFrame(h) } function f() { var t = this; function e() { t.pos.x = Math.random() * o, t.pos.y = a + 100 * Math.random(), t.alpha = .1 + Math.random() * d.clearOffset, t.scale = .1 + .3 * Math.random(), t.speed = Math.random(), \"random\" === d.color ? t.color = \"rgba(\" + Math.floor(255 * Math.random()) + \", \" + Math.floor(0 * Math.random()) + \", \" + Math.floor(0 * Math.random()) + \", \" + Math.random().toPrecision(2) + \")\" : t.color = d.color } t.pos = {}, e(), this.draw = function() { t.alpha"},{"title":"标签","date":"2022-01-11T01:48:58.203Z","updated":"2022-01-11T01:48:58.203Z","comments":true,"path":"tags/index.html","permalink":"https://zhangxin66666.github.io/tags/index.html","excerpt":"","text":""},{"title":"留言板","date":"2022-01-11T01:48:58.203Z","updated":"2022-01-11T01:48:58.203Z","comments":true,"path":"message/index.html","permalink":"https://zhangxin66666.github.io/message/index.html","excerpt":"","text":"本页面还在开发中……"},{"title":"技术笔记","date":"2022-01-11T01:48:58.204Z","updated":"2022-01-11T01:48:58.204Z","comments":true,"path":"技术笔记/index.html","permalink":"https://zhangxin66666.github.io/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/index.html","excerpt":"","text":"我是技术笔记"},{"title":"分类","date":"2022-01-11T01:48:58.103Z","updated":"2022-01-11T01:48:58.103Z","comments":true,"path":"categories/index.html","permalink":"https://zhangxin66666.github.io/categories/index.html","excerpt":"","text":""},{"title":"关于我","date":"2024-08-22T03:23:14.738Z","updated":"2024-08-22T03:23:14.738Z","comments":true,"path":"关于我/index.html","permalink":"https://zhangxin66666.github.io/%E5%85%B3%E4%BA%8E%E6%88%91/index.html","excerpt":"","text":"关于我十年生死两茫茫,写程序，到天亮。千行代码，Bug何处藏。纵使上线又怎样，朝令改，夕断肠。领导每天新想法，天天改，日日忙。 相顾无言，惟有泪千行。每晚灯火阑珊处，程序员，又加班，工作狂~ 来一张楼主无美颜生活照，见笑了！🤣🤣🤣 基本信息 类别 信息 出生年月 1990年2月 现居地 北京市朝阳区 籍贯 辽宁省锦州市 邮箱 &#56;&#51;&#52;&#54;&#49;&#51;&#50;&#x40;&#x71;&#x71;&#x2e;&#x63;&#x6f;&#x6d; 教育经历 时间 学校 专业 备注 2009.09~2013.07 沈阳化工大学 电子科学与技术 统招本科 2006.09~2009.07 辽宁省北镇市高级中学 高级基础教育 市重点 工作经历 时间 公司 职位 2021.04~至今 龙湖集团 Java 开发工程师 2017.05~2021.04 包头市包银消费金融股份有限公司 Java 开发工程师 2016.09~2017.04 大连灵动科技发展有限公司 Java 开发工程师 2013.07~2016.08 大连安吉尼尔科技有限公司 Java 开发工程师 专业技能 Java 基础扎实、掌握 JVM 原理、多线程、网络原理、设计模式、常用的数据结构和算法 熟悉 Windows、Mac、Linux 操作系统，熟练使用 linux 常用操作指令 熟练使用 IntelliJ IDEA 开发工具(及各种插件)、熟练使用 Git 版本同步工具 阅读过 Spring、SpringMVC、等开源框架源码，理解其设计原理及底层架构，具备框架定制开发能力 理解 Redis 线程模型，Netty 线程模型，掌握基于响应式的异步非阻塞模型的基本原理，了解 Webflux 熟练掌握分布式缓存 Redis、Elaticsearch，对分布式锁，幂等等常见问题有深入研究及多年实战经验 熟悉常见消息中间件的使用，有多年 RabbitMQ 的实战开发经验，对高级消息队列有深入理解 熟练掌握 Mysql 事务，索引，锁，SQL 优化相关知识，可根据业务场景给出详细及高性能设计方案 熟练使用数据库操作框架 Mybatis、Mybatsi-plus 进行高效业务功能开发 掌握 springCloud 相关框架，对 SpringBoot、SpringCloud 原理有一定了解，有成熟项目经验 熟悉定时任务及延迟任务等业务相关设计，如 xxl-job，延迟消息等相关技术有多年开发经验 熟悉微服务思想，MVC 分层，DDD 理论，服务拆分，治理，监控，服务熔断，降级等相关能力 熟悉 jvm 原理，熟悉垃圾回收以 jvm 性能调优技术，有过线上服务器性能监测及调优经验 熟悉多线程及线程池使用，有多年多线程业务处理经验，封装过多线程批处理工具类等公用组建 了解 Mysql 分库分表相关原理，如 Sardingsphere、Mycat 等框架有相关使用经验 了解操作系统底层原理以及 C、C++程序开发，对计算机底层原理有初步了解 了解前端开发，了解 html，css，js，vue 等前端技术，对前端开发有一定的了解 研究过单片机等硬件开发，喜欢科技产品，喜欢软件，喜欢折腾各种电子产品以及软件 项目经验​ 项目经验只写了在北京之后参与过的相关项目，在大连做的项目偏向于传统，项目也都是单点部署，主要用的框架都是spring、mybatis、Hibernate、springMVC等相互结合使用，即：SSM，SSH，相比于springBoot来说不值一提，现在应该没有几个公司还没用springBoot了吧(#^.^#)，值得一提的是，刚毕业那会，做了半年的C语言嵌入式开发，外包到大连东软做对日的佳能相机系统，虽然目前我已经做了多年的java开发，但是那毕竟是我第一次参加开发项目，人都是有初恋情节的嘛，对自己的第一次念念不忘(我指得是工作🤭)，那半年让我对硬件底层有了一些理解，也算是最大的收获了吧。 —— 包银消费金融(现名：蒙商消费) ——​ 包头市包银消费金融股份有限公司是经中国银监会批准成立的持牌消费金融公司，由包商银行发起设立，包银消费金融为个人消费者提供消费信贷服务。其主要合作渠道有：证大财富，京东金条，微粒贷，去哪儿，京东借贷平台，分期乐，小米等多家放款渠道。目前，已累计完成 1200 多万客户注册和 320 多亿放款规模。 acs：核心交易系统主要包含信贷核算业务和虚拟账户业务，信贷核算主要负责借还款、核销减免交易、利息、罚息计提、各种费用等业务的计算和落地，日终生成交易流水文件供会计核算系统生成会计分录；虚拟账户部分主要为清结算人员提供交易产生的不同资金账户金额的变化及资金流向，为清结算人员对账提供数据支持；系统可以处理每分钟峰值 640 多笔授信申请，每小时峰值 2 万笔授信申请，贷后支持每小时 17 万客户数据。核心日终处理量达 152 万笔(借据数量)。 gls：财务核算系统对核心交易系统日终生成的交易流水解析之后按照分录借贷规则生成财务分录文件交给金蝶系统，17 年财务核算系统是买来的系统，19 年由我重新开发出属于公司自己的财务核算系统，并增加了财务对账功能(核心交易系统和财务分录对账)，不但及时发现线上交易的错误数据，及时解决问题，更提升了月终对账，年结的效率，实现了财务对账自动化，解决了年结人工对账的痛点。 ecif：渠道系统要负责对接第三方引流渠道，客户通过第三方渠道授信、借款，渠道引流到包银，由于不同渠道的授信、放款业务逻辑不同，针对不同渠道提供不同的功能开发。其中部分渠道积累一天客户授信请求指定时间统一发送授信申请到包银，系统可处理每分钟 800 笔授信请求。 css：清结算系统此系统是公司内部清结算人员使用的内部业务系统，主要功能有个人溢缴款账户管理、对公付款、对私付款、退款以及清结算同事转账业务的发起和审核功能 联合贷款系统主要功能有联合贷款协议，路由配置，路由规则，借据还款计划拆分，资方授信，借据、交易流水文件，联合贷款授信影音资料推送 监管报送系统按照监管报送需求，对借据(每月报送全量数据)，还款计划，还款流水，客户，产品，核销，借款申请，资产证券化，五级分类等信息进行加工之后推送给监管报送系统 自我评价业精于勤，荒于嬉 、行成于思，毁于随。 性格乐观开朗，话痨，热爱生活，喜欢拍视频记录生活，喜欢分享知识，分享技术，分享生活中的点点滴滴，脾气好，怕老婆 个人爱好 电子科技产品 软件 —— 喜欢win、Mac、Android 好用无广告软件研究及分享 足球 —— 喜欢但是没机会玩，怀念高中时代呀…. 三国杀 —— 基本已经被凉企逼到退游了….而且生活中也很难找到一起玩这个游戏的朋友了🤣 逛B站 —— 生活区和科技区一个不知名阿婆主😂，佛系更新视频 👉 点击打开我的B站个人空间 记得三连 👍👍👍 看电影 —— 喜欢科幻、漫威粉、追斗罗大陆….."},{"title":"","date":"2024-08-22T03:23:14.709Z","updated":"2024-08-22T03:23:14.709Z","comments":true,"path":"css/custom.css","permalink":"https://zhangxin66666.github.io/css/custom.css","excerpt":"","text":"/* 文章页H1-H6图标样式效果 */ h1::before, h2::before, h3::before, h4::before, h5::before, h6::before { -webkit-animation: ccc 1.6s linear infinite ; animation: ccc 1.6s linear infinite ; } @-webkit-keyframes ccc { 0% { -webkit-transform: rotate(0deg); transform: rotate(0deg) } to { -webkit-transform: rotate(-1turn); transform: rotate(-1turn) } } @keyframes ccc { 0% { -webkit-transform: rotate(0deg); transform: rotate(0deg) } to { -webkit-transform: rotate(-1turn); transform: rotate(-1turn) } } #content-inner.layout h1::before { color: #ef50a8 ; margin-left: -1.55rem; font-size: 1.3rem; margin-top: -0.23rem; } #content-inner.layout h2::before { color: #fb7061 ; margin-left: -1.35rem; font-size: 1.1rem; margin-top: -0.12rem; } #content-inner.layout h3::before { color: #ffbf00 ; margin-left: -1.22rem; font-size: 0.95rem; margin-top: -0.09rem; } #content-inner.layout h4::before { color: #a9e000 ; margin-left: -1.05rem; font-size: 0.8rem; margin-top: -0.09rem; } #content-inner.layout h5::before { color: #57c850 ; margin-left: -0.9rem; font-size: 0.7rem; margin-top: 0.0rem; } #content-inner.layout h6::before { color: #5ec1e0 ; margin-left: -0.9rem; font-size: 0.66rem; margin-top: 0.0rem; } #content-inner.layout h1:hover, #content-inner.layout h2:hover, #content-inner.layout h3:hover, #content-inner.layout h4:hover, #content-inner.layout h5:hover, #content-inner.layout h6:hover { color: #49b1f5 ; } #content-inner.layout h1:hover::before, #content-inner.layout h2:hover::before, #content-inner.layout h3:hover::before, #content-inner.layout h4:hover::before, #content-inner.layout h5:hover::before, #content-inner.layout h6:hover::before { color: #49b1f5 ; -webkit-animation: ccc 3.2s linear infinite ; animation: ccc 3.2s linear infinite ; } /* 页面设置icon转动速度调整 */ #rightside_config i.fas.fa-cog.fa-spin { animation: fa-spin 5s linear infinite ; } /*--------更换字体------------*/ @font-face { font-family: 'tzy'; /* 字体名自定义即可 */ src: url('https://cdn.jsdelivr.net/gh/tzy13755126023/BLOG_SOURCE/font/ZhuZiAWan.woff2'); /* 字体文件路径 */ font-display: swap; } /*body,*/ /*.gitcalendar {*/ /* font-family: tzy !important;*/ /*}*/ .categoryBar-list { max-height: 400px; } .clock-row { overflow: hidden; text-overflow: ellipsis; } /*3s为加载动画的时间，1为加载动画的次数，ease-in-out为动画效果*/ #page-header, #web_bg { -webkit-animation: imgblur 2s 1 ease-in-out; animation: imgblur 2s 1 ease-in-out; } @keyframes imgblur { 0% { filter: blur(5px); } 100% { filter: blur(0px); } } /*适配使用-webkit内核的浏览器 */ @-webkit-keyframes imgblur { 0% { -webkit-filter: blur(5px); } 100% { -webkit-filter: blur(0px); } } .table-wrap img { margin: .6rem auto .1rem !important; } /* 标签外挂 网站卡片 start */ .site-card-group img { margin: 0 auto .1rem !important; } .site-card-group .info a img { margin-right: 10px !important; } [data-theme='dark'] .site-card-group .site-card .info .title { color: #f0f0f0 !important; } [data-theme='dark'] .site-card-group .site-card .info .desc { color: rgba(255, 255, 255, .7) !important; } .site-card-group .info .desc { margin-top: 4px !important; } /* 代码块颜色 */ figure.highlight pre .addition { color: #00bf03 !important; }"}],"posts":[{"title":"ConcurrentHashMap源码解读","slug":"1.基础知识/ConcurrentHashMap","date":"2024-08-22T03:23:14.702Z","updated":"2024-08-22T03:23:14.702Z","comments":true,"path":"2024/08/22/1.基础知识/ConcurrentHashMap/","link":"","permalink":"https://zhangxin66666.github.io/2024/08/22/1.%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/ConcurrentHashMap/","excerpt":"","text":"#1.成员变量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147//散列表数组的最大限制private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//散列表默认值private static final int DEFAULT_CAPACITY = 16;static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;//并发级别：jdk7历史遗留问题，仅仅在初始化的时候使用到，并不是真正的代表并发级别private static final int DEFAULT_CONCURRENCY_LEVEL = 16;//负载因子，JDK1.8中 ConcurrentHashMap 是固定值private static final float LOAD_FACTOR = 0.75f;//树化阈值，指定桶位 链表长度达到8的话，有可能发生树化操作。static final int TREEIFY_THRESHOLD = 8;//红黑树转化为链表的阈值static final int UNTREEIFY_THRESHOLD = 6;//联合TREEIFY_THRESHOLD控制桶位是否树化，只有当table数组长度达到64且 某个桶位 中的链表长度达到8，才会真正树化static final int MIN_TREEIFY_CAPACITY = 64;//线程迁移数据最小步长，控制线程迁移任务最小区间一个值private static final int MIN_TRANSFER_STRIDE = 16;//计算扩容时候生成的一个 标识戳private static int RESIZE_STAMP_BITS = 16;//结果是65535 表示并发扩容最多线程数private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;//扩容相关private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;//当node节点hash=-1 表示当前节点已经被迁移了 ，fwd节点static final int MOVED = -1; // hash for forwarding nodes//node hash=-2 表示当前节点已经树化 且 当前节点为treebin对象 ，代理操作红黑树static final int TREEBIN = -2; // hash for roots of treesstatic final int RESERVED = -3; // hash for transient reservations//转化成二进制实际上是 31个 1 可以将一个负数通过位移运算得到一个正数static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash//当前系统的cpu数量static final int NCPU = Runtime.getRuntime().availableProcessors();//为了兼容7版本的chp保存的，核心代码并没有使用到private static final ObjectStreamField[] serialPersistentFields = &#123; new ObjectStreamField(&quot;segments&quot;, Segment[].class), new ObjectStreamField(&quot;segmentMask&quot;, Integer.TYPE), new ObjectStreamField(&quot;segmentShift&quot;, Integer.TYPE) &#125;;//散列表，长度一定是2次方数transient volatile Node&lt;K,V&gt;[] table;//扩容过程中，会将扩容中的新table 赋值给nextTable 保持引用，扩容结束之后，这里会被设置为Nullprivate transient volatile Node&lt;K,V&gt;[] nextTable;//LongAdder 中的 baseCount 未发生竞争时 或者 当前LongAdder处于加锁状态时，增量累到到baseCount中private transient volatile long baseCount;/** * sizeCtl &lt; 0 * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * sizeCtl &gt; 0 * * 1. 如果table未初始化，表示初始化大小 * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */private transient volatile int sizeCtl;/** * * 扩容过程中，记录当前进度。所有线程都需要从transferIndex中分配区间任务，去执行自己的任务。 */private transient volatile int transferIndex;/** * LongAdder中的cellsBuzy 0表示当前LongAdder对象无锁状态，1表示当前LongAdder对象加锁状态 */private transient volatile int cellsBusy;/** * LongAdder中的cells数组，当baseCount发生竞争后，会创建cells数组， * 线程会通过计算hash值 取到 自己的cell ，将增量累加到指定cell中 * 总数 = sum(cells) + baseCount */private transient volatile CounterCell[] counterCells;// Unsafe mechanicsprivate static final sun.misc.Unsafe U;/**表示sizeCtl属性在ConcurrentHashMap中内存偏移地址*/private static final long SIZECTL;/**表示transferIndex属性在ConcurrentHashMap中内存偏移地址*/private static final long TRANSFERINDEX;/**表示baseCount属性在ConcurrentHashMap中内存偏移地址*/private static final long BASECOUNT;/**表示cellsBusy属性在ConcurrentHashMap中内存偏移地址*/private static final long CELLSBUSY;/**表示cellValue属性在CounterCell中内存偏移地址*/private static final long CELLVALUE;/**表示数组第一个元素的偏移地址*/private static final long ABASE;private static final int ASHIFT;static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField(&quot;sizeCtl&quot;)); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField(&quot;transferIndex&quot;)); BASECOUNT = U.objectFieldOffset (k.getDeclaredField(&quot;baseCount&quot;)); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField(&quot;cellsBusy&quot;)); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField(&quot;value&quot;)); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); //表示数组单元所占用空间大小,scale 表示Node[]数组中每一个单元所占用空间大小 int scale = U.arrayIndexScale(ak); //1 0000 &amp; 0 1111 = 0 if ((scale &amp; (scale - 1)) != 0) throw new Error(&quot;data type scale not a power of two&quot;); //numberOfLeadingZeros() 这个方法是返回当前数值转换为二进制后，从高位到低位开始统计，看有多少个0连续在一块。 //8 =&gt; 1000 numberOfLeadingZeros(8) = 28 //4 =&gt; 100 numberOfLeadingZeros(4) = 29 //ASHIFT = 31 - 29 = 2 ？？ //ABASE + （5 &lt;&lt; ASHIFT） ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125; #2.基础方法##2.1 spread高位运算 123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; ##2.2 tabAt该方法获取对象中offset偏移地址对应的对象field的值。实际上这段代码的含义等价于tab[i],但是为什么不直接使用 tab[i]来计算呢？ getObjectVolatile，一旦看到 volatile 关键字，就表示可见性。因为对 volatile 写操作 happen-before 于 volatile 读操作，因此其他线程对 table 的修改均对 get 读取可见； 虽然 table 数组本身是增加了 volatile 属性，但是“volatile 的数组只针对数组的引用具有volatile 的语义，而不是它的元素”。 所以如果有其他线程对这个数组的元素进行写操作，那么当前线程来读的时候不一定能读到最新的值。出于性能考虑，Doug Lea 直接通过 Unsafe 类来对 table 进行操作。 123static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; ##2.3 casTabAtcas设置当前节点为桶位的头节点 1234static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; ##2.4 setTabAt 123static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; ##2.5 resizeStampresizeStamp 用来生成一个和扩容有关的扩容戳，具体有什么作用呢？ 123static final int resizeStamp(int n) &#123; return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1));&#125; Integer.numberOfLeadingZeros 这个方法是返回无符号整数 n 最高位非 0 位前面的 0 的个数。 比如 10 的二进制是 0000 0000 0000 0000 0000 0000 0000 1010，那么这个方法返回的值就是 28。 根据 resizeStamp 的运算逻辑，我们来推演一下，假如 n=16，那么 resizeStamp(16)=32796转化为二进制是[0000 0000 0000 0000 1000 0000 0001 1100] 接着再来看,当第一个线程尝试进行扩容的时候，会执行下面这段代码： U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)rs 左移 16 位，相当于原本的二进制低位变成了高位 1000 0000 0001 1100 0000 0000 00000000 然后再+2 =1000 0000 0001 1100 0000 0000 0000 0000+10=1000 0000 0001 1100 0000 00000000 0010 高 16 位代表扩容的标记、低 16 位代表并行扩容的线程数 这样来存储有什么好处呢？ 1，首先在 CHM 中是支持并发扩容的，也就是说如果当前的数组需要进行扩容操作，可以由多个线程来共同负责 2，可以保证每次扩容都生成唯一的生成戳，每次新的扩容，都有一个不同的 n，这个生成戳就是根据 n 来计算出来的一个数字，n 不同，这个数字也不同 第一个线程尝试扩容的时候，为什么是+2 因为 1 表示初始化，2 表示一个线程在执行扩容，而且对 sizeCtl 的操作都是基于位运算的，所以不会关心它本身的数值是多少，只关心它在二进制上的数值，而 sc + 1 会在低 16 位上加 1。##2.6 tableSizeFor经过多次位移返回大于等于c的最小的二次方数 1234567891011121314151617181920 /** * Returns a power of two table size for the given desired capacity. * See Hackers Delight, sec 3.2 * 返回&gt;=c的最小的2的次方数 * c=28 * n=27 =&gt; 0b 11011 * 11011 | 01101 =&gt; 11111 * 11111 | 00111 =&gt; 11111 * .... * =&gt; 11111 + 1 =100000 = 32 */private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; #3. 构造方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public ConcurrentHashMap() &#123;&#125;public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); //如果指定的容量超过允许的最大值，设置为最大值 int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); /** * sizeCtl &gt; 0 * 当目前table未初始化时，sizeCtl表示初始化容量 */ this.sizeCtl = cap;&#125;public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.sizeCtl = DEFAULT_CAPACITY; putAll(m);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor) &#123; this(initialCapacity, loadFactor, 1);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; //参数校验 if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); //如果初始容量小于并发级别，那就设置初始容量为并发级别 if (initialCapacity &lt; concurrencyLevel) initialCapacity = concurrencyLevel; //16/0.75 +1 = 22 long size = (long)(1.0 + (long)initialCapacity / loadFactor); // 22 - &gt; 32 int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); /** * sizeCtl &gt; 0 * 当目前table未初始化时，sizeCtl表示初始化容量 */ this.sizeCtl = cap;&#125; #4.put 1234public V put(K key, V value) &#123; //如果key已经存在，是否覆盖，默认是false return putVal(key, value, false);&#125; #5 putVal 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129final V putVal(K key, V value, boolean onlyIfAbsent) &#123; //控制k 和 v 不能为null if (key == null || value == null) throw new NullPointerException(); //通过spread方法，可以让高位也能参与进寻址运算。 int hash = spread(key.hashCode()); //binCount表示当前k-v 封装成node后插入到指定桶位后，在桶位中的所属链表的下标位置 //0 表示当前桶位为null，node可以直接放着 //2 表示当前桶位已经可能是红黑树 int binCount = 0; //tab 引用map对象的table //自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //f 表示桶位的头结点 //n 表示散列表数组的长度 //i 表示key通过寻址计算后，得到的桶位下标 //fh 表示桶位头结点的hash值 Node&lt;K,V&gt; f; int n, i, fh; //CASE1：成立，表示当前map中的table尚未初始化.. if (tab == null || (n = tab.length) == 0) //最终当前线程都会获取到最新的map.table引用。 tab = initTable(); //CASE2：i 表示key使用路由寻址算法得到 key对应 table数组的下标位置，tabAt 获取指定桶位的头结点 f else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //进入到CASE2代码块 前置条件 当前table数组i桶位是Null时。 //使用CAS方式 设置 指定数组i桶位 为 new Node&lt;K,V&gt;(hash, key, value, null),并且期望值是null //cas操作成功 表示ok，直接break for循环即可 //cas操作失败，表示在当前线程之前，有其它线程先你一步向指定i桶位设置值了。 //当前线程只能再次自旋，去走其它逻辑。 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //CASE3：前置条件，桶位的头结点一定不是null。 //条件成立表示当前桶位的头结点 为 FWD结点，表示目前map正处于扩容过程中.. else if ((fh = f.hash) == MOVED) //看到fwd节点后，当前节点有义务帮助当前map对象完成迁移数据的工作 //帮助扩容 tab = helpTransfer(tab, f); //CASE4：当前桶位 可能是 链表 也可能是 红黑树代理结点TreeBin else &#123; //当插入key存在时，会将旧值赋值给oldVal，返回给put方法调用处.. V oldVal = null; //使用sync 加锁“头节点”，理论上是“头结点” synchronized (f) &#123; //为什么又要对比一下，看看当前桶位的头节点 是否为 之前获取的头结点？ //为了避免其它线程将该桶位的头结点修改掉，导致当前线程从sync 加锁 就有问题了。之后所有操作都不用在做了。 if (tabAt(tab, i) == f) &#123;//条件成立，说明咱们 加锁 的对象没有问题，可以进来造了！ //条件成立，说明当前桶位就是普通链表桶位。 if (fh &gt;= 0) &#123; //1.当前插入key与链表当中所有元素的key都不一致时，当前的插入操作是追加到链表的末尾，binCount表示链表长度 //2.当前插入key与链表当中的某个元素的key一致时，当前插入操作可能就是替换了。binCount表示冲突位置（binCount - 1） binCount = 1; //迭代循环当前桶位的链表，e是每次循环处理节点。 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; //当前循环节点 key K ek; //条件一：e.hash == hash 成立 表示循环的当前元素的hash值与插入节点的hash值一致，需要进一步判断 //条件二：((ek = e.key) == key ||(ek != null &amp;&amp; key.equals(ek))) // 成立：说明循环的当前节点与插入节点的key一致，发生冲突了 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; //将当前循环的元素的 值 赋值给oldVal oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; //当前元素 与 插入元素的key不一致 时，会走下面程序。 //1.更新循环处理节点为 当前节点的下一个节点 //2.判断下一个节点是否为null，如果是null，说明当前节点已经是队尾了，插入数据需要追加到队尾节点的后面。 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //前置条件，该桶位一定不是链表 //条件成立，表示当前桶位是 红黑树代理结点TreeBin else if (f instanceof TreeBin) &#123; //p 表示红黑树中如果与你插入节点的key 有冲突节点的话 ，则putTreeVal 方法 会返回冲突节点的引用。 Node&lt;K,V&gt; p; //强制设置binCount为2，因为binCount &lt;= 1 时有其它含义，所以这里设置为了2 binCount = 2; //条件一：成立，说明当前插入节点的key与红黑树中的某个节点的key一致，冲突了 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; //将冲突节点的值 赋值给 oldVal oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; //说明当前桶位不为null，可能是红黑树 也可能是链表 if (binCount != 0) &#123; //如果binCount&gt;=8 表示处理的桶位一定是链表 if (binCount &gt;= TREEIFY_THRESHOLD) //调用转化链表为红黑树的方法 treeifyBin(tab, i); //说明当前线程插入的数据key，与原有k-v发生冲突，需要将原数据v返回给调用者。 if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //1.统计当前table一共有多少数据 //2.判断是否达到扩容阈值标准，触发扩容。 addCount(1L, binCount); return null;&#125; #6 initTable数组初始化方法，这个方法比较简单，就是初始化一个合适大小的数组。 sizeCtl ：这个标志是在 Node 数组初始化或者扩容的时候的一个控制位标识，负数代表正在进行初始化或者扩容操作。 -1 代表正在初始化 -N 代表有 N-1 个线程正在进行扩容操作，这里不是简单的理解成 n 个线程，sizeCtl 就是-N 0 标识 Node 数组还没有被初始化，正数代表初始化或者下一次扩容的大小 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Initializes table, using the size recorded in sizeCtl. * * sizeCtl &lt; 0 * * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * * * sizeCtl &gt; 0 * * * * 1. 如果table未初始化，表示初始化大小 * * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */private final Node&lt;K,V&gt;[] initTable() &#123; //tab 引用map.table //sc sizeCtl的临时值 Node&lt;K,V&gt;[] tab; int sc; //自旋 条件：map.table 尚未初始化 while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0) //大概率就是-1，表示其它线程正在进行创建table的过程，当前线程没有竞争到初始化table的锁。 Thread.yield(); // lost initialization race; just spin //1.sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 //2.如果table未初始化，表示初始化大小 //3.如果table已经初始化，表示下次扩容时的 触发条件（阈值） else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; //这里为什么又要判断呢？ 防止其它线程已经初始化完毕了，然后当前线程再次初始化..导致丢失数据。 //条件成立，说明其它线程都没有进入过这个if块，当前线程就是具备初始化table权利了。 if ((tab = table) == null || tab.length == 0) &#123; //sc大于0 创建table时 使用 sc为指定大小，否则使用 16 默认值. int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; //最终赋值给 map.table table = tab = nt; //n &gt;&gt;&gt; 2 =&gt; 等于 1/4 n n - (1/4)n = 3/4 n =&gt; 0.75 * n //sc 0.75 n 表示下一次扩容时的触发条件。 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; //1.如果当前线程是第一次创建map.table的线程话，sc表示的是 下一次扩容的阈值 //2.表示当前线程 并不是第一次创建map.table的线程，当前线程进入到else if 块 时，将 //sizeCtl 设置为了-1 ，那么这时需要将其修改为 进入时的值。 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; #7 addCount 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124private final void addCount(long x, int check) &#123; //as 表示 LongAdder.cells //b 表示LongAdder.base //s 表示当前map.table中元素的数量 CounterCell[] as; long b, s; //条件一：true-&gt;表示cells已经初始化了，当前线程应该去使用hash寻址找到合适的cell 去累加数据 // false-&gt;表示当前线程应该将数据累加到 base //条件二：false-&gt;表示写base成功，数据累加到base中了，当前竞争不激烈，不需要创建cells // true-&gt;表示写base失败，与其他线程在base上发生了竞争，当前线程应该去尝试创建cells。 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; //有几种情况进入到if块中？ //1.true-&gt;表示cells已经初始化了，当前线程应该去使用hash寻址找到合适的cell 去累加数据 //2.true-&gt;表示写base失败，与其他线程在base上发生了竞争，当前线程应该去尝试创建cells。 //a 表示当前线程hash寻址命中的cell CounterCell a; //v 表示当前线程写cell时的期望值 long v; //m 表示当前cells数组的长度 int m; //true -&gt; 未竞争 false-&gt;发生竞争 boolean uncontended = true; //条件一：as == null || (m = as.length - 1) &lt; 0 //true-&gt; 表示当前线程是通过 写base竞争失败 然后进入的if块，就需要调用fullAddCount方法去扩容 或者 重试.. LongAdder.longAccumulate //条件二：a = as[ThreadLocalRandom.getProbe() &amp; m]) == null 前置条件：cells已经初始化了 //true-&gt;表示当前线程命中的cell表格是个空，需要当前线程进入fullAddCount方法去初始化 cell，放入当前位置. //条件三：!(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x) // false-&gt;取反得到false，表示当前线程使用cas方式更新当前命中的cell成功 // true-&gt;取反得到true,表示当前线程使用cas方式更新当前命中的cell失败，需要进入fullAddCount进行重试 或者 扩容 cells。 if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) ) &#123; fullAddCount(x, uncontended); //考虑到fullAddCount里面的事情比较累，就让当前线程 不参与到 扩容相关的逻辑了，直接返回到调用点。 return; &#125; if (check &lt;= 1) return; //获取当前散列表元素个数，这是一个期望值 s = sumCount(); &#125; //表示一定是一个put操作调用的addCount if (check &gt;= 0) &#123; //tab 表示map.table //nt 表示map.nextTable //n 表示map.table数组的长度 //sc 表示sizeCtl的临时值 Node&lt;K,V&gt;[] tab, nt; int n, sc; /** * sizeCtl &lt; 0 * 1. -1 表示当前table正在初始化（有线程在创建table数组），当前线程需要自旋等待.. * 2.表示当前table数组正在进行扩容 ,高16位表示：扩容的标识戳 低16位表示：（1 + nThread） 当前参与并发扩容的线程数量 * * sizeCtl = 0，表示创建table数组时 使用DEFAULT_CAPACITY为大小 * * sizeCtl &gt; 0 * * 1. 如果table未初始化，表示初始化大小 * 2. 如果table已经初始化，表示下次扩容时的 触发条件（阈值） */ //自旋 //条件一：s &gt;= (long)(sc = sizeCtl) // true-&gt; 1.当前sizeCtl为一个负数 表示正在扩容中.. // 2.当前sizeCtl是一个正数，表示扩容阈值 // false-&gt; 表示当前table尚未达到扩容条件 //条件二：(tab = table) != null // 恒成立 true //条件三：(n = tab.length) &lt; MAXIMUM_CAPACITY // true-&gt;当前table长度小于最大值限制，则可以进行扩容。 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; //扩容批次唯一标识戳 //16 -&gt; 32 扩容 标识为：1000 0000 0001 1011 int rs = resizeStamp(n); //条件成立：表示当前table正在扩容 // 当前线程理论上应该协助table完成扩容 if (sc &lt; 0) &#123; //条件一：(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // true-&gt;说明当前线程获取到的扩容唯一标识戳 非 本批次扩容 // false-&gt;说明当前线程获取到的扩容唯一标识戳 是 本批次扩容 //条件二： JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs &lt;&lt; 16 ) + 1 // true-&gt; 表示扩容完毕，当前线程不需要再参与进来了 // false-&gt;扩容还在进行中，当前线程可以参与 //条件三：JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs&lt;&lt;16) + MAX_RESIZERS // true-&gt; 表示当前参与并发扩容的线程达到了最大值 65535 - 1 // false-&gt;表示当前线程可以参与进来 //条件四：(nt = nextTable) == null // true-&gt;表示本次扩容结束 // false-&gt;扩容正在进行中 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //前置条件：当前table正在执行扩容中.. 当前线程有机会参与进扩容。 //条件成立：说明当前线程成功参与到扩容任务中，并且将sc低16位值加1，表示多了一个线程参与工作 //条件失败：1.当前有很多线程都在此处尝试修改sizeCtl，有其它一个线程修改成功了，导致你的sc期望值与内存中的值不一致 修改失败 // 2.transfer 任务内部的线程也修改了sizeCtl。 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) //协助扩容线程，持有nextTable参数 transfer(tab, nt); &#125; //1000 0000 0001 1011 0000 0000 0000 0000 +2 =&gt; 1000 0000 0001 1011 0000 0000 0000 0010 //条件成立，说明当前线程是触发扩容的第一个线程，在transfer方法需要做一些扩容准备工作 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) //触发扩容条件的线程 不持有nextTable transfer(tab, null); s = sumCount(); &#125; &#125;&#125; #8. transferConcurrentHashMap 支持并发扩容，实现方式是，把 Node 数组进行拆分，让每个线程处理自己的区域，假设 table 数组总长度是 64，默认情况下，那么每个线程可以分到 16 个 bucket。然后每个线程处理的范围，按照倒序来做迁移。 通过 for 自循环处理每个槽位中的链表元素，默认 advace 为真，通过 CAS 设置 transferIndex属性值，并初始化 i 和 bound 值，i 指当前处理的槽位序号，bound 指需要处理的槽位边界，先处理槽位 31 的节点； （bound,i） =(16,31) 从 31 的位置往前推动。 每存在一个线程执行完扩容操作，就通过 cas 执行 sc-1。 接着判断(sc-2) !=resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT ; 如果相等，表示当前为整个扩容操作的 最后一个线程，那么意味着整个扩容操作就结束了；如果不相等，说明还得继续。 这么做的目的，一方面是防止不同扩容之间出现相同的 sizeCtl，另外一方面，还可以避免sizeCtl 的 ABA 问题导致的扩容重叠的情况。 扩容图解判断是否需要扩容，也就是当更新后的键值对总数 baseCount &gt;= 阈值 sizeCtl 时，进行rehash，这里面会有两个逻辑。 如果当前正在处于扩容阶段，则当前线程会加入并且协助扩容。 如果当前没有在扩容，则直接触发扩容操作。 扩容操作的核心在于数据的转移，在单线程环境下数据的转移很简单，无非就是把旧数组中的数据迁移到新的数组。但是这在多线程环境下，在扩容的时候其他线程也可能正在添加元素，这时又触发了扩容怎么办？可能大家想到的第一个解决方案是加互斥锁，把转移过程锁住，虽然是可行的解决方案，但是会带来较大的性能开销。因为互斥锁会导致所有访问临界区的线程陷入到阻塞状态，持有锁的线程耗时越长，其他竞争线程就会一直被阻塞，导致吞吐量较低。而且还可能导致死锁。 而 ConcurrentHashMap 并没有直接加锁，而是采用 CAS 实现无锁的并发同步策略，最精华的部分是它可以利用多线程来进行协同扩容。 它把 Node 数组当作多个线程之间共享的任务队列，然后通过维护一个指针来划分每个线程锁负责的区间，每个线程通过区间逆向遍历来实现扩容，一个已经迁移完的bucket会被替换为一个ForwardingNode节点，标记当前bucket已经被其他线程迁移完了。接下来分析一下它的源码实现。 fwd:这个类是个标识类，用于指向新表用的，其他线程遇到这个类会主动跳过这个类，因为这个类要么就是扩容迁移正在进行，要么就是已经完成扩容迁移，也就是这个类要保证线程安全，再进行操作。 advance:这个变量是用于提示代码是否进行推进处理，也就是当前桶处理完，处理下一个桶的标识。 finishing:这个变量用于提示扩容是否结束用的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; //n 表示扩容之前table数组的长度 //stride 表示分配给线程任务的步长 int n = tab.length, stride; // stride 固定为 16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range //条件成立：表示当前线程为触发本次扩容的线程，需要做一些扩容准备工作 //条件不成立：表示当前线程是协助扩容的线程.. if (nextTab == null) &#123; // initiating try &#123; //创建了一个比扩容之前大一倍的table @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; //赋值给对象属性 nextTable ，方便协助扩容线程 拿到新表 nextTable = nextTab; //记录迁移数据整体位置的一个标记。index计数是从1开始计算的。 transferIndex = n; &#125; //表示新数组的长度 int nextn = nextTab.length; //fwd 节点，当某个桶位数据处理完毕后，将此桶位设置为fwd节点，其它写线程 或读线程看到后，会有不同逻辑。 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); //推进标记 boolean advance = true; //完成标记 boolean finishing = false; // to ensure sweep before committing nextTab //i 表示分配给当前线程任务，执行到的桶位 //bound 表示分配给当前线程任务的下界限制 int i = 0, bound = 0; //自旋 for (;;) &#123; //f 桶位的头结点 //fh 头结点的hash Node&lt;K,V&gt; f; int fh; /** * 1.给当前线程分配任务区间 * 2.维护当前线程任务进度（i 表示当前处理的桶位） * 3.维护map对象全局范围内的进度 */ while (advance) &#123; //分配任务的开始下标 //分配任务的结束下标 int nextIndex, nextBound; //CASE1: //条件一：--i &gt;= bound //成立：表示当前线程的任务尚未完成，还有相应的区间的桶位要处理，--i 就让当前线程处理下一个 桶位. //不成立：表示当前线程任务已完成 或 者未分配 if (--i &gt;= bound || finishing) advance = false; //CASE2: //前置条件：当前线程任务已完成 或 者未分配 //条件成立：表示对象全局范围内的桶位都分配完毕了，没有区间可分配了，设置当前线程的i变量为-1 跳出循环后，执行退出迁移任务相关的程序 //条件不成立：表示对象全局范围内的桶位尚未分配完毕，还有区间可分配 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; //CASE3: //前置条件：1、当前线程需要分配任务区间 2.全局范围内还有桶位尚未迁移 //条件成立：说明给当前线程分配任务成功 //条件失败：说明分配给当前线程失败，应该是和其它线程发生了竞争吧 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; //CASE1： //条件一：i &lt; 0 //成立：表示当前线程未分配到任务 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; //保存sizeCtl 的变量 int sc; if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; //条件成立：说明设置sizeCtl 低16位 -1 成功，当前线程可以正常退出 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; //1000 0000 0001 1011 0000 0000 0000 0000 //条件成立：说明当前线程不是最后一个退出transfer任务的线程 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) //正常退出 return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; //前置条件：【CASE2~CASE4】 当前线程任务尚未处理完，正在进行中 //CASE2: //条件成立：说明当前桶位未存放数据，只需要将此处设置为fwd节点即可。 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //CASE3: //条件成立：说明当前桶位已经迁移过了，当前线程不用再处理了，直接再次更新当前线程任务索引，再次处理下一个桶位 或者 其它操作 else if ((fh = f.hash) == MOVED) advance = true; // already processed //CASE4: //前置条件：当前桶位有数据，而且node节点 不是 fwd节点，说明这些数据需要迁移。 else &#123; //sync 加锁当前桶位的头结点 synchronized (f) &#123; //防止在你加锁头对象之前，当前桶位的头对象被其它写线程修改过，导致你目前加锁对象错误... if (tabAt(tab, i) == f) &#123; //ln 表示低位链表引用 //hn 表示高位链表引用 Node&lt;K,V&gt; ln, hn; //条件成立：表示当前桶位是链表桶位 if (fh &gt;= 0) &#123; //lastRun //可以获取出 当前链表 末尾连续高位不变的 node int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; //条件成立：说明lastRun引用的链表为 低位链表，那么就让 ln 指向 低位链表 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; //否则，说明lastRun引用的链表为 高位链表，就让 hn 指向 高位链表 else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; //条件成立：表示当前桶位是 红黑树 代理结点TreeBin else if (f instanceof TreeBin) &#123; //转换头结点为 treeBin引用 t TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //低位双向链表 lo 指向低位链表的头 loTail 指向低位链表的尾巴 TreeNode&lt;K,V&gt; lo = null, loTail = null; //高位双向链表 lo 指向高位链表的头 loTail 指向高位链表的尾巴 TreeNode&lt;K,V&gt; hi = null, hiTail = null; //lc 表示低位链表元素数量 //hc 表示高位链表元素数量 int lc = 0, hc = 0; //迭代TreeBin中的双向链表，从头结点 至 尾节点 for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; // h 表示循环处理当前元素的 hash int h = e.hash; //使用当前节点 构建出来的 新的 TreeNode TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); //条件成立：表示当前循环节点 属于低位链 节点 if ((h &amp; n) == 0) &#123; //条件成立：说明当前低位链表 还没有数据 if ((p.prev = loTail) == null) lo = p; //说明 低位链表已经有数据了，此时当前元素 追加到 低位链表的末尾就行了 else loTail.next = p; //将低位链表尾指针指向 p 节点 loTail = p; ++lc; &#125; //当前节点 属于 高位链 节点 else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 链表迁移原理 1）高低位原理分析 ConcurrentHashMap 在做链表迁移时，会用高低位来实现，这里有两个问题要分析一下 1，如何实现高低位链表的区分 假如有这样一个队列第 14 个槽位插入新节点之后，链表元素个数已经达到了 8，且数组长度为 16，优先通过扩容来缓解链表过长的问题 假如当前线程正在处理槽位为 14 的节点，它是一个链表结构，在代码中，首先定义两个变量节点 ln 和 hn，实际就是 lowNode 和 HighNode，分别保存 hash 值的第 x 位为 0 和不等于0 的节点 通过 fn&amp;n 可以把这个链表中的元素分为两类，A 类是 hash 值的第 X 位为 0，B 类是 hash 值的第 x 位为不等于 0（至于为什么要这么区分，稍后分析），并且通过 lastRun 记录最后要处理的节点。最终要达到的目的是，A 类的链表保持位置不动，B 类的链表为 14+16(扩容增加的长度)=30 把 14 槽位的链表单独伶出来，用蓝色表示 fn&amp;n=0 的节点，假如链表的分类是这样 1234567for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125;&#125; 通过上面这段代码遍历，会记录 runBit 以及 lastRun，按照上面这个结构，那么 runBit 应该是蓝色节点，lastRun 应该是第 6 个节点接着，再通过这段代码进行遍历，生成 ln 链以及 hn 链 1234567for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn);&#125; 接着，通过 CAS 操作，把 hn 链放在 i+n 也就是 14+16 的位置，ln 链保持原来的位置不动。并且设置当前节点为 fwd，表示已经被当前线程迁移完了。 123setTabAt(nextTab, i, ln);setTabAt(nextTab, i + n, hn);setTabAt(tab, i, fwd); 迁移完成以后的数据分布如下 2)为什么要做高低位的划分 要想了解这么设计的目的，我们需要从 ConcurrentHashMap 的根据下标获取对象的算法来看，在 putVal 方法中 1018 行： (f = tabAt(tab, i = (n - 1) &amp; hash)) == null 通过(n-1) &amp; hash 来获得在 table 中的数组下标来获取节点数据，【&amp;运算是二进制运算符，1&amp; 1=1，其他都为 0】。 #9.helpTransfer如果对应的节点存在，判断这个节点的 hash 是不是等于 MOVED(-1)，说明当前节点是ForwardingNode 节点，意味着有其他线程正在进行扩容，那么当前现在直接帮助它进行扩容，因此调用 helpTransfer方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; //nextTab 引用的是 fwd.nextTable == map.nextTable 理论上是这样。 //sc 保存map.sizeCtl Node&lt;K,V&gt;[] nextTab; int sc; //条件一：tab != null 恒成立 true //条件二：(f instanceof ForwardingNode) 恒成立 true //条件三：((ForwardingNode&lt;K,V&gt;)f).nextTable) != null 恒成立 true if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; //拿当前标的长度 获取 扩容标识戳 假设 16 -&gt; 32 扩容：1000 0000 0001 1011 int rs = resizeStamp(tab.length); //条件一：nextTab == nextTable //成立：表示当前扩容正在进行中 //不成立：1.nextTable被设置为Null 了，扩容完毕后，会被设为Null // 2.再次出发扩容了...咱们拿到的nextTab 也已经过期了... //条件二：table == tab //成立：说明 扩容正在进行中，还未完成 //不成立：说明扩容已经结束了，扩容结束之后，最后退出的线程 会设置 nextTable 为 table //条件三：(sc = sizeCtl) &lt; 0 //成立：说明扩容正在进行中 //不成立：说明sizeCtl当前是一个大于0的数，此时代表下次扩容的阈值，当前扩容已经结束。 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; //条件一：(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs // true-&gt;说明当前线程获取到的扩容唯一标识戳 非 本批次扩容 // false-&gt;说明当前线程获取到的扩容唯一标识戳 是 本批次扩容 //条件二： JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs &lt;&lt; 16 ) + 1 // true-&gt; 表示扩容完毕，当前线程不需要再参与进来了 // false-&gt;扩容还在进行中，当前线程可以参与 //条件三：JDK1.8 中有bug jira已经提出来了 其实想表达的是 = sc == (rs&lt;&lt;16) + MAX_RESIZERS // true-&gt; 表示当前参与并发扩容的线程达到了最大值 65535 - 1 // false-&gt;表示当前线程可以参与进来 //条件四：transferIndex &lt;= 0 // true-&gt;说明map对象全局范围内的任务已经分配完了，当前线程进去也没活干.. // false-&gt;还有任务可以分配。 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table;&#125; #10.get 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public V get(Object key) &#123; //tab 引用map.table //e 当前元素 //p 目标节点 //n table数组长度 //eh 当前元素hash //ek 当前元素key Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //扰动运算后得到 更散列的hash值 int h = spread(key.hashCode()); //条件一：(tab = table) != null //true-&gt;表示已经put过数据，并且map内部的table也已经初始化完毕 //false-&gt;表示创建完map后，并没有put过数据，map内部的table是延迟初始化的，只有第一次写数据时会触发创建逻辑。 //条件二：(n = tab.length) &gt; 0 true-&gt;表示table已经初始化 //条件三：(e = tabAt(tab, (n - 1) &amp; h)) != null //true-&gt;当前key寻址的桶位 有值 //false-&gt;当前key寻址的桶位中是null，是null直接返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //前置条件：当前桶位有数据 //对比头结点hash与查询key的hash是否一致 //条件成立：说明头结点与查询Key的hash值 完全一致 if ((eh = e.hash) == h) &#123; //完全比对 查询key 和 头结点的key //条件成立：说明头结点就是查询数据 if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; //条件成立： //1.-1 fwd 说明当前table正在扩容，且当前查询的这个桶位的数据 已经被迁移走了 //2.-2 TreeBin节点，需要使用TreeBin 提供的find 方法查询。 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; //当前桶位已经形成链表的这种情况 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; #11.remove 123public V remove(Object key) &#123; return replaceNode(key, null, null);&#125; #12.replaceNode 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143final V replaceNode(Object key, V value, Object cv) &#123; //计算key经过扰动运算后的hash int hash = spread(key.hashCode()); //自旋 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //f表示桶位头结点 //n表示当前table数组长度 //i表示hash命中桶位下标 //fh表示桶位头结点 hash Node&lt;K,V&gt; f; int n, i, fh; //CASE1： //条件一：tab == null true-&gt;表示当前map.table尚未初始化.. false-&gt;已经初始化 //条件二：(n = tab.length) == 0 true-&gt;表示当前map.table尚未初始化.. false-&gt;已经初始化 //条件三：(f = tabAt(tab, i = (n - 1) &amp; hash)) == null true -&gt; 表示命中桶位中为null，直接break， 会返回 if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) &amp; hash)) == null) break; //CASE2： //前置条件CASE2 ~ CASE3：当前桶位不是null //条件成立：说明当前table正在扩容中，当前是个写操作，所以当前线程需要协助table完成扩容。 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); //CASE3: //前置条件CASE2 ~ CASE3：当前桶位不是null //当前桶位 可能是 &quot;链表&quot; 也可能 是 &quot;红黑树&quot; TreeBin else &#123; //保留替换之前的数据引用 V oldVal = null; //校验标记 boolean validated = false; //加锁当前桶位 头结点，加锁成功之后会进入 代码块。 synchronized (f) &#123; //判断sync加锁是否为当前桶位 头节点，防止其它线程，在当前线程加锁成功之前，修改过 桶位 的头结点。 //条件成立：当前桶位头结点 仍然为f，其它线程没修改过。 if (tabAt(tab, i) == f) &#123; //条件成立：说明桶位 为 链表 或者 单个 node if (fh &gt;= 0) &#123; validated = true; //e 表示当前循环处理元素 //pred 表示当前循环节点的上一个节点 Node&lt;K,V&gt; e = f, pred = null; for (;;) &#123; //当前节点key K ek; //条件一：e.hash == hash true-&gt;说明当前节点的hash与查找节点hash一致 //条件二：((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) //if 条件成立，说明key 与查询的key完全一致。 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; //当前节点的value V ev = e.val; //条件一：cv == null true-&gt;替换的值为null 那么就是一个删除操作 //条件二：cv == ev || (ev != null &amp;&amp; cv.equals(ev)) 那么是一个替换操作 if (cv == null || cv == ev || (ev != null &amp;&amp; cv.equals(ev))) &#123; //删除 或者 替换 //将当前节点的值 赋值给 oldVal 后续返回会用到 oldVal = ev; //条件成立：说明当前是一个替换操作 if (value != null) //直接替换 e.val = value; //条件成立：说明当前节点非头结点 else if (pred != null) //当前节点的上一个节点，指向当前节点的下一个节点。 pred.next = e.next; else //说明当前节点即为 头结点，只需要将 桶位设置为头结点的下一个节点。 setTabAt(tab, i, e.next); &#125; break; &#125; pred = e; if ((e = e.next) == null) break; &#125; &#125; //条件成立：TreeBin节点。 else if (f instanceof TreeBin) &#123; validated = true; //转换为实际类型 TreeBin t TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; //r 表示 红黑树 根节点 //p 表示 红黑树中查找到对应key 一致的node TreeNode&lt;K,V&gt; r, p; //条件一：(r = t.root) != null 理论上是成立 //条件二：TreeNode.findTreeNode 以当前节点为入口，向下查找key（包括本身节点） // true-&gt;说明查找到相应key 对应的node节点。会赋值给p if ((r = t.root) != null &amp;&amp; (p = r.findTreeNode(hash, key, null)) != null) &#123; //保存p.val 到pv V pv = p.val; //条件一：cv == null 成立：不必对value，就做替换或者删除操作 //条件二：cv == pv ||(pv != null &amp;&amp; cv.equals(pv)) 成立：说明“对比值”与当前p节点的值 一致 if (cv == null || cv == pv || (pv != null &amp;&amp; cv.equals(pv))) &#123; //替换或者删除操作 oldVal = pv; //条件成立：替换操作 if (value != null) p.val = value; //删除操作 else if (t.removeTreeNode(p)) //这里没做判断，直接搞了...很疑惑 setTabAt(tab, i, untreeify(t.first)); &#125; &#125; &#125; &#125; &#125; //当其他线程修改过桶位 头结点时，当前线程 sync 头结点 锁错对象时，validated 为false，会进入下次for 自旋 if (validated) &#123; if (oldVal != null) &#123; //替换的值 为null，说明当前是一次删除操作，oldVal ！=null 成立，说明删除成功，更新当前元素个数计数器。 if (value == null) addCount(-1L, -1); return oldVal; &#125; break; &#125; &#125; &#125; return null;&#125; #13.TreeBin##13.1 属性 1234567891011121314151617//红黑树 根节点 TreeNode&lt;K,V&gt; root;//链表的头节点volatile TreeNode&lt;K,V&gt; first;//等待者线程（当前lockState是读锁状态）volatile Thread waiter;/** * 1.写锁状态 写是独占状态，以散列表来看，真正进入到TreeBin中的写线程 同一时刻 只有一个线程。 1 * 2.读锁状态 读锁是共享，同一时刻可以有多个线程 同时进入到 TreeBin对象中获取数据。 每一个线程 都会给 lockStat + 4 * 3.等待者状态（写线程在等待），当TreeBin中有读线程目前正在读取数据时，写线程无法修改数据，那么就将lockState的最低2位 设置为 0b 10 */volatile int lockState;// values for lockStatestatic final int WRITER = 1; // set while holding write lockstatic final int WAITER = 2; // set when waiting for write lockstatic final int READER = 4; // increment value for setting read lock ##13.2 构造器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586TreeBin(TreeNode&lt;K,V&gt; b) &#123; //设置节点hash为-2 表示此节点是TreeBin节点 super(TREEBIN, null, null, null); //使用first 引用 treeNode链表 this.first = b; //r 红黑树的根节点引用 TreeNode&lt;K,V&gt; r = null; //x表示遍历的当前节点 for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; //强制设置当前插入节点的左右子树为null x.left = x.right = null; //条件成立：说明当前红黑树 是一个空树，那么设置插入元素 为根节点 if (r == null) &#123; //根节点的父节点 一定为 null x.parent = null; //颜色改为黑色 x.red = false; //让r引用x所指向的对象。 r = x; &#125; else &#123; //非第一次循环，都会来带else分支，此时红黑树已经有数据了 //k 表示 插入节点的key K k = x.key; //h 表示 插入节点的hash int h = x.hash; //kc 表示 插入节点key的class类型 Class&lt;?&gt; kc = null; //p 表示 为查找插入节点的父节点的一个临时节点 TreeNode&lt;K,V&gt; p = r; for (;;) &#123; //dir (-1, 1) //-1 表示插入节点的hash值大于 当前p节点的hash //1 表示插入节点的hash值 小于 当前p节点的hash //ph p表示 为查找插入节点的父节点的一个临时节点的hash int dir, ph; //临时节点 key K pk = p.key; //插入节点的hash值 小于 当前节点 if ((ph = p.hash) &gt; h) //插入节点可能需要插入到当前节点的左子节点 或者 继续在左子树上查找 dir = -1; //插入节点的hash值 大于 当前节点 else if (ph &lt; h) //插入节点可能需要插入到当前节点的右子节点 或者 继续在右子树上查找 dir = 1; //如果执行到 CASE3，说明当前插入节点的hash 与 当前节点的hash一致，会在case3 做出最终排序。最终 //拿到的dir 一定不是0，（-1， 1） else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); //xp 想要表示的是 插入节点的 父节点 TreeNode&lt;K,V&gt; xp = p; //条件成立：说明当前p节点 即为插入节点的父节点 //条件不成立：说明p节点 底下还有层次，需要将p指向 p的左子节点 或者 右子节点，表示继续向下搜索。 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; //设置插入节点的父节点 为 当前节点 x.parent = xp; //小于P节点，需要插入到P节点的左子节点 if (dir &lt;= 0) xp.left = x; //大于P节点，需要插入到P节点的右子节点 else xp.right = x; //插入节点后，红黑树性质 可能会被破坏，所以需要调用 平衡方法 r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; //将r 赋值给 TreeBin对象的 root引用。 this.root = r; assert checkInvariants(root);&#125; ##13.3 putTreeVal 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970final TreeNode&lt;K,V&gt; putTreeVal(int h, K k, V v) &#123; Class&lt;?&gt; kc = null; boolean searched = false; for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; K pk; if (p == null) &#123; first = root = new TreeNode&lt;K,V&gt;(h, k, v, null, null); break; &#125; else if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) &#123; if (!searched) &#123; TreeNode&lt;K,V&gt; q, ch; searched = true; if (((ch = p.left) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null) || ((ch = p.right) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null)) return q; &#125; dir = tieBreakOrder(k, pk); &#125; TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; //当前循环节点xp 即为 x 节点的爸爸 //x 表示插入节点 //f 老的头结点 TreeNode&lt;K,V&gt; x, f = first; first = x = new TreeNode&lt;K,V&gt;(h, k, v, f, xp); //条件成立：说明链表有数据 if (f != null) //设置老的头结点的前置引用为 当前的头结点。 f.prev = x; if (dir &lt;= 0) xp.left = x; else xp.right = x; if (!xp.red) x.red = true; else &#123; //表示 当前新插入节点后，新插入节点 与 父节点 形成 “红红相连” lockRoot(); try &#123; //平衡红黑树，使其再次符合规范。 root = balanceInsertion(root, x); &#125; finally &#123; unlockRoot(); &#125; &#125; break; &#125; &#125; assert checkInvariants(root); return null;&#125; ##13.4 find 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748final Node&lt;K,V&gt; find(int h, Object k) &#123; if (k != null) &#123; //e 表示循环迭代的当前节点 迭代的是first引用的链表 for (Node&lt;K,V&gt; e = first; e != null; ) &#123; //s 保存的是lock临时状态 //ek 链表当前节点 的key int s; K ek; //(WAITER|WRITER) =&gt; 0010 | 0001 =&gt; 0011 //lockState &amp; 0011 != 0 条件成立：说明当前TreeBin 有等待者线程 或者 目前有写操作线程正在加锁 if (((s = lockState) &amp; (WAITER|WRITER)) != 0) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; e = e.next; &#125; //前置条件：当前TreeBin中 等待者线程 或者 写线程 都没有 //条件成立：说明添加读锁成功 else if (U.compareAndSwapInt(this, LOCKSTATE, s, s + READER)) &#123; TreeNode&lt;K,V&gt; r, p; try &#123; //查询操作 p = ((r = root) == null ? null : r.findTreeNode(h, k, null)); &#125; finally &#123; //w 表示等待者线程 Thread w; //U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) //1.当前线程查询红黑树结束，释放当前线程的读锁 就是让 lockstate 值 - 4 //(READER|WAITER) = 0110 =&gt; 表示当前只有一个线程在读，且“有一个线程在等待” //当前读线程为 TreeBin中的最后一个读线程。 //2.(w = waiter) != null 说明有一个写线程在等待读操作全部结束。 if (U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) &amp;&amp; (w = waiter) != null) //使用unpark 让 写线程 恢复运行状态。 LockSupport.unpark(w); &#125; return p; &#125; &#125; &#125; return null;&#125; #总结在java8中，ConcurrentHashMap使用数组+链表+红黑树的组合方式，利用cas和synchronized保证并发写的安全。 引入红黑树的原因：链表查询的时间复杂度为On，但是红黑树的查询时间复杂度为O(log(n)),所以在节点比较多的情况下，使用红黑树可以大大提升性能。 链式桶是一个由node节点组成的链表。树状桶是一颗由TreeNode节点组成的红黑树。输的根节点为TreeBin类型。 当链表长度大于8整个hash表长度大于64的时候，就会转化为TreeBin。TreeBin作为根节点，其实就是红黑树对象。在ConcurrentHashMap的table数组中，存放的就是TreeBin对象，而不是TreeNoe对象。 数组table是懒加载的，只有第一次添加元素的时候才会初始化，所以initTable()存在线程安全问题。 重要的属性就是sizeCtl，用来控制table的初始化和扩容操作的过程： ● -1代表table正在初始化，其他线程直接join等待。 ● -N代表有N-1个线程正在进行扩容操作，严格来说，当其为负数的时候，只用到了低16位，如果低16位为M，此时有M-1个线程进行扩容。 ● 大于0有两种情况：如果table没有初始化，她就表示table初始化的大小，如果table初始化完了，就表示table的容量，默认是table大小的四分之三。 Transfer()扩容 table数据转移到nextTable。扩容操作的核心在于数据的转移，把旧数组中的数据迁移到新的数组。ConcurrentHashMap精华的部分是它可以利用多线程来进行协同扩容，简单来说，它把table数组当作多个线程之间共享的任务队列，然后通过维护一个指针来划分每个线程所负责的区间，每个线程通过区间逆向遍历来实现扩容，一个已经迁移完的 Bucket会被替换为一个Forwarding节点，标记当前Bucket已经被其他线程迁移完了。 helpTransfer()帮助扩容 ConcurrentHashMap并发添加元素时，如果正在扩容，其他线程会帮助扩容，也就是多线程扩容。 第一次添加元素时，默认初始长度为16，当往table中继续添加元素时，通过Hash值跟数组长度取余来决定放在数组的哪个Bucket位置，如果出现放在同一个位置，就优先以链表的形式存放，在同一个位置的个数达到了8个以上，如果数组的长度还小于64，就会扩容数组。如果数组的长度大于等于64，就会将该节点的链表转换成树。 通过扩容数组的方式来把这些节点分散开。然后将这些元素复制到扩容后的新数组中，同一个Bucket中的元素通过Hash值的数组长度位来重新确定位置，可能还是放在原来的位置，也可能放到新的位置。而且，在扩容完成之后，如果之前某个节点是树，但是现在该节点的“Key-Value对”数又小于等于6个，就会将该树转为链表。 put() JDK1.8在使用CAS自旋完成桶的设置时，使用synchronized内置锁保证桶内并发操作的线程安全。尽管对同一个Map操作的线程争用会非常激烈，但是在同一个桶内的线程争用通常不会很激烈，所以使用CAS自旋、synchronized不会降低ConcurrentHashMap的性能。为什么不用ReentrantLock显式锁呢?如果为每 个桶都创建一个ReentrantLock实 例，就会带来大量的内存消耗，反过来，使用CAS自旋、synchronized，内存消耗的增加更小。 get() get()通过UnSafe的getObjectVolatile()来读取数组中的元素。为什么要这样做?虽然HashEntry数组的引用是volatile类型，但是数组内元素的 用不是volatile类型，因此多线程对 数组元素的修改是不安全的，可能会在数组中读取到尚未构造完成的元素对象。get()方法通过UnSafe的getObjectVolatile方法来保证元素的读取安全，调用getObjectVolatile()去读取数组元素需要先获得元素在数组中的偏移量，在这里，get()方法根据哈希码计算出偏移量为u，然后通过偏移量u来尝试读取数值。","categories":[{"name":"1.基础知识","slug":"1-基础知识","permalink":"https://zhangxin66666.github.io/categories/1-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"集合源码分析","slug":"集合源码分析","permalink":"https://zhangxin66666.github.io/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]},{"title":"mysql执行流程","slug":"7.mysql/mysql执行流程","date":"2022-04-01T16:00:00.000Z","updated":"2024-08-22T03:23:14.706Z","comments":true,"path":"2022/04/02/7.mysql/mysql执行流程/","link":"","permalink":"https://zhangxin66666.github.io/2022/04/02/7.mysql/mysql%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/","excerpt":"","text":"一、MySQL的内部组件结构 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 Server层 主要包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数 （如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 Store层 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在 最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。也就是说如果我们在create table时不指定 表的存储引擎类型,默认会给你设置存储引擎为InnoDB。 本节课演示表的DDL： 123456789CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8; 下面我们重点来分析连接器、查询缓存、分析器、优化器、执行器分别主要干了哪些事情。 二、连接器我们知道由于MySQL是开源的，他有非常多种类的客户端：navicat,mysql front,jdbc,SQLyog等非常丰富的客户端,这些 客户端要向mysql发起通信都必须先跟Server端建立通信连接，而建立连接的工作就是有连接器完成的。 第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管 理连接。连接命令一般是这么写的： 1[root@192 ~]# mysql ‐h host[数据库地址] ‐u root[用户] ‐p root[密码] ‐P 3306 连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份， 这个时候用的就是你输入的用户名和密码。 1、如果用户名或密码不对，你就会收到一个”Access denied for user”的错误，然后客户端程序结束执行。 2、如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权 限。 这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权 限。修改完成后，只有再新建的连接才会使用新的权限设置。用户的权限表在系统表空间的mysql的user表中。 修改user密码 123456789mysql&gt; CREATE USER &#x27;username&#x27;@&#x27;host&#x27; IDENTIFIED BY &#x27;password&#x27;; //创建新用户 mysql&gt; grant all privileges on *.* to &#x27;username&#x27;@&#x27;%&#x27;; //赋权限,%表示所有(host) mysql&gt; flush privileges //刷新数据库 mysql&gt; update user set password=password(”123456″) where user=’root’;(设置用户名密码) mysql&gt; show grants for root@&quot;%&quot;; 查看当前用户的权限 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。文本中这个 图是 show processlist 的结果，其中的 Command 列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。 客户端如果长时间不发送command到Server端，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值 是 8 小时。 查看wait_timeout 123mysql&gt; show global variables like &quot;wait_timeout&quot;; mysql&gt;set global wait_timeout=28800; 设置全局服务器关闭非交互连接之前等待活动的秒数 如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次 查询就断开连接，下次查询再重新建立一个。 开发当中我们大多数时候用的都是长连接,把连接放在Pool内进行管理，但是长连接有些时候会导致 MySQL 占用内存涨得特别 快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如 果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。 怎么解决这类问题呢？ 1、定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 2、如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资 源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 三、查询缓存常用的一些操作 1234567mysql&gt;show databases; 显示所有数据库 mysql&gt;use dbname； 打开数据库： mysql&gt;show tables; 显示数据库mysql中所有的表； mysql&gt;describe user; 显示表mysql数据库中user表的列信息）； 连接建立完成后，你就可以执行 select 语句了。执行逻辑就会来到第二步：查询缓存。 MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找 到 key，那么这个 value 就会被直接返回给客户端。 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查 询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。 大多数情况查询缓存就是个鸡肋，为什么呢？ 因为查询缓存往往弊大于利。查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。 因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率 会非常低。 一般建议大家在静态表里使用查询缓存，什么叫静态表呢？就是一般我们极少更新的表。比如，一个系统配置表、字典 表，那这张表上的查询才适合使用查询缓存。好在 MySQL 也提供了这种“按需使用”的方式。你可以将my.cnf参数 query_cache_type 设置成 DEMAND。 1234567 my.cnf #query_cache_type有3个值 0代表关闭查询缓存OFF，1代表开启ON，2（DEMAND）代表当sql语句中有SQL_CACHE 关键词时才缓存 query_cache_type=2 这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，像下 面这个语句一样： 1mysql&gt; select SQL_CACHE * from test where ID=5; 查看当前mysql实例是否开启缓存机制 1mysql&gt; show global variables like &quot;%query_cache_type%&quot;; 监控查询缓存的命中率: 1mysql&gt; show status like&#x27;%Qcache%&#x27;; //查看运行的缓存信息 Qcache_free_blocks:表示查询缓存中目前还有多少剩余的blocks，如果该值显示较大，则说明查询缓存中的内存碎片 过多了，可能在一定的时间进行整理。 Qcache_free_memory:查询缓存的内存大小，通过这个参数可以很清晰的知道当前系统的查询内存是否够用，是多 了，还是不够用，DBA可以根据实际情况做出调整。 Qcache_hits:表示有多少次命中缓存。我们主要可以通过该值来验证我们的查询缓存的效果。数字越大，缓存效果越 理想。 Qcache_inserts: 表示多少次未命中然后插入，意思是新来的SQL请求在缓存中未找到，不得不执行查询处理，执行 查询处理后把结果insert到查询缓存中。这样的情况的次数，次数越多，表示查询缓存应用到的比较少，效果也就不理 想。当然系统刚启动后，查询缓存是空的，这很正常。 Qcache_lowmem_prunes:该参数记录有多少条查询因为内存不足而被移除出查询缓存。通过这个值，用户可以适当的 调整缓存大小。 Qcache_not_cached: 表示因为query_cache_type的设置而没有被缓存的查询数量。 Qcache_queries_in_cache:当前缓存中缓存的查询数量。 Qcache_total_blocks:当前缓存的block数量。 mysql8.0已经移除了查询缓存功能 四、分析器如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。 分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是 什么，代表什么。 MySQL 从你输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符 串“ID”识别成“列 ID”。 做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句 是否满足 MySQL 语法。 如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句 from 写成了 “rom”。 12345 mysql&gt; select * fro test where id=1; ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds t o your MySQL server version for the right syntax to use near &#x27;fro test where id=1&#x27; at line 1 词法分析器原理 词法分析器分成6个主要步骤完成对sql语句的分析 1、词法分析 2、语法分析 3、语义分析 4、构造执行树 5、生成执行计划 6、计划的执行 下图是SQL词法分析的过程步骤： SQL语句的分析分为词法分析与语法分析，mysql的词法分析由MySQLLex[MySQL自己实现的]完成，语法分析由Bison生 成。关于语法树大家如果想要深入研究可以参考这篇wiki文章：https://en.wikipedia.org/wiki/LR_parser。那么除了Bison 外，Java当中也有开源的词法结构分析工具例如Antlr4，ANTLR从语法生成一个解析器，可以构建和遍历解析树，可以在IDEA 工具当中安装插件：antlr v4 grammar plugin。插件使用详见课程 经过bison语法分析之后，会生成一个这样的语法树 至此我们分析器的工作任务也基本圆满了。接下来进入到优化器 五、优化器经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接 顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join： 123 mysql&gt; select * from test1 join test2 using(ID) where test1.name=yangguo and test2.name=xiaol ongnv; 既可以先从表 test1 里面取出 name=yangguo的记录的 ID 值，再根据 ID 值关联到表 test2，再判断 test2 里面 name的 值是否等于 yangguo。 也可以先从表 test2 里面取出 name=xiaolongnv 的记录的 ID 值，再根据 ID 值关联到 test1，再判断 test1 里面 name 的值是否等于 yangguo。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。优化器阶段 完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。如果你还有一些疑问，比如优化器是怎么选择索引的，有 没有可能选择错等等。 六、执行器开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在 工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权 限)。 1mysql&gt; select * from test where id=1; 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 比如我们这个例子中的表 test 中，ID 字段没有索引，那么执行器的执行流程是这样的： \\1. 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； \\2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 \\3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接 口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加 的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。","categories":[{"name":"7.mysql","slug":"7-mysql","permalink":"https://zhangxin66666.github.io/categories/7-mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"}]},{"title":"面试知识点总结","slug":"12.面试/常见面试题","date":"2022-03-30T16:00:00.000Z","updated":"2024-12-23T06:46:36.103Z","comments":true,"path":"2022/03/31/12.面试/常见面试题/","link":"","permalink":"https://zhangxin66666.github.io/2022/03/31/12.%E9%9D%A2%E8%AF%95/%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"","text":"一、基础知识1.ConcurrentHashMapConcurrentHashMap 2.Set,List,Map有什么区别 结构特点 List 和 Set 是存储单列数据的集合， Map 是存储键和值这样的双列数据的集合; List中存储的数据是有顺序，并且允许重复; Map 中存储的数据是没有顺序的，其键是不能重复的，它的值是可以有重复的， Set 中存储的数据是无序的，且不允许有重复，但元素在集合中的位置由元素的hashcode决定，位置是固定的(Set集合根据 hashcode 来进行数据的存储，所以位置是固定的，但是位置不是用户可以控制的，所以对于用户来说 set 中的元素还是无序的); 3.HashMap和HashTable有什么区别？区别： HashMap方法没有synchronized修饰，线程非安全，HashTable线程安全； HashMap允许key和value为null，而HashTable不允许 4.HashMap底层实现 jdk1.7底层采用的是数组+链表实现；jdk1.8底层采用数组+链表+红黑树实现，相对于1.7提升了查询的性能 链表过度到红黑树的阈值为8，红黑树退化为链表的阈值是6，原因在源码中332行给了明确解释(Poisson distribution)泊松分布，这是数学里面类似于正态分布的一个问题，这是统计学里面的一些东西 HashMap初始化的数组长度为16(算法导论中的除法散列法讲到 K取模m得到一个对应的位置，根据具体位置插入一个数据值，假如长度是16，长度范围就是015，取模之后我的余数为015之间，而算法导论中指出，一个不太接近2的整数幂的素数（质数：只能被1和它本身整出的数），常常是m的最好选择。但是hashMap中未采用素数，而是采用2的n次方(合数)作为数组长度)，原因为：1、更便于位置计算；2、方便做数据迁移。 为了保证存入的数据均匀的散列分布在数组中，需要设计良好的hash散列算法(hashMap中采用扰动函数来做的：移位和异或相关操作ConcurrentHashMap源码中684行spread方法，即：让int的高16位异或它本身)，尽可能的避免hash冲突(概率问题)，具体落到数组哪个节点并非是通过取模运算得出，而是通过与上数组长度-1即可(只需要int类型数32位的后四位参与运算)，即可得到0~15之间的数，因为取模运算效率远低于位运算，所以这也是hashMap要求底层数组长度必须为2的n次方的原因之一（ConcurrentHashMap源码中514行规定）。 5.HashMap put流程 HashMap/ConcurrentHashMap 并不是通过构造函数创建默认空间的，而是通过put数据的时候获取到对应的数据空间的，如果数组长度是0，则通过CAS操作后初始化数组，如果有线程正在做初始化数组操作，其他线程则让出时间片 6.HashMap扩容流程 hashMap规定了当我的数组快放满的时候就要开始扩容了，什么时候算是快放满？HashMap是通过扩容因子来规定的 HashMap规定扩容因子是0.75，如果默认长度是16，也就是说当HashMap底层数组的容量达到12的时候进行扩容操作。0.75则是根据统计学得来的。private static final float LOAD_FACTOR = 0.75f; 扩容首先要创建新的数组（原来大小左移1位）ConcurrentHaspMap源码2367行开始扩容操作 转移旧数据到新的数组中去（怎么计算扩容后数据下标位置？）原来：h&amp;(n-1)计算下标位置，扩容后 h$(31)，原来占用4个二进制位，扩容后占用5个二进制位，所以只需要看第五位即可，如果第五位是0，扩容后的的下标跟原位置一样，如果是1，新下标位置在原数组的位置的基础上加上原来数组长度即可，这也是数组长度采用2的N次方扩容的第二个原因 假设原来长度是128，扩容后是256，整体扩容方式是通过多线程方式运行的，但是要保证数据不能乱，将原来数组分段，规定每个线程最少负责16个桶的迁移工作，8个线程可以并行执行，如果小于16个桶，直接单线程执行 7.为什么选择用红黑树二叉树在极端情况下会退化为链表，查询时间复杂度跟链表相似，而AVL树，SB树，红黑树，都属于平衡二叉树，尽量保持左子树和右子树的高度差不要相差太大，而这三种树的差别就在于左树和右树的高度规则不同。 AVL树：严格意义平衡树，的要求左子树和右子树的高度差不能超过1，损失了部分插入性能，带来了高效的查询 SB树： 红黑树：要求最长子树不能超过最短子树的2倍即可，损失了部分查询性能，使得查询效率高于链表的同时，相比于其他树提升了插入性能，尽量做到了插入和查询的一个平衡点，而HashMap则是查多，插入少(这里的插入指的是产生Hash冲突下的插入)，所以红黑树更适合HashMap来做底层的存储结构。 8.面向对象的特征?面向对象的特征:封装、继承、多态、抽象。 封装:就是把对象的属性和行为(数据)结合为一个独立的整体，并尽可能隐藏对象的内 部实现细节，就是把不想告诉或者不该告诉别人的东西隐藏起来，把可以告诉别人的公开，别人只能用我提供的功能实现需求，而不知道是如何实现的。增加安全性。 继承:子类继承父类的数据属性和行为，并能根据自己的需求扩展出新的行为，提高了代 码的复用性。 多态:指允许不同的对象对同一消息做出相应。即同一消息可以根据发送对象的不同而采 用多种不同的行为方式(发送消息就是函数调用)。封装和继承几乎都是为多态而准备 的，在执行期间判断引用对象的实际类型，根据其实际的类型调用其相应的方法。 抽象:表示对问题领域进行分析、设计中得出的抽象的概念，是对一系列看上去不同，但是本质上相同的具体概念的抽象。在 Java 中抽象用 abstract 关键字来修饰，用abstract修饰类时，此类就不能被实例化，从这里可以看出，抽象类(接口)就是为了继承而存在的。 9.常见的RuntimeException java.lang.NullPointerException 空指针异常;出现原因:调用了未经初始化的对象或者是不存在 的对象。 java.lang.ClassNotFoundException 指定的类找不到;出现原因:类的名称和路径加载错误;通常 都是程序试图通过字符串来加载某个类时可能引发异常。 java.lang.NumberFormatException 字符串转换为数字异常;出现原因:字符型数据中包含非数 字型字符。 java.lang.IndexOutOfBoundsException 数组角标越界异常，常见于操作数组对象时发生。 java.lang.IllegalArgumentException 方法传递参数错误。 java.lang.ClassCastException 数据类型转换异常。 java.lang.NoClassDefFoundException 未找到类定义错误。 SQLException SQL 异常，常见于操作数据库时的 SQL 语句错误。 java.lang.InstantiationException实例化异常。 java.lang.NoSuchMethodException方法不存在异常。 10.常见的引用类型java的引用类型一般分为四种：强引用、软引用、弱引用、虚引用 强引用：普通的变量引用 软引用：将对象用SoftReference软引用类型的对象包裹，正常情况不会被回收，但是GC做完后发现释放不出空间存放新的对象，则会把这些软引用的对象回收掉。软引用可用来实现内存敏感的高速缓存。 弱引用：将对象用WeakReference软引用类型的对象包裹，弱引用跟没引用差不多，GC会直接回收掉，很少用 虚引用：虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系，几乎不用 四、缓存:Redis1.redis基本数据类型及常用场景 2.redis数据类型底层数据结构 3.redis单线程为什么快 redis的速度非常的快，单机的redis就可以支撑每秒10几万的并发，相对于mysql来说，性能是mysql的几十倍。速度快的原因主要有几点： 完全基于内存操作 C语言实现，优化过的数据结构，基于几种基础的数据结构，redis做了大量的优化，性能极高 使用单线程，无上下文的切换成本 基于非阻塞的IO多路复用机制 4.redis单线程模型 5.那为什么Redis6.0之后又改用多线程呢? redis使用多线程并非是完全摒弃单线程，redis还是使用单线程模型来处理客户端的请求，只是使用多线程来处理数据的读写和协议解析，执行命令还是使用单线程。 这样做的目的是因为redis的性能瓶颈在于网络IO而非CPU，使用多线程能提升IO读写的效率，从而整体提高redis的性能。 6.缓存雪崩、缓存击穿、缓存穿透及解决方案 缓存雪崩：缓存同一时间大面积失效，后面请求都落到数据库上，造成大量请求直接打到数据库。 过期时间随机，防止同一时间大量数据过期 缓存预热：项目启动加载缓存到redis 互斥锁：修改的时候只允许一个线程修改数据库，其他读写线程等待 加相应缓存标记，记录缓存是否失效，如果失效，更新缓存(内存消耗大，一般不用) 缓存穿透：指数据库没有数据，导致请求落到数据库上 接口层增加校验，对id进行规则拦截 缓存取不到数据，设置null值到缓存，设置短时间超时，防止网络攻击 布隆过滤器，所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据就会被这个bitmap拦截 缓存击穿：缓存没有，数据库中有数据（一般是缓存时间到期），并发用户特别多，同时大量请求落到数据库，缓存击穿指并发查询同一条数据，缓存雪崩是不同数据都过期 key不过期 加互斥锁 7.大key 8.热key 9.Redis的过期策略有哪些 10.redis淘汰策略？（同问题定期+惰性都没有删除过期的key怎么办？） 11.redis分布式锁、zk分布式锁（普通、有序两种）、redission分布式锁 12.redis持久化两种方式（AOF&amp;RDB） RDB的原理是什么 给出两个词汇就可以了，fork和cow。fork是指redis通过创建子进程来进行RDB操作，cow指的是copy on write，子进程创建后，父子进程共享数据段，父进程继续提供读写服务，写脏的页面数据会逐渐和子进程分离开来。 注：回答这个问题的时候，如果你还能说出AOF和RDB的优缺点，我觉得我是面试官在这个问题上我会给你点赞，两者其实区别还是很大的 13.redis主从复制原理（全量同步&amp;增量同步） 14.redis哨兵模式 15.redis集群模式 16.一致性hash 17.redis和数据库数据一致性问题 18.BloomFilter过滤器原理 19.场景题 排行榜（多维度） 签到 redis实现消息队列 一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。 如果对方追问可不可以不用sleep呢？ list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。 如果对方接着追问能不能生产一次消费多次呢？ 使用pub/sub主题订阅者模式，可以实现 1:N 的消息队列。 如果对方继续追问 pub/su b有什么缺点？ 在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如RocketMQ等。 如果对方究极TM追问Redis如何实现延时队列？ 使用sortedset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。 Redis单线程模型 Redis基于Reactor模式开发的网络事件处理器，这个文件事件处理器是单线程的，采用IO多路复用机制监听多个Socker，根据Socker上的事件类型选择对应的事件处理器处理这个事件，可以实现高性能的网络通信。文件事件处理器包含4个部分，多个Socker，IO多路复用程序，文件事件分派器和事件处理器(命令请求处理器，命令回复处理器，链接应答处理器)，多个Socket可能并发产生不同操作，每个操作对应不同的文件事件，但是IO多路复用会监听多个Socket，会将Socket放入一个队列，每次从队列中取出一个Socket给时间分派器，时间分派器把Socket给对应的事件处理器。 单线程快的原因：1.纯内存操作；2.核心是基于非阻塞的IO多路复用机制；3.单线程反而避免多线程的频繁上下文切换 分布式锁怎么实现的？ 基于数据库实现分布式锁：唯一索引，状态机唯一联合索引 基于缓存（Redis等）实现分布式锁：SET key value NX PX 30000 基于Zookeeper实现分布式锁； setnx用到的参数SET key value NX PX 30000 第三个参数：把key、value set到redis中的策略 nx ： not exists, 只有key 不存在时才把key value set 到redis xx ： is exists ，只有 key 存在是，才把key value set 到redis 第四个参数：过期时间单位 ex ：seconds 秒 px : milliseconds 毫秒 使用其他值，抛出 异常 ： redis.clients.jedis.exceptions.JedisDataException : ERR syntax error 第五个参数：有两种可选的值， int 和long 的time，都是过期时间 ，expx 参数是px的时候，使用long类型的参数，可以表示更多时间 缓存穿透，击穿，雪崩 Redis过期键删除策略Redis对于过期键有三种清除策略： 被动删除：当读/写一个已经过期的key时，会触发惰性删除策略，直接删除掉这个过期key 主动删除：由于惰性删除策略无法保证冷数据被及时删掉，所以Redis会定期主动淘汰一批已过期的key 当前已用内存超过maxmemory限定时，触发主动清理策略 主动清理策略在Redis 4.0 之前一共实现了 6 种内存淘汰策略，在 4.0 之后，又增加了 2 种策略，总共8种： a) 针对设置了过期时间的key做处理： volatile-ttl：在筛选时，会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除。 volatile-random：就像它的名称一样，在设置了过期时间的键值对中，进行随机删除。 volatile-lru：会使用 LRU 算法筛选设置了过期时间的键值对删除。 volatile-lfu：会使用 LFU 算法筛选设置了过期时间的键值对删除。 b) 针对所有的key做处理： allkeys-random：从所有键值对中随机选择并删除数据。 allkeys-lru：使用 LRU 算法在所有数据中进行筛选删除。 allkeys-lfu：使用 LFU 算法在所有数据中进行筛选删除。 c) 不处理： noeviction：不会剔除任何数据，拒绝所有写入操作并返回客户端错误信息”(error) OOM command not allowed when used memory”，此时Redis只响应读操作。 LRU 算法（Least Recently Used，最近最少使用） 淘汰很久没被访问过的数据，以最近一次访问时间作为参考。 LFU 算法（Least Frequently Used，最不经常使用） 淘汰最近一段时间被访问次数最少的数据，以次数作为参考。 当存在热点数据时，LRU的效率很好，但偶发性的、周期性的批量操作会导致LRU命中率急剧下降，缓存污染情况比较严重。这时使用LFU可能更好点。根据自身业务类型，配置好maxmemory-policy(默认是noeviction)，推荐使用volatile-lru。如果不设置最大内存，当 Redis 内存超出物理内存限制时，内存的数据会开始和磁盘产生频繁的交换 (swap)，会让 Redis 的性能急剧下降。当Redis运行在主从模式时，只有主结点才会执行过期删除策略，然后把删除操作”del key”同步到从结点删除数据。 Redis持久化 RDB快照(snapshot) Redis 将内存数据快照保存在名字为 dump.rdb 的二进制文件中。可以对 Redis 进行设置， 让它在“ N 秒内数据集至少有 M 个改动”这一条件被满足时， 自动保存一次数据；**# save 60 1000 //** (60秒改1000次进行一次RDB持久化)关闭RDB只需要将所有的save保存策略注释掉即可。 问题：会阻塞客户端命令。 还可以手动执行命令生成RDB快照，进入redis客户端执行命令save或bgsave可以生成dump.rdb文件，每次命令执行都会将所有redis内存快照到一个新的rdb文件里，并覆盖原有rdb快照文件。 bgsave的写时复制(COW)机制：Redis 借助操作系统提供的写时复制技术（Copy-On-Write, COW），在生成快照的同时，依然可以正常处理写命令。bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。此时，如果主线程对这些数据也都是读操作，那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据，那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，在这个过程中，主线程仍然可以修改原来的数据。 AOF(append-only file) 如果 Redis 因为某些原因而造成故障停机， 那么服务器将丢失最近写入、且仍未保存到快照中的那些数据。从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化，将修改的每一条指令记录进文件appendonly.aof中(先写入os cache，每隔一段时间fsync到磁盘) 缓存与数据库数据一致性 解决方案： 对于并发几率很小的数据(如个人维度的订单数据、用户数据等)，这种几乎不用考虑这个问题，很少会发生缓存不一致，可以给缓存数据加上过期时间，每隔一段时间触发读的主动更新即可。 就算并发很高，如果业务上能容忍短时间的缓存数据不一致(如商品名称，商品分类菜单等)，缓存加上过期时间依然可以解决大部分业务对于缓存的要求。 如果不能容忍缓存数据不一致，可以通过加读写锁保证并发读写或写写的时候按顺序排好队，读读的时候相当于无锁。 也可以用阿里开源的canal通过监听数据库的binlog日志及时的去修改缓存，但是引入了新的中间件，增加了系统的复杂度。 五、MQ消息队列 Rabbitmq生产高可用怎么实现的 生产部署为集群模式：避免单机模式下MQ服务器挂了导致服务不可用，还可以承载更高的并发量，但是集群模式有个问题，就是在哪个节点上创建队列，该队列只会存在该节点上，其他集群节点不会备份该队列，一旦该节点宕机，该队列中的消息就会丢失(亲测设置持久化也丢失) 使用镜像队列：可以解决集群模式下，每个机器上只有一个队列的问题，让消息在其他节点再备份一份。开启方式：任何节点添加policy策略即可，该集群就具有镜像队列能力，可设置备份的份数和备份队列规则，即使其中一个节点宕机，也会保证整个集群中保存2份 高可用负载均衡：haproxy+keepalive，nginx，lvs等实现高可用负载均衡 其他：Federation Exchange联邦交换机插件解决两地接受消息确认消息延迟问题，也可以用Shovel来做数据同步，需要安装插件 六、多线程死锁 死锁：是指一组互相竞争资源的线程，因为互相等待，导致永久阻塞的现象。 原因：必须同时满足以下四个条件 共享互斥条件：共享资源x和y只能被一个线程占用 占有且等待：线程t1已经占有共享资源x，在等待共享资源y的时候不释放共享资源x 不可抢占：其他线程不能强行抢占线程t1占有的资源 循环等待：线程t1等待线程t2占有的资源，线程t2等待线程t1占有的资源 如何避免死锁： 既然产生死锁必然满足四个条件，那我们只要打破四个条件中的一个就可以避免，第一个互斥条件是无法被破坏的，因为锁本身就是通过互斥来解决线程安全的 针对后三个条件，我们逐一分析，占有且等待这个条件我们可以一次性申请所有资源，不存在等待； 不可抢占这个条件：占有部分资源的线程进一步申请其他资源的时候如果申请不到，可以主动释放已占有的资源，这样不可抢占这条件就破坏了； 循环等待这个条件：可以按照顺序去申请资源进行预防，就是说资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，线性化之后，就不存在循环等待了 sleep和wait区别 sleep 是 Thread 类的静态本地方法，wait 则是 Object 类的本地方法 sleep方法不会释放锁，但是wait会释放，而且会加入到等待队列中 sleep方法不依赖于同步器synchronized，但是wait需要依赖synchronized关键字 sleep不需要被唤醒（休眠之后推出阻塞），但是wait需要（不指定时间需要被别人中断） sleep 一般用于当前线程休眠，或者轮循暂停操作，wait 则多用于多线程之间的通信 sleep 会让出 CPU 执行时间且强制上下文切换，而 wait 则不一定，wait 后可能还是有机会重新竞争到锁继续执行的。 sleep就是把cpu的执行资格和执行权释放出去，不再运行此线程，当定时时间结束再取回cpu资源，参与cpu的调度，获取到cpu资源后就可以继续运行了。而如果sleep时该线程有锁，那么sleep不会释放这个锁，而是把锁带着进入了冻结状态，也就是说其他需要这个锁的线程根本不可能获取到这个锁。也就是说无法执行程序。如果在睡眠期间其他线程调用了这个线程的interrupt方法，那么这个线程也会抛出interruptexception异常返回，这点和wait是一样的。 当我们调用wait（）方法后，线程会放到等待池当中，等待池的线程是不会去竞争同步锁。只有调用了notify（）或notifyAll()后等待池的线程才会开始去竞争锁，notify（）是随机从等待池选出一个线程放到锁池，而notifyAll()是将等待池的所有线程放到锁池当中 Volitile作用 可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。内存屏障-&gt;汇编Lock关键字-&gt;缓存一致性协议 原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性 有序性：对volatile修饰的变量的读写操作前后加上各种特定的内存屏障来禁止指令重排序来保障有序性 Volitile如何保证可见性 缓存一致性：在共享内存多处理器系统中，每个处理器都有一个单独的缓存内存，共享数据在主内存和各处理器私有缓存同时存在。当数据的一个副本发生更改，其他副本需要感知到最新修改数据。缓存一致性的手段主要有： 总线锁定：总线锁定就是使用处理器提供的一个 LOCK＃信号，当其中一个处理器在总线上输出此信号时，其它处理器的请求将被阻塞，那么该处理器可以独占共享内存。 总线窥探(Bus snooping)：是缓存中的一致性控制器窥探总线事务的一种方案（该方案由Ravishankar和Goodman于1983年提出）当数据被多个缓存共享时，一个处理器修改了共享数据的值，可以是刷新缓存块或使缓存块失效还可以通过缓存块状态的改变，来达到缓存一致性的目的，主要取决于窥探协议类型： 写失效（Write-invalidate） ：当一个处理器写入共享缓存时，其他缓存中的所有共享副本都会通过总线窥探失效。确保处理器只能读写一个数据副本，其他缓存中的所有其他副本都无效。这是最常用的窥探协议。MSI、MESI、MOSI、MOESI和MESIF协议属于该类型。 写更新（Write-update）：当处理器写入一个共享缓存块时，其他缓存的所有共享副本都会通过总线窥探更新。这个方法将写数据广播到总线上的所有缓存中。它比写失效协议引起更大的总线流量，因此这种方法不常见。Dragon和firefly协议属于此类别。 指令重排序Java语言规范规定JVM线程内部维持顺序化语义，即只要程序的最终结果与它顺序化情况的结果相等，那么指令的执行顺序可以与代码顺序不一致，此过程叫指令的重排序。指令重排序的意义：：JVM能根据处理器特性（CPU多级缓存系统、多核处理器等）适当的对机器指令进行重排序，使机器指令能更符合CPU的执行特性，最大限度的发挥机器性能。 Java线程的生命周期 NEW（初始化状态） RUNNABLE（可运行状态+运行状态） BLOCKED（阻塞状态） WAITING（无时限等待） TIMED_WAITING（有时限等待） TERMINATED（终止状态） 在操作系统层面，有五种状态，Java 线程中的 BLOCKED、WAITING、TIMED_WAITING 是一种状态，即前面我们提到的休眠状态。也就是说只要 Java 线程处于这三种状态之一，那么这个线程就永远没有 CPU 的使用权。 linux查看线程命令 ps - fe 查看所有进程 ps - fT - p 查看某个进程（ PID ）的所有线程 kill 杀死进程 top 按大写 H 切换是否显示线程 top - H - p 查看某个进程（ PID ）的所有线程 jps 命令查看所有 Java 进程 jstack 查看某个 Java 进程（ PID ）的所有线程状态 jconsole 来查看某个 Java 进程中线程的运行情况（图形界面） Java线程的实现方式 方式1：使用 Thread类或继承Thread类 实现 Runnable 接口配合Thread 使用有返回值的 Callable，借助线程池使用 使用 lambda 本质上Java中实现线程只有一种方式，都是通过new Thread()创建线程，调用Thread#start启动线程最终都会调用Thread#run方法 Thread常用方法sleep方法 调用 sleep 会让当前线程从 Running 进入TIMED_WAITING状态，不会释放对象锁 其它线程可以使用 interrupt 方法打断正在睡眠的线程，这时 sleep 方法会抛出InterruptedException，并且会清除中断标志 睡眠结束后的线程未必会立刻得到执行 sleep当传入参数为0时，和yield相同 yield方法 yield会释放CPU资源，让当前线程从 Running 进入 Runnable状态，让优先级更高（至少是相同）的线程获得执行机会，不会释放对象锁； 假设当前进程只有main线程，当调用yield之后，main线程会继续运行，因为没有比它优先级更高的线程； 具体的实现依赖于操作系统的任务调度器 ThreadLocal的实现原理ThreadLocal的实现原理，每一个Thread维护一个ThreadLocalMap，key为使用弱引用的ThreadLocal实例，value为线程变量的副本 ThreadLocal应用场景 在进行对象跨层传递的时候，使用ThreadLocal可以避免多次传递，打破层次间的约束。 线程间数据隔离 进行事务操作，用于存储线程事务信息。 数据库连接，Session会话管理。 Spring框架在事务开始时会给当前线程绑定一个Jdbc Connection,在整个事务过程都是使用该线程绑定的connection来执行数据库操作，实现了事务的隔离性。Spring框架里面就是用的ThreadLocal来实现这种隔离 ThreadLocal内存泄露原因，如何避免 强引用：使用最普遍的引用(new)，一个对象具有强引用，不会被垃圾回收器回收。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不回收这种对象。 弱引用：JVM进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。在java中，用java.lang.ref.WeakReference类来表示。可以在缓存中使用弱引用。 ThreadLocal正确的使用方法 每次使用完ThreadLocal都调用它的remove()方法清除数据 将ThreadLocal变量定义成private static，这样就一直存在ThreadLocal的强引用，也就能保证任何时候都能通过ThreadLocal的弱引用访问到Entry的value值，进而清除掉 。 synchronized和ReentranLock区别？synchronized是jvm层面的关键字，底层是通过monitor对象来完成，monitor对象底层依赖于操作系统的Mutex互斥量，底层调用的是操作系统的Pthread库，是由操作系统来维护的，而操作系统分为用户空间和内核空间的，JVM运行在用户空间，调用底层操作系统cpu会进行一轮状态切换，这个状态切换是比较重型操作，wait/notify等方法也依赖monitor对象只有在同步块或方法中才能调wait/notify等方法。 Lock是具体类（java.util.concurrent.Locks.Lock)是api层面的锁，是从jdk1.5开始引入的，那个时候synchronized性能还比较差，ReentranLock的出现是为了解决当时性能差的问题 使用方法：synchronized不需要手动释放锁，ReentranLock则需要用户手动释放锁(需要lock()和unlock()配合try/finally使用) 等待是否可中断：synchronized不可中断，除非抛出异常或者执行完成，ReentranLock可中断 设置超时方法tryLock(long timeout, timeUnit unit) lockInterruptibly()放代码块忠，调用interrupt()方法可中断 加锁是否公平：synchronized非公平锁；ReentranLock 两者都可以，默认非公平锁，构造方法传入true为公平锁，false为非公平锁 锁绑定多个条件Condition：synchronized 没有；ReentranLock 用来实现分组唤醒需要唤醒的线程，可以精确唤醒，不像synchronized要么随机唤醒一个线程，要么全部唤醒 线程池参数有，核心线程配置，原因？ corePoolSize：线程池中的核心线程数 maximumPoolSize：线程池中允许的最大线程数。如果当前阻塞队列满了，且继续提交任务，则创建新的线程执行任务 keepAliveTime：核心线程外的线程存活超时时间 unit：时间单位 workQueue：用来保存等待被执行的任务的阻塞队列，且任务必须实现Runable接口 threadFactory：用来创建新线程 线程池的饱和策略：( AbortPolicy：直接抛出异常，默认策略；CallerRunsPolicy：用调用者所在的线程来执行任务；DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务；DiscardPolicy：直接丢弃任务) CPU密集型（CPU-bound） CPU密集型也叫计算密集型，在多重程序系统中，大部份时间用来做计算、逻辑判断等CPU动作的程序称之CPU bound。例如一个计算圆周率至小数点一千位以下的程序，在执行的过程当中绝大部份时间用在三角函数和开根号的计算，便是属于CPU bound的程序。 线程数 = CPU核数+1 (现代CPU支持超线程) IO密集型（I/O bound） IO密集型指的是系统的CPU性能相对硬盘、内存要好很多，此时，系统运作，大部分的状况是CPU在等I/O (硬盘/内存) 的读/写操作 线程数 = （（线程等待时间+线程CPU时间）/线程CPU时间 ）* CPU数目 七、springspring是一个框架，更像是一个生态环境，在我们的开发流程中，所有的框架基本上都依赖于spring，spring起到了一个IOC容器的作用，用来承载我们整体的bean对象，它帮我们处理了bean对象从创建到销毁的整个生命周期的管理，我们在使用spring的时候，可以使用配置文件，也可以使用注解的方式进行相关开发。spring框架的工作主要流程是，当我们程序开始启动之后，spring把注解或者配置文件定义好的bean对象转化成为BeanDefinitionran，然后通BeanFactoryPostProcessor完成整个的BeanDefinitionran的解析和加载过程，然后根据BeanDefinitionran通过反射的方式创建bean对象，然后进行对象初始化，包括：aware接口相关操作，BeanPostProcessor操作，Init-methord操作完成整个bean的创建过程，之后我们就可以使用了。 Bean的初始化过程 Bean的生命周期 循环依赖AOP的顺序 spring4和spring5是不一样的 springMVC处理请求流程 spring事务的传播机制 TransactionDefinition.PROPAGATION_REQUIRED：支持当前事务，如果没有事务会创建一个新的事务 TransactionDefinition.PROPAGATION_SUPPORTS：支持当前事务，如果没有事务的话以非事务方式执行 TransactionDefinition.PROPAGATION_MANDATORY：支持当前事务，如果没有事务抛出异常 TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务并挂起当前事务 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式执行，如果当前存在事务则将当前事务挂起 TransactionDefinition.PROPAGATION_NEVER：以非事务方式进行，如果存在事务则抛出异常 TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。 八、网络网络七层模型 第一层：物理层 第二层：数据链路层 802.2、802.3ATM、HDLC、FRAME RELAY 第三层：网络层 IP、IPX、ARP、APPLETALK、ICMP 第四层：传输层 TCP、UDP、SPX 第五层：会话层 RPC、SQL、NFS 、X WINDOWS、ASP 第六层：表示层 ASCLL、PICT、TIFF、JPEG、 MIDI、MPEG 第七层：应用层 HTTP,FTP,SNMP等 物理层：物理层负责最后将信息编码成电流脉冲或其它信号用于网上传输； 数据链路层：数据链路层通过物理网络链路􏰁供数据传输。不同的数据链路层定义了不同的网络和协 议特征,其中包括物理编址、网络拓扑结构、错误校验、数据帧序列以及流控。可以简单的理解为：规定了0和1的分包形式，确定了网络数据包的形式； 网络层：网络层负责在源和终点之间建立连接。可以理解为，此处需要确定计算机的位置，怎么确定？IPv4，IPv6！ 传输层：传输层向高层提供可靠的端到端的网络数据流服务。可以理解为：每一个应用程序都会在网卡注册一个端口号，该层就是端口与端口的通信！常用的（TCP／IP）协议； 会话层：会话层建立、管理和终止表示层与实体之间的通信会话；建立一个连接（自动的手机信息、自动的网络寻址）; 表示层：表示层提供多种功能用于应用层数据编码和转化,以确保以一个系统应用层发送的信息 可以被另一个系统应用层识别;可以理解为：解决不同系统之间的通信，eg：Linux下的QQ和Windows下的QQ可以通信； 应用层：OSI 的应用层协议包括文件的传输、访问及管理协议(FTAM) ,以及文件虚拟终端协议(VIP)和公用管理系统信息(CMIP)等; http1.0和http1.1区别HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的HTTP协议。 主要区别主要体现在： 缓存处理，在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用，HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 错误通知的管理，在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。 Host头处理，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。 长连接，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。 HTTPS与HTTP的一些区别 HTTPS协议需要到CA申请证书，一般免费证书很少，需要交费。 HTTP协议运行在TCP之上，所有传输的内容都是明文，HTTPS运行在SSL/TLS之上，SSL/TLS运行在TCP之上，所有传输的内容都经过加密的。 HTTP和HTTPS使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 HTTPS可以有效的防止运营商劫持，解决了防劫持的一个大问题。 http三次握手四次挥手其实这么说是不太准确的，应该说是tcp协议建立连接要3次握手，断开连接要4次挥手，而http是基于tcp协议的，所以通常我们也这么说，tcp可以提供全双工的数据流传输服务 三次握手 TCP服务器进程先创建传输控制块TCB，时刻准备接受客户进程的连接请求，此时服务器就进入了LISTEN（监听）状态； TCP客户进程也是先创建传输控制块TCB，然后向服务器发出连接请求报文，这是报文首部中的同部位SYN=1，同时选择一个初始序列号 seq=x ，此时，TCP客户端进程进入了 SYN-SENT（同步已发送状态）状态。TCP规定，SYN报文段（SYN=1的报文段）不能携带数据，但需要消耗掉一个序号。 TCP服务器收到请求报文后，如果同意连接，则发出确认报文。确认报文中应该 ACK=1，SYN=1，确认号是ack=x+1，同时也要为自己初始化一个序列号 seq=y，此时，TCP服务器进程进入了SYN-RCVD（同步收到）状态。这个报文也不能携带数据，但是同样要消耗一个序号。 TCP客户进程收到确认后，还要向服务器给出确认。确认报文的ACK=1，ack=y+1，自己的序列号seq=x+1，此时，TCP连接建立，客户端进入ESTABLISHED（已建立连接）状态。TCP规定，ACK报文段可以携带数据，但是如果不携带数据则不消耗序号。 当服务器收到客户端的确认后也进入ESTABLISHED状态，此后双方就可以开始通信了。 四次挥手九、设计模式十、Dubbo你知道的json序列化方式？ 谷歌的Gson json-smart：号称是速度最快的JSON解析器 Common Lang3(3.1)的SerializationUtils 阿里巴巴的 FastJson、以及 Jackson 两个系统之间怎么交互的？ 套接字（socket）：这是一种更为一般得进程间通信机制，它可用于网络中不同机器之间的进程间通信，应用非常广泛。 可以通过RPC框架（dubbo、feign等）进行远程调用 也可以引入消息队列等消息中间件作为系统之间信息交互的桥梁 共享内存（shared memory）：可以说这是最有用的进程间通信方式。它使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据得更 dubbo的序列化？dubbo 支持 hession、Java 二进制序列化、json、SOAP 文本序列化多种序列化协议。 hessian 是其默认的序列化协议 dubbo 提供者(Provider)启动时，会向注册中心写入自己的元数据信息(调用方式)。 消费者(Consumer)启动时，也会在注册中心写入自己的元数据信息，并且订阅服务提供者，路由和配置元数据的信息。 服务治理中心(duubo-admin)启动时，会同时订阅所有消费者，提供者，路由和配置元数据的信息。 当提供者离开或者新提供者加入时，注册中心发现变化会通知消费者和服务治理中心。 十一、NettyNIO三大核心组件 Channel(通道)， Buffer(缓冲区)，Selector(多路复用器) channel 类似于流，每个 channel 对应一个 buffer缓冲区，buffer 底层就是个数组 channel 会注册到 selector 上，由 selector 根据 channel 读写事件的发生将其交由某个空闲的线程处理 NIO 的 Buffer 和 channel 都是既可以读也可以写 Netty线程模型Netty通过Reactor模型基于多路复用器接收并处理用户请求，内部实现boss和work两个线程池，其中boss的线程负责处理请求的accept事件，当接收到accept事件的请求时，把对应的socket封装到一个NioSocketChannel中，并交给work线程池，其中work线程池负责请求的read和write事件，交由对应的Handler处理。 Netty 抽象出两组线程池BossGroup和WorkerGroup，BossGroup专门负责接收客户端的连接, WorkerGroup专门负责网络的读写 BossGroup和WorkerGroup类型都是NioEventLoopGroup NioEventLoopGroup 相当于一个事件循环线程组, 这个组中含有多个事件循环线程 ， 每一个事件循环线程是NioEventLoop 每个NioEventLoop都有一个selector , 用于监听注册在其上的socketChannel的网络通讯 每个BossNioEventLoop线程内部循环执行的步骤有 3 步 处理accept事件 , 与client 建立连接 , 生成 NioSocketChannel 将NioSocketChannel注册到某个worker NIOEventLoop上的selector 处理任务队列的任务 ， 即runAllTasks 每个workerNIOEventLoop线程循环执行的步骤 轮询注册到自己selector上的所有NioSocketChannel 的read, write事件 处理 I/O 事件， 即read , write 事件， 在对应NioSocketChannel 处理业务 runAllTasks处理任务队列TaskQueue的任务 ，一些耗时的业务处理一般可以放入TaskQueue中慢慢处理，这样不影响数据在 pipeline 中的流动处理 Netty模块组件 Bootstrap、ServerBootstrap：Bootstrap 意思是引导，一个 Netty 应用通常由一个 Bootstrap 开始，主要作用是配置整个 Netty 程序，串联各个组件，Netty 中 Bootstrap 类是客户端程序的启动引导类，ServerBootstrap 是服务端启动引导类。 Future、ChannelFuture：正如前面介绍，在 Netty 中所有的 IO 操作都是异步的，不能立刻得知消息是否被正确处理。但是可以过一会等它执行完成或者直接注册一个监听，具体的实现就是通过 Future 和 ChannelFutures，他们可以注册一个监听，当操作执行成功或失败时监听会自动触发注册的监听事件。 Channel：Netty 网络通信的组件，能够用于执行网络 I/O 操作。Channel 为用户提供： 当前网络连接的通道的状态（例如是否打开？是否已连接？） 网络连接的配置参数 （例如接收缓冲区大小） 提供异步的网络 I/O 操作(如建立连接，读写，绑定端口)，异步调用意味着任何 I/O 调用都将立即返回，并且不保证在调用结束时所请求的 I/O 操作已完成。 调用立即返回一个 ChannelFuture 实例，通过注册监听器到 ChannelFuture 上，可以 I/O 操作成功、失败或取消时回调通知调用方。 支持关联 I/O 操作与对应的处理程序。 不同协议、不同的阻塞类型的连接都有不同的 Channel 类型与之对应。 NioSocketChannel，异步的客户端 TCP Socket 连接。 NioServerSocketChannel，异步的服务器端 TCP Socket 连接。 NioDatagramChannel，异步的 UDP 连接。 NioSctpChannel，异步的客户端 Sctp 连接。 NioSctpServerChannel，异步的 Sctp 服务器端连接。 这些通道涵盖了 UDP 和 TCP 网络 IO 以及文件 IO。 Selector：Netty 基于 Selector 对象实现 I/O 多路复用，通过 Selector 一个线程可以监听多个连接的 Channel 事件。当向一个 Selector 中注册 Channel 后，Selector 内部的机制就可以自动不断地查询(Select) 这些注册的 Channel 是否有已就绪的 I/O 事件（例如可读，可写，网络连接完成等），这样程序就可以很简单地使用一个线程高效地管理多个 Channel 。 NioEventLoop：NioEventLoop 中维护了一个线程和任务队列，支持异步提交执行任务，线程启动时会调用 NioEventLoop 的 run 方法，执行 I/O 任务和非 I/O 任务：I/O 任务，即 selectionKey 中 ready 的事件，如 accept、connect、read、write 等，由 processSelectedKeys 方法触发。非 IO 任务，添加到 taskQueue 中的任务，如 register0、bind0 等任务，由 runAllTasks 方法触发。 NioEventLoopGroup：NioEventLoopGroup，主要管理 eventLoop 的生命周期，可以理解为一个线程池，内部维护了一组线程，每个线程(NioEventLoop)负责处理多个 Channel 上的事件，而一个 Channel 只对应于一个线程。 ChannelHandler：ChannelHandler 是一个接口，处理 I/O 事件或拦截 I/O 操作，并将其转发到其 ChannelPipeline(业务处理链)中的下一个处理程序。 ChannelHandlerContext：保存 Channel 相关的所有上下文信息，同时关联一个 ChannelHandler 对象。 ChannelPipline：保存 ChannelHandler 的 List，用于处理或拦截 Channel 的入站事件和出站操作。ChannelPipeline 实现了一种高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，以及 Channel 中各个的 ChannelHandler 如何相互交互。在 Netty 中每个 Channel 都有且仅有一个 ChannelPipeline 与之对应，它们的组成关系如下： 一个 Channel 包含了一个 ChannelPipeline，而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表，并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler。read事件(入站事件)和write事件(出站事件)在一个双向链表中，入站事件会从链表 head 往后传递到最后一个入站的 handler，出站事件会从链表 tail 往前传递到最前一个出站的 handler，两种类型的 handler 互不干扰。 Netty粘包拆包TCP是一个流协议，就是没有界限的一长串二进制数据。TCP作为传输层协议并不不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行数据包的划分，所以在业务上认为是一个完整的包，可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。面向流的通信是无消息保护边界的。 解决方案 消息定长度，传输的数据大小固定长度，例如每段的长度固定为100字节，如果不够空位补空格 在数据包尾部添加特殊分隔符，比如下划线，中划线等，这种方法简单易行，但选择分隔符的时候一定要注意每条数据的内部一定不能出现分隔符。 发送长度：发送每条数据的时候，将数据的长度一并发送，比如可以选择每条数据的前4位是数据的长度，应用层处理时可以根据长度来判断每条数据的开始和结束。 Netty提供了多个解码器，可以进行分包的操作，如下： LineBasedFrameDecoder （回车换行分包） DelimiterBasedFrameDecoder（特殊分隔符分包） FixedLengthFrameDecoder（固定长度报文来分包） 十二、zookeeperzookeeper的理解 集群管理：提供了CP模型来保证集群中每个节点的数据一致性 分布式锁 集群选举 zookeeper选举机制 LeaderElection AuthFastLeaderElection FastLeaderElection （最新默认） 目前有5台服务器，每台服务器均没有数据，它们的编号分别是1,2,3,4,5,按编号依次启动，它们的选择举过程如下： 服务器1启动，给自己投票，然后发投票信息，由于其它机器还没有启动所以它收不到反馈信息，服务器1的状态一直属于Looking(选举状态)。 服务器2启动，给自己投票，同时与之前启动的服务器1交换结果，由于服务器2的编号大所以服务器2胜出，但此时投票数没有大于半数，所以两个服务器的状态依然是LOOKING。 服务器3启动，给自己投票，同时与之前启动的服务器1,2交换信息，由于服务器3的编号最大所以服务器3胜出，此时投票数正好大于半数，所以服务器3成为领导者，服务器1,2成为小弟。 服务器4启动，给自己投票，同时与之前启动的服务器1,2,3交换信息，尽管服务器4的编号大，但之前服务器3已经胜出，所以服务器4只能成为小弟。 服务器5启动，后面的逻辑同服务器4成为小弟。 十三、理论CAP理论​ CAP 理论又叫 Brewer 理论，这是加州大学伯克利分校的埃里克 · 布鲁尔（Eric Brewer）教授，在 2000 年 7 月“ACM 分布式计算原理研讨会（PODC）”上提出的一个猜想。CAP理论原稿（那时候还只是猜想）然后到了 2002 年，麻省理工学院的赛斯 · 吉尔伯特（Seth Gilbert）和南希 · 林奇（Nancy Lynch）就以严谨的数学推理证明了这个 CAP 猜想。在这之后，CAP 理论就正式成为了分布式计算领域公认的著名定理。这个定理里，描述了一个分布式的系统中，当涉及到共享数据问题时，以下三个特性最多只能满足其中两个： 一致性（Consistency）：代表在任何时刻、任何分布式节点中，我们所看到的数据都是没有矛盾的。这与 ACID 中的 C 是相同的单词，但它们又有不同的定义（分别指 Replication 的一致性和数据库状态的一致性）。在分布式事务中，ACID 的 C 要以满足 CAP 中的 C 为前提。 可用性（Availability）：代表系统不间断地提供服务的能力。 分区容忍性（Partition Tolerance）：代表分布式环境中，当部分节点因网络原因而彼此失联（即与其他节点形成“网络分区”）时，系统仍能正确地提供服务的能力。 https://mp.weixin.qq.com/s?__biz=MzU3MTQwNDEyMg==&amp;mid=2247483680&amp;idx=1&amp;sn=a844dc83df3316ac0102ea11511b8b46&amp;chksm=fce1fb15cb9672032e98857a74f4e971a1ff833a6e353cafee655587c70a93e730a652daf5a2&amp;scene=21#wechat_redirect DDDddd不是一种架构风格，而是一种方法论，什么是方法论，每个人按照自己的想法来设计就是一套方法论；ddd是一种业务比较认可，对于微服务拆分的一种方法论。 十四、项目 最近做的比较熟悉的项目是哪个？画一下项目技术架构图 写两个类，能够实现堆内存溢出和栈内存溢出 写一个线程安全的单例。 两个可变有序链表放到新数组中，有序 ES简介ES他是建立在全文搜索引擎库ApacheLucene基础上的一个开源搜索和分析引擎，ES本身具有一个分布式存储，检索速度快的特性，所以我们经常用它去实现全文检索这一类的场景，比如像网站搜索，公司内部用ELK做日志聚集和检索，来去快速查找服务器的日志记录，去定位问题，基本上涉及到TB级别的数据场景，用ES是一个比较好的选择 ES查询快的原因 第一、Lucene是擅长管理大量的索引数据的，另一方面他会对数据进行分词以后在保存索引，这样能搞提升数据的检索效率 第二、ES采用倒排索引，所谓倒排索引，就是通过属性值来确定数据记录的位置的索引，来避免全表扫描这样一个问题， 第三、ES采用分片存储机制 第四、ES扩展性好，我们可以水平扩展增加服务器，提升ES处理性能，可以达到百台服务器节点扩展 第五、ES内部提供了数据汇总和索引生命周期管理的一些功能，可以方便我们更加高效的存储和搜索数据 使用ES注意事项 ES里面不建议使用复杂的关联查询，会对性能影响很大 避免深度分页查询，因为ES分页是支持front和size参数，在查询的时候，每个分片必须构造一个长度为front加size的优先队列，然后回传到网关节点，网关节点再对这些队列进行排序再找到正确的size文档。而当front足够大的时候容易造成OOM以及网络传输性能差的一些问题","categories":[{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"mysql面试","slug":"12.面试/mysql","date":"2022-03-30T16:00:00.000Z","updated":"2024-08-22T03:23:14.704Z","comments":true,"path":"2022/03/31/12.面试/mysql/","link":"","permalink":"https://zhangxin66666.github.io/2022/03/31/12.%E9%9D%A2%E8%AF%95/mysql/","excerpt":"","text":"1.索引底层结构&amp;行格式&amp;页格式文章内容包含索引结构，联合索引结构，聚簇索引、非聚簇索引区别，在页上如何搜索具体数据过程，一次完成的索引检索过程 地址：行页索引底层结构 2.一条 SQL 的执行过程详解 从准备更新一条数据到事务的提交的流程描述 首先执行器根据 MySQL 的执行计划来查询数据，先是从缓存池中查询数据，如果没有就会去数据库中查询，如果查询到了就将其放到缓存池中 在数据被缓存到缓存池的同时，会写入 undo log 日志文件 更新的动作是在 BufferPool 中完成的，同时会将更新后的数据添加到 redo log buffer 中 完成以后就可以提交事务，在提交的同时会做以下三件事 将redo log buffer中的数据刷入到 redo log 文件中 将本次操作记录写入到 bin log文件中 将 bin log 文件名字和更新内容在 bin log 中的位置记录到redo log中，同时在 redo log 最后添加 commit 标记 至此表示整个更新事务已经完成 为什么Mysql不能直接更新磁盘上的数据而且设置这么一套复杂的机制来执行SQL了？ 因为来一个请求就直接对磁盘文件进行随机读写，然后更新磁盘文件里的数据性能可能相当差。因为磁盘随机读写的性能是非常差的，所以直接更新磁盘文件是不能让数据库抗住很高并发的。 Mysql这套机制看起来复杂，但它可以保证每个更新请求都是更新内存BufferPool，然后顺序写日志文件，同时还能 保证各种异常情况下的数据一致性。 更新内存的性能是极高的，然后顺序写磁盘上的日志文件的性能也是非常高的，要远高于随机读写磁盘文件。 正是通过这套机制，才能让我们的MySQL数据库在较高配置的机器上每秒可以抗下几干的读写请求。 细节介绍server层连接器、词法分析器、优化器、执行器、查询缓存功能见单独文章 地址：mysql执行流程 3.ACID及实现原理 （Atomicity）原子性： 事务是最小的执行单位，不允许分割。要么全都执行,要么全都不执行；——-&gt;原子性是由undolog日志来保证的，它记录了需要回滚的日志信息，事务回滚时，撤销已经执行成功的sql （Consistency）一致性： 执行事务前后，数据保持一致；——-&gt;原子性、隔离性、持久性就是为了来保证一致性 （Isolation）隔离性： 并发访问数据库时，一个事务不被其他事务所干扰，数据库系统提供一定的隔离机制,保证事务在不受外部并发操作影响的“独立”环境执行；——-&gt;锁机制和MVCC共同实现 （Durability）持久性: 一个事务被提交之后。对数据库中数据的改变是持久的，即使数据库发生故障也不会受到影响。——-&gt;持久性由redolog来保证的，mysql修改数据的时候会在redolog中记录一份日志数据，就算数据没有保存成功，只要日志保存成功了，数据仍然不会丢失。 4.MVCCMVCC叫做多版本并发控制，实际上就是保存了数据在某个时间节点的快照。 MVCC主要解决三个问题： 第一、通过MVCC可以解决读写并发阻塞问题，从而提高数据库的并发处理能力； 第二、MVCC采用的是乐观锁的方式实现，降低了死锁的概率； 第三、解决了一致性读的问题，也就是事务启动的时候，根据某个条件去读取到的数据，知道事务结束的时候再去执行相同条件，还是读到同一份数据，不会发生变化变化。 MVCC用于读已提交（RC）和可重复读级别（RR）的控制，主要通过undo log日志版本链和read view来实现， InnoDB中MVCC的实现依赖四个条件: 隐藏字段, 事务链表，read view, undo log 数据表额外字段 DB_TRX_ID(6字节): 在innoDB中, 会为每个事物分配一个事务ID. 而该字段表示的就是插入或更新该行的最后一个事务的事务标识符. DB_ROLL_PTR(7字节): 这个字段称为回滚指针, 指向了回滚段的撤销日志. (undo log) DB_ROW_ID(6字节): 这个字段包含了一个行ID. 该ID在插入新行时会单调增加. 这个字段是为了在表里没有主键ID的时候, 生成聚簇索引使用的. rowid存的是什么？ rowid默认是主键，如果没有主键，会选择唯一键，如果没有唯一键，会生成6字节的rowid undo日志版本链是指一行数据被多个事务依次修改过后，在每个事务修改完后，Mysql会保留修改前的数据undo回滚 日志，并且用两个隐藏字段trx_id和roll_pointer把这些undo日志串联起来形成一个历史记录版本链 在可重复读隔离级别（RR），当事务开启，执行任何查询sql时会生成当前事务的一致性视图read-view，该视图在事务结束 之前都不会变化(如果是读已提交隔离级别在每次执行查询sql时都会重新生成)，这个视图由执行查询时所有未提交事 务id数组（数组里最小的id为min_id）和已创建的最大事务id（max_id）组成，事务里的任何sql查询结果需要从对应 版本链里的最新数据开始逐条跟read-view做比对从而得到最终的快照结果。 版本链比对规则： 如果 row 的 trx_id 落在绿色部分( trx_id&lt;min_id )，表示这个版本是已提交的事务生成的，这个数据是可见的； 如果 row 的 trx_id 落在红色部分( trx_id&gt;max_id )，表示这个版本是由将来启动的事务生成的，是不可见的(若 row 的 trx_id 就是当前自己的事务是可见的）；3. 如果 row 的 trx_id 落在黄色部分(min_id &lt;=trx_id&lt;= max_id)，那就包括两种情况 ​ a. 若 row 的 trx_id 在视图数组中，表示这个版本是由还没提交的事务生成的，不可见(若 row 的 trx_id 就是当前自 己的事务是可见的)； ​ b. 若 row 的 trx_id 不在视图数组中，表示这个版本是已经提交了的事务生成的，可见。 对于删除的情况可以认为是update的特殊情况，会将版本链上最新的数据复制一份，然后将trx_id修改成删除操作的 trx_id，同时在该条记录的头信息（record header）里的（deleted_flag）标记位写上true，来表示当前记录已经被 删除，在查询时按照上面的规则查到对应的记录如果delete_flag标记位为true，意味着记录已被删除，则不返回数 据。 注意：begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个修改操作InnoDB表的语句， 事务才真正启动，才会向mysql申请事务id，mysql内部是严格按照事务的启动顺序来分配事务id的。 总结： MVCC机制的实现就是通过read-view机制与undo版本链比对机制，使得不同的事务会根据数据版本链对比规则读取 同一条数据在版本链上的不同版本数据。 5.mysql三大日志地址：mysql三大日志 6.redo日志地址：redo日志 7.undo日志地址：undo日志 8.能说下myisam 和 innodb的区别吗 区别 InnoDB MyISAM 事务 支持 不支持 外键 支持 不支持 索引 聚簇索引和非聚簇索引 非聚簇索引 行锁 支持 不支持 表锁 支持 支持 存储文件 frm(表结构)，ibd(数据和索引) frm，myi(索引文件)，myd(数据文件) 具体行数 全表扫描统计行数 通过变量保存行数 MyISAM不支持事务，在执行查询语句SELECT前，会自动给涉及的所有表加读锁,在执行update、insert、delete操作会自动给涉及的表加写锁。 InnoDB在执行查询语句SELECT时(非串行隔离级别)，(因为有mvcc机制)不会加锁。但是update、insert、delete操作会加行锁。简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁则会把读和写都阻塞。 9.说下mysql的索引有哪些吧，聚簇和非聚簇索引又是什么，有什么区别聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因 10.什么是覆盖索引和回表覆盖索引指的是在一次查询中，如果一个索引包含或者说覆盖所有需要查询的字段的值，我们就称之为覆盖索引，而不再需要回表查询。 辅助索引的存在并不影响数据在聚集索引中的组织，因此每张表上可以有多个辅助 索引。当通过辅助索引来寻找数据时，InnoDB存储引擎会遍历辅助索引并通过叶级别的指针获得指向主键索引的主键，然后再通过主键索引（聚集索引）来找到一个完整的行记录。这个过程也被称为回表。也就是根据辅助索引的值查询一条完整的用户记录需要使用到2棵B+树—-一次辅助索引，一次聚集索引。 11.mysql的各种锁。 间隙锁锁分类图解 按照兼容性分类 InnoDB实现了以下两种类型的行锁。 「共享锁（S)」：又称读锁 (read lock)，是读取操作创建的锁。其他用户可以并发读取数据， 但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。当如果事务对读锁进行修改操作，很可能会造成死锁。 「排他锁（X)」：exclusive lock（也叫writer lock）又称写锁。 若某个事物对某一行加上了排他锁，只能这个事务对其进行读写，在此事务结束之前， 其他事务不能对其进行加任何锁，其他进程可以读取,不能进行写操作，需等待其释放。 「排它锁是悲观锁的一种实现」。 另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁。 「意向共享锁（IS）」：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。 「意向排他锁（IX）」：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。 意向锁是有数据引擎自己维护的，用户无法手动操作意向锁，在为数据行加共享 / 排他锁之前，InooDB 会先获取该数据行所在在数据表的对应意向锁。 「InnoDB行锁模式兼容性列表」 请求锁模式 是否兼容当前锁模式 X IX S IS X 冲突 冲突 冲突 冲突 IX 冲突 兼容 冲突 兼容 S 冲突 冲突 兼容 兼容 IS 冲突 兼容 兼容 兼容 如果一个事务请求的锁模式与当前的锁兼容，InnoDB就将请求的锁授予该事务；反之，如果两者不兼容，该事务就要等待锁释放。 意向锁是InnoDB自动加的，不需用户干预。对于UPDATE、DELETE和INSERT写操作，InnoDB会自动给涉及数据集加排他锁（X)； 对于普通SELECT语句，InnoDB不会加任何锁；事务可以通过以下语句显示给记录集加共享锁或排他锁。 共享锁（S）：SELECT * FROM table_name WHERE … LOCK IN SHARE MODE。 排他锁（X)：SELECT * FROM table_name WHERE … FOR UPDATE。 用SELECT … IN SHARE MODE获得共享锁，主要用在需要数据依存关系时来确认某行记录是否存在， 并确保没有人对这个记录进行UPDATE或者DELETE操作。但是如果当前事务也需要对该记录进行更新操作， 则很有可能造成死锁，对于锁定行记录后需要进行更新操作的应用，应该使用SELECT… FOR UPDATE方式获得排他锁。 InnoDB在事务执行过程中，使用两阶段锁协议： 随时都可以执行锁定，InnoDB会根据隔离级别在需要的时候自动加锁； 锁只有在执行commit或者rollback的时候才会释放，并且所有的锁都是在同一时刻被释放。 按算法分类 「Record Lock」： 单个行记录上的锁 锁总会锁住索引记录，锁住的是key。 如果InnoDB存储引擎表在建立的时候没有设置任何一个索引，那么这时InnoDB会使用隐式的主键进行锁定。 如果要锁的没有索引，则会进行全表记录加锁。 「Gap Lock」 ：间隙锁，锁定一个范围，但不包含记录本身 锁定索引记录间隙，确保索引记录的间隙不变 间隙锁时针对事务隔离级别为可重复读或以上级别而配的 1Gap Lock在InnoDB的唯一作用就是防止其他事务的插入操作，以此防止幻读 「Next-Key Lock」：临键锁，Gap Lock + Record Lock，锁定一个范围，并且包含记录本身 当查询的索引含有唯一属性时，InnoDB存储引擎会对Next-Key Lock 进行优化，将其降级为 Record Lock，即仅锁住索引本身，而不是范围。 当查询的索引为辅助索引时，默认使用Next-Key Locking技术进行加锁，锁定范围是前一个索引到后一个索引之间范围。 12.MRR每次从二级索引中读取到一条记录后，就会根据该记录的主键值 执行回表操作。而在某个扫描区间中的二级索引记录的主键值是无序的，也就是说这些 二级索引记录对应的聚簇索引记录所在的页面的页号是无序的。 每次执行回表操作时都相当于要随机读取一个聚簇索引页面，而这些随机IO带来的 性能开销比较大。MySQL中提出了一个名为Disk-Sweep Multi-Range Read (MRR，多范围 读取)的优化措施，即先读取一部分二级索引记录，将它们的主键值排好序之后再统一执 行回表操作。 相对于每读取一条二级索引记录就立即执行回表操作，这样会节省一些IO开销。使用这个 MRR优化措施的条件比较苛刻，所以我们直接认为每读取一条二级索引记录就立即执行回表操作。MRR的详细信息，可以查询官方文档。 13.分库分表14.分布式唯一id15.数据库三大范式是什么第一范式：每个列都不可以再拆分。 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。 在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。 16.InnoDB引擎的4大特性插入缓冲（insert buffer) 二次写(double write) 自适应哈希索引(ahi) 预读(read ahead) 17.索引失效的情况谈到索引主要从两方面来分析：IO 和数据结构：不管索引数据还是行数据，都是存在磁盘里面的，我们尽可能少的取出数据 IO—–&gt;读取次数少、量少——&gt;分块读取——&gt;局部性原理、磁盘预读 数据结构——&gt;B+树——&gt;二叉树、AVL树、红黑树、B树 组合索引不遵循最左匹配原则 组合索引前面索引列使用范围查询(&lt;,&gt;,like),会导致后续索引失效； 不要在索引上做任何操作（计算，函数，类型转换） is null和is not null 无法使用索引 尽量少使用or操作符，否则连接时索引会失效 字符串不添加引号会导致索引失效（隐式类型转换） 两表关联使用的条件字段中字段的长度，编码不一致会导致索引失效 like语句中，以%开头的模糊查询会导致索引失效 如果mysql中使用全表扫描比索引快，也会导致索引失效 （force index:强制使用索引） 18.最左前缀原则。 联合索引使用分析实战顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式 实战？？ 19.sql优化之explain 详解在 select 语句之前增加 explain 关键字，MySQL 会在查询上设置一个标记，执行查询会返回执行计划的信息，而不是执行这条SQL 接下来我们将展示 explain 中每个列的信息 19.1. id列 id列的编号是 select 的序列号，有几个 select 就有几个id，并且id的顺序是按 select 出现的顺序增长的。 id列越大执行优先级越高，id相同则从上往下执行，id为NULL最后执行。 19.2. select_type列 select_type 表示对应行是简单还是复杂的查询。 1）simple： 简单查询。查询不包含子查询和union 11 mysql&gt; explain select * from film where id = 2; 2）primary： 复杂查询中最外层的 select 3）subquery： 包含在 select 中的子查询（不在 from 子句中） 4）derived： 包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表（derived的英文含义） 用这个例子来了解 primary、subquery 和 derived 类型 1231 mysql&gt; set session optimizer_switch=&#x27;derived_merge=off&#x27;; #关闭mysql5.7新特性对衍生表的合 并优化 2 mysql&gt; explain select (select 1 from actor where id = 1) from (select * from film where id = 1) der; 11 mysql&gt; set session optimizer_switch=&#x27;derived_merge=on&#x27;; #还原默认配置 5）union： 在 union 中的第二个和随后的 select 11 mysql&gt; explain select 1 union all select 1; 19.3. table列 这一列表示 explain 的一行正在访问哪个表。 当 from 子句中有子查询时，table列是 格式，表示当前查询依赖 id=N 的查询，于是先执行 id=N 的查 询。 当有 union 时，UNION RESULT 的 table 列的值为&lt;union1,2&gt;，1和2表示参与 union 的 select 行id。 19.4. type列 这一列表示关联类型或访问类型，即MySQL决定如何查找表中的行，查找数据行记录的大概范围。 依次从最优到最差分别为：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL 一般来说，得保证查询达到range级别，最好达到ref NULL： mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可 以单独查找索引来完成，不需要在执行时访问表 1 mysql&gt; explain select min(id) from film; const, system： mysql能对查询的某部分进行优化并将其转化成一个常量（可以看show warnings 的结果）。用于 primary key 或 unique key 的所有列与常数比较时，所以表最多有一个匹配行，读取1次，速度比较快。system是 const的特例，表里只有一条元组匹配时为system 1 mysql&gt; explain extended select * from (select * from film where id = 1) tmp; 1 mysql&gt; show warnings; eq_ref： primary key 或 unique key 索引的所有部分被连接使用 ，最多只会返回一条符合条件的记录。这可能是在 const 之外最好的联接类型了，简单的 select 查询不会出现这种 type。 1 mysql&gt; explain select * from film_actor left join film on film_actor.film_id = film.id; ref： 相比 eq_ref，不使用唯一索引，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会 找到多个符合条件的行。 简单 select 查询，name是普通索引（非唯一索引） 1 mysql&gt; explain select * from film where name = ‘film1’; 2.关联表查询，idx_film_actor_id是film_id和actor_id的联合索引，这里使用到了film_actor的左边前缀film_id部分。 1 mysql&gt; explain select film_id from film left join film_actor on film.id = film_actor.fi lm_id; range： 范围扫描通常出现在 in(), between ,&gt; ,&lt;, &gt;= 等操作中。使用一个索引来检索给定范围的行。 1 mysql&gt; explain select * from actor where id &gt; 1;index：扫描全索引就能拿到结果，一般是扫描某个二级索引，这种扫描不会从索引树根节点开始快速查找，而是直接 对二级索引的叶子节点遍历和扫描，速度还是比较慢的，这种查询一般为使用覆盖索引，二级索引一般比较小，所以这 种通常比ALL快一些。 1 mysql&gt; explain select * from film; ALL： 即全表扫描，扫描你的聚簇索引的所有叶子节点。通常情况下这需要增加索引来进行优化了。 1 mysql&gt; explain select * from actor; 19.5. possible_keys列 这一列显示查询可能使用哪些索引来查找。 explain 时可能出现 possible_keys 有列，而 key 显示 NULL 的情况，这种情况是因为表中数据不多，mysql认为索引 对此查询帮助不大，选择了全表查询。 如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查 where 子句看是否可以创造一个适当的索引来提 高查询性能，然后用 explain 查看效果。 19.6. key列 这一列显示mysql实际采用哪个索引来优化对该表的访问。 如果没有使用索引，则该列是 NULL。如果想强制mysql使用或忽视possible_keys列中的索引，在查询中使用 force index、ignore index。 19.7. key_len列 这一列显示了mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列。 举例来说，film_actor的联合索引 idx_film_actor_id 由 film_id 和 actor_id 两个int列组成，并且每个int是4字节。通 过结果中的key_len=4可推断出查询使用了第一个列：film_id列来执行索引查找。 11 mysql&gt; explain select * from film_actor where film_id = 2; key_len计算规则如下： 字符串，char(n)和varchar(n)，5.0.3以后版本中，n均代表字符数，而不是字节数，如果是utf-8，一个数字或字母占1个字节，一个汉字占3个字节 ​ char(n)：如果存汉字长度就是 3n 字节 ​ varchar(n)：如果存汉字则长度是 3n + 2 字节，加的2字节用来存储字符串长度，因为varchar是变长字符串 数值类型 ​ tinyint：1字节 ​ smallint：2字节 ​ int：4字节 ​ bigint：8字节 时间类型 ​ date：3字节timestamp：4字节 ​ datetime：8字节 如果字段允许为 NULL，需要1字节记录是否为 NULL 索引最大长度是768字节，当字符串过长时，mysql会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索引。 19.8. ref列 这一列显示了在key列记录的索引中，表查找值所用到的列或常量，常见的有：const（常量），字段名（例：film.id） 19.9. rows列 这一列是mysql估计要读取并检测的行数，注意这个不是结果集里的行数。 19.10. Extra列 这一列展示的是额外信息。常见的重要值如下： 1）Using index：使用覆盖索引 覆盖索引定义：mysql执行计划explain结果里的key有使用索引，如果select后面查询的字段都可以从这个索引的树中获取，这种情况一般可以说是用到了覆盖索引，extra里一般都有using index；覆盖索引一般针对的是辅助索引，整个查询结果只通过辅助索引就能拿到结果，不需要通过辅助索引树找到主键，再通过主键去主键索引树里获取其它字段值 11 mysql&gt; explain select film_id from film_actor where film_id = 1; 2）Using where：使用 where 语句来处理结果，并且查询的列未被索引覆盖 11 mysql&gt; explain select * from actor where name = &#x27;a&#x27;; 3）Using index condition：查询的列不完全被索引覆盖，where条件中是一个前导列的范围； 11 mysql&gt; explain select * from film_actor where film_id &gt; 1; 4）Using temporary：mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索 引来优化。 actor.name没有索引，此时创建了张临时表来distinct 11 mysql&gt; explain select distinct name from actor; film.name建立了idx_name索引，此时查询时extra是using index,没有用临时表 11 mysql&gt; explain select distinct name from film; 5）Using filesort：将用外部排序而不是索引排序，数据较小时从内存排序，否则需要在磁盘完成排序。这种情况下一般也是要考虑使用索引来优化的。 actor.name未创建索引，会浏览actor整个表，保存排序关键字name和对应的id，然后排序name并检索行记录 11 mysql&gt; explain select * from actor order by name; film.name建立了idx_name索引,此时查询时extra是using index 11 mysql&gt; explain select * from film order by name; 6）Select tables optimized away：使用某些聚合函数（比如 max、min）来访问存在索引的某个字段是 11 mysql&gt; explain select min(id) from film; 20.mysql主从复制原理 master提交完事务后，写入binlog slave连接到master，获取binlog master创建dump线程，推送binglog到slave slave启动一个IO线程读取同步过来的master的binlog，记录到relay log中继日志中 slave再开启一个sql线程读取relay log事件并在slave执行，完成同步 slave记录自己的binglog 21.主从同步延时如何解决22.隔离级别首先回忆四种mysql隔离级别 隔离级别 说明 读未提交（Read uncommitted） 一个事务还没提交时，它做的变更就能被别的事务看到 读提交（Read committed） 一个事务提交之后，它做的变更才会被其他事务看到。出 于性能考虑互联网公司会使用RC+乐观锁 可重复读（Repeatable read） 一个事务中，对同一份数据的读取结果总是相同的，无论是否有其他事务对这份数据进行操作，以及这个事务是否提交。InnoDB默认级别。 串行化（Serializable ） 事务串行化执行，每次读都需要获得表级共享锁，读写相互都会阻塞，隔离级别最高，牺牲系统并发性。 查看当前数据库的事务隔离级别:show variables like ‘%tx_isolation%’; 设置事务隔离级别：set tx_isolation=’REPEATABLE-READ’; 不同的隔离级别是为了解决不同的问题。也就是脏读、幻读、不可重复读。 脏读：事务A读取到了事务B已经修改但尚未提交的数据，。 不可重复读：事务A内部的相同查询语句在不同时刻读出的结果不一致，不符合隔离性。 幻读：事务A读取到了事务B提交的新增数据，不符合隔离性 不可重复读&amp;&amp;幻读区别 不可重复读，重点是修改。在同一事务中，同样的条件，第一次读的数据和第二次读的数据不一样，数据变化 幻读，重点在于新增或者删除。在同一事务中，同样的条件,，第一次和第二次读出来的记录数不一样，行数变化。 隔离级别 脏读 不可重复读 幻读 读未提交 可以出现 可以出现 可以出现 读提交 不允许出现 可以出现 可以出现 可重复读 不允许出现 不允许出现 可以出现 序列化 不允许出现 不允许出现 不允许出现 不同的隔离级别下，隔离性是靠锁机制和MVCC共同实现的。 23.RC和RR的本质区别。readview层面RC级别下每次都是重新生成read view，而在RR级别下则是一直使用事务内第一次查询时产生的read view。 24.什么是readviewReadView可以理解为一个数据结构，在事务开始的时候会根据事务链表构造一个ReadView,初始化方法如下： 12345678910111213// readview 初始化// m_low_limit_id = trx_sys-&gt;max_trx_id; // m_up_limit_id = !m_ids.empty() ? m_ids.front() : m_low_limit_id;ReadView::ReadView() : m_low_limit_id(), m_up_limit_id(), m_creator_trx_id(), m_ids(), m_low_limit_no()&#123; ut_d(::memset(&amp;m_view_list, 0x0, sizeof(m_view_list)));&#125; trx_ids: 表示事务开启的时候, 其它未提交的活跃的事务ID. 这个集合中的事务对于当前事务来说, 一直都是不可见的. low_limit_id: 表示在生成ReadView时当前系统中最大事务id. up_limit_id: 表示未提交活跃事务Id(trx_ids)的最大值. 如果trx_ids为空, 则up_limit_id=low_limit_id. 通过该ReadView，新的事务可以根据查询到的所有活跃事务记录的事务ID来匹配能够看见该记录，从而实现数据库的事务隔离，主要逻辑如下： 通过聚簇索引的行结构中DB_TRX_ID隐藏字段可以知道最近被哪个事务ID修改过。一个新的事务开始时会根据事务链表构造一个ReadView。当前事务根据ReadView中的数据去跟检索到的每一条数据去校验,看看当前事务是不是能看到这条数据 1234567891011121314151617181920212223242526// 判断数据对应的聚簇索引中的事务id在这个readview中是否可见bool changes_visible( trx_id_t id, // 记录的id const table_name_t&amp; name) constMY_ATTRIBUTE((warn_unused_result))&#123; ut_ad(id &gt; 0); // 如果当前记录id &lt; 事务链表的最小值或者等于创建该readview的id就是它自己,那么是可见的 if (id &lt; m_up_limit_id || id == m_creator_trx_id) &#123; return(true); &#125; check_trx_id_sanity(id, name); // 如果该记录的事务id大于事务链表中的最大值,那么不可见 if (id &gt;= m_low_limit_id) &#123; return(false); // 如果事务链表是空的,那也是可见的 &#125; else if (m_ids.empty()) &#123; return(true); &#125; const ids_t::value_type* p = m_ids.data(); //判断是否在ReadView中，如果在说明在创建ReadView时 此条记录还处于活跃状态则不应该查询到，否则说明创建ReadView是此条记录已经是不活跃状态则可以查询到 return(!std::binary_search(p, p + m_ids.size(), id));&#125; 可见性算法逻辑总结： 当检索到的数据的事务ID小于事务链表中的最小值(数据行的DB_TRX_ID &lt; m_up_limit_id)表示这个数据在当前事务开启前就已经被其他事务修改过了,所以是可见的。 当检索到的数据的事务ID表示的是当前事务自己修改的数据(数据行的DB_TRX_ID = m_creator_trx_id) 时，数据可见。 当检索到的数据的事务ID大于事务链表中的最大值(数据行的DB_TRX_ID &gt;= m_low_limit_id) 表示这个数据在当前事务开启后到下一次查询之间又被其他的事务修改过,那么就是不可见的。 如果事务链表为空,那么也是可见的,也就是当前事务开始的时候,没有其他任意一个事务在执行。 当检索到的数据的事务ID在事务链表中的最小值和最大值之间，从m_low_limit_id到m_up_limit_id进行遍历，取出DB_ROLL_PTR指针所指向的回滚段的事务ID，把它赋值给 trx_id_current ，然后从步骤1重新开始判断，这样总能最后找到一个可用的记录。 25.快照读与当前读的区别对于这种读取历史数据的方式，我们叫它快照读 (snapshot read)，而读取数据库当前版本数据的方式，叫当前读 (current read)。很显然，在MVCC中： 快照读：就是select select * from table ….; 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，处理的都是当前的数据，需要加锁。 select * from table where ? lock in share mode; select * from table where ? for update; insert; update ; delete; 26.幻读和不可重复读区别不可重复读，重点是修改。在同一事务中，同样的条件，第一次读的数据和第二次读的数据不一样，数据变化 幻读，重点在于新增或者删除。在同一事务中，同样的条件,，第一次和第二次读出来的记录数不一样，行数变化。 27.MySQL在RR级别中完全解决了幻读的问题么？28.脏页是什么，刷脏页时机29.bufferpool30.几大核心线程purge线程 对于一条delete语句 delete from t where a = 1，如果列a有聚集索引，则不会进行真正的删除，而只是在主键列等于1的记录delete flag设置为1，即记录还是存在在B+树中。而对于update操作，不是直接对记录进行更新，而是标识旧记录为删除状态，然后新产生一条记录。那这些旧版本标识位删除的记录何时真正的删除？怎么删除？ 其实InnoDB是通过undo日志来进行旧版本的删除操作的，在InnoDB内部，这个操作被称之为purge操作，原来在srv_master_thread主线程中完成，后来进行优化，开辟了purge线程进行purge操作，并且可以设置purge线程的数量。purge操作每10s进行一次。 为了节省存储空间，InnoDB存储引擎的undo log设计是这样的：一个页上允许多个事务的undo log存在。虽然这不代表事务在全局过程中提交的顺序，但是后面的事务产生的undo log总在最后。此外，InnoDB存储引擎还有一个history列表，它根据事务提交的顺序，将undo log进行连接，如下面的一种情况： 在执行purge过程中，InnoDB存储引擎首先从history list中找到第一个需要被清理的记录，这里为trx1，清理之后InnoDB存储引擎会在trx1所在的Undo page中继续寻找是否存在可以被清理的记录，这里会找到事务trx3，接着找到trx5，但是发现trx5被其他事务所引用而不能清理，故再去history list中取查找，发现最尾端的记录时trx2，接着找到trx2所在的Undo page，依次把trx6、trx4清理，由于Undo page2中所有的记录都被清理了，因此该Undo page可以进行重用。 InnoDB存储引擎这种先从history list中找undo log，然后再从Undo page中找undo log的设计模式是为了避免大量随机读操作，从而提高purge的效率。 31.事务传播机制32.分布式事务通用方案33.成本分析34.表中数据量如何计算总量35.count(1) count(*) count(id) count(字段) 性能对比分析四个sql的执行计划一样，说明这四个sql执行效率应该差不多 字段有索引：count(*)≈count(1)&gt;count(字段)&gt;count(主键 id) //字段有索引，count(字段)统计走二级索引，二 级索引存储数据比主键索引少，所以count(字段)&gt;count(主键 id) 字段无索引：count(*)≈count(1)&gt;count(主键 id)&gt;count(字段) //字段没有索引count(字段)统计走不了索引， count(主键 id)还可以走主键索引，所以count(主键 id)&gt;count(字段) count(1)跟count(字段)执行过程类似，不过count(1)不需要取出字段统计，就用常量1做统计，count(字段)还需要取出 字段，所以理论上count(1)比count(字段)会快一点。 count(*) 是例外，mysql并不会把全部字段取出来，而是专门做了优化，不取值，按行累加，效率很高，所以不需要用 count(列名)或count(常量)来替代 count(*)。 为什么对于count(id)，mysql最终选择辅助索引而不是主键聚集索引？因为二级索引相对主键索引存储数据更少，检索 性能应该更高，mysql内部做了点优化(应该是在5.7版本才优化)。 36.mysql中in 和exists 区别37.join查询原理38.索引下推索引下推（Index Condition Pushdown，ICP), like KK%其实就是用到了索引下推优化 什么是索引下推了？ 对于辅助的联合索引(name,age,position)，正常情况按照最左前缀原则，SELECT * FROM employees WHERE name like ‘LiLei%’ AND age = 22 AND position =’manager’ 这种情况只会走name字段索引，因为根据name字段过滤完，得到的索引行里的age和 position是无序的，无法很好的利用索引。 在MySQL5.6之前的版本，这个查询只能在联合索引里匹配到名字是 ‘LiLei’ 开头的索引，然后拿这些索引对应的主键逐个回表，到主键索 引上找出相应的记录，再比对age和position这两个字段的值是否符合。 MySQL 5.6引入了索引下推优化，可以在索引遍历过程中，对索引中包含的所有字段先做判断，过滤掉不符合条件的记录之后再回表，可 以有效的减少回表次数。使用了索引下推优化后，上面那个查询在联合索引里匹配到名字是 ‘LiLei’ 开头的索引之后，同时还会在索引里过 滤age和position这两个字段，拿着过滤完剩下的索引对应的主键id再回表查整行数据。 索引下推会减少回表次数，对于innodb引擎的表索引下推只能用于二级索引，innodb的主键索引（聚簇索引）树叶子节点上保存的是全 行数据，所以这个时候索引下推并不会起到减少查询全行数据的效果。 个人理解升华： 我觉得like是这样，用到索引下推的场景，直接利用三个索引都过滤玩，然后回表查数据; 关闭索引下推，利用的第一个索引完成过滤，直接回表，然后继续在server层对第二三字段过滤,都是用到了三个字段，但是第二三个字段用的地方不一样 为什么范围查找Mysql没有用索引下推优化？ 然后理论上like和大于小于都是range类型，只不过大于小于这种有点正负无穷的感觉，所以mysql没有特殊处理，like ‘xx%’ mysql默认认为范围可控，就使用刚才的逻辑来处理，当然这也不是绝对的，有时like KK% 也不一定就会走索引下推。 39.两种排序方式1、MySQL支持两种方式的排序filesort和index，Using index是指MySQL扫描索引本身完成排序。index 效率高，filesort效率低。 40.filesort排序原理Using filesort文件排序原理详解 filesort文件排序方式 单路排序：是一次性取出满足条件行的所有字段，然后在sort buffer中进行排序；用trace工具可 以看到sort_mode信息里显示&lt; sort_key, additional_fields &gt;或者&lt; sort_key, packed_additional_fields &gt; 双路排序（又叫回表排序模式）：是首先根据相应的条件取出相应的排序字段和可以直接定位行 数据的行 ID，然后在 sort buffer 中进行排序，排序完后需要再次取回其它需要的字段；用trace工具 可以看到sort_mode信息里显示&lt; sort_key, rowid &gt; MySQL 通过比较系统变量 max_length_for_sort_data(默认1024字节) 的大小和需要查询的字段总大小来 41.filesort判断使用哪种排序模式。 （单路or双路）如果 字段的总长度小于max_length_for_sort_data ，那么使用 单路排序模式； 如果 字段的总长度大于max_length_for_sort_data ，那么使用 双路排序模∙式。 40.单路排序、双路排序详细过程我们先看单路排序的详细过程： 从索引name找到第一个满足 name = ‘zhuge’ 条件的主键 id 根据主键 id 取出整行，取出所有字段的值，存入 sort_buffer 中 从索引name找到下一个满足 name = ‘zhuge’ 条件的主键 id 重复步骤 2、3 直到不满足 name = ‘zhuge’ 对 sort_buffer 中的数据按照字段 position 进行排序 返回结果给客户端 我们再看下双路排序的详细过程： 从索引 name 找到第一个满足 name = ‘zhuge’ 的主键id 根据主键 id 取出整行，把排序字段 position 和主键 id 这两个字段放到 sort buffer 中 从索引 name 取下一个满足 name = ‘zhuge’ 记录的主键 id 重复 3、4 直到不满足 name = ‘zhuge’ 对 sort_buffer 中的字段 position 和主键 id 按照字段 position 进行排序 遍历排序好的 id 和字段 position，按照 id 的值回到原表中取出 所有字段的值返回给客户端 其实对比两个排序模式，单路排序会把所有需要查询的字段都放到 sort buffer 中，而双路排序只会把主键和需要排序的字段放到 sort buffer 中进行排序，然后再通过主键回到原表查询需要的字段。 41.三星索引对于一个查询而言，一个三星索引，可能是其最好的索引。 索引将相关的记录放到一起则获得一星； 如果索引中的数据顺序和查找中的排列顺序一致则获得二星； 如果索引中的列包含了查询中需要的全部列则获得三星。 42.场景题新建一张数据表user，后续所有操作都依托于初始化的这三条数据。 id name age 1 zhangsan 1 2 lisi 5 3 wangwu 10 操作1： 时刻 t1 t2 1 begin begin 2 update user set name=’zhangsan2’ where age=1;commit; 3 select * from user where age=1; 此时不管是RC还是RR，t1的select都能够读取到t2update的值 因为在t1select的时刻才会去生成read view，根据可见性算法得出能看到最新的t2数据 操作2: 时刻 t1 t2 1 begin begin 2 select * from user where age=1; 3 update user set name=’zhangsan2’ where age=1;commit; 4 select * from user where age=1; 此时RC级别下，t1的 第二次select能够查看到t2update的值，因为RC级别下每次select都会生成新的read view 在RR级别下，t1的 第二次不能查询到t2update的值，因为RR级别下的后面查询都是使用的第一次select生成的read view 同理，t2语句为insert时也是一样的情况。 操作3: 时刻 t1 t2 1 begin begin update user set name=’zhangsan2’ where age=1; 3 insert into user values (4,’haha’,3);waiting~~~ 5 commit; 6 插入成功commit; RR级别下，t1进行当前读时，会对数据加锁，具体是间隙锁｜行锁｜next-key，后面细说，此时t2的插入会waiting，直到t1释提交事务，来解决insert情况下的幻读问题 那么，感觉RR就已经解决了幻读，为啥还需要串行化（Serializable ）？？？ 操作4: 时刻 t1 t2 1 begin begin select * from user where age=1; 3 insert into user values (4,’haha’,1);commit; 5 update user set name=’zhangsan2’ where age=1; 6 commit; RR级别下，此时想象中的结果为下面，因为实际上在第五行如果不是update而是select，是肯定差不到新数据的 id name age 1 zhangsan2 1 2 lisi 5 3 wangwu 10 4 haha 1 但是实际的数据库中结果为： id name age 1 zhangsan2 1 2 lisi 5 3 wangwu 10 4 zhangsan2 1 其实在 MySQL在RR级别中并不是完全解决了幻读的问题，对照操作2中t2使用insert、操作3、以及操作4可以看出某些场景下RR解决了幻读，但是MVCC 对于幻读的解决是不彻底的。（快照读和当前读的混合使用会产生幻读问题）","categories":[{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"}]},{"title":"面试知识点总结","slug":"12.面试/JVM","date":"2022-03-30T16:00:00.000Z","updated":"2024-12-23T06:48:07.035Z","comments":true,"path":"2022/03/31/12.面试/JVM/","link":"","permalink":"https://zhangxin66666.github.io/2022/03/31/12.%E9%9D%A2%E8%AF%95/JVM/","excerpt":"","text":"1.jvm内存模型 堆：堆Java虚拟机中最大的一块内存，是线程共享的内存区域，基本上所有的对象实例数组都是在堆上分配空间。堆区细分为Yound区年轻代和Old区老年代，其中年轻代又分为Eden、S0、S1 3个部分，他们默认的比例是8:1:1的大小。 栈：栈是线程私有的内存区域，每个方法执行的时候都会在栈创建一个栈帧，方法的调用过程就对应着栈的入栈和出栈的过程。每个栈帧的结构又包含局部变量表、操作数栈、动态连接、方法返回地址。 局部变量表用于存储方法参数和局部变量。当第一个方法被调用的时候，他的参数会被传递至从0开始的连续的局部变量表中。 操作数栈用于一些字节码指令从局部变量表中传递至操作数栈，也用来准备方法调用的参数以及接收方法返回结果。 动态连接用于将符号引用表示的方法转换为实际方法的直接引用。 元数据：在Java1.7之前，包含方法区的概念，常量池就存在于方法区（永久代）中，而方法区本身是一个逻辑上的概念，在1.7之后则是把常量池移到了堆内，1.8之后移出了永久代的概念(方法区的概念仍然保留)，实现方式则是现在的元数据。它包含类的元信息和运行时常量池。 Class文件就是类和接口的定义信息。 运行时常量池就是类和接口的常量池运行时的表现形式。 本地方法栈：主要用于执行本地native方法的区域 程序计数器：也是线程私有的区域，用于记录当前线程下虚拟机正在执行的字节码的指令地址 7.程序计数器-PCJVM中的程序计数器，命名源于CPU的寄存器，寄存器存储相关的现场信息，CPU只有把数据装载到寄存器才能运行，这里，也不是指广义上的物理寄存器，或许将其翻译为PC计数器，会更贴切，并且也不容易引起一些不必要的误会，JVM中的PC寄存器是对物理PC寄存器的一种抽象。 程序计数器是用来存储指向下一条指令的地址，也即将要执行的指令代码，由执行引擎读取下一条指令。 运行速度最快的区域。线程私有的，生命周期与线程的生命周期一致。 任何时间一个线程都只有一个方法在执行，也就是所谓的当前方法，程序计数器会存储当前线程正在执行的Java方法的JVM指令地址，或者，如果在执行native方法，则为空。 程序计数器存放的是Java字节码的地址，而native方法的方法体是非Java的，所以程序计数器的值才未定义。 那在native方法执行后，线程又如何确保下一次执行的位置？ 这是因为每个Java线程都直接映射到一个OS线程上执行。所以native方法就在本地线程上执行，无需理会JVM规范中的程序计数器的概念。仔细看一下JVM规范，如果一个线程执行Native方法，程序计数器的值未定义，可不是一定为空，任何值都可以。native方法执行后会退出(栈帧pop)，方法退出返回到被调用的地方继续执行程序。 循环，分支跳转，异常处理，线程恢复都是依赖于程序计数器 字节码解释器工作时，就是通过改变这个计数器的值来选取下一条需要执行的字节码指令 JVM规范中唯一一个没有内存溢出错误的区域 1.使用PC寄存器存储字节码指令地址有什么作用？ 因为cpu需要不停地切换各个线程，这时候切换回来以后，就得知道接着从哪里继续执行。JVM的字节码解释器就需要通过改变PC寄存器的值来明确下一条应该执行什么样的字节码指令。 2.pc寄存器为什么要设置成线程私有的？ 每一个线程都需要一个PC线程计数器来记录执行到了哪条字节码指令。 cpu时间片 CPU分配给各个程序的时间，每个线程被分配一个时间段，称作他的时间片。微观上，由于只有一个cpu，一次只能处理程序要求的一部分，如何处理公平，一种方法就是引入时间片，每个程序轮流执行。 并行vs串行 并行：几个线程同时执行 串行：几个线程一个一个执行 并发：垃圾回收线程和用户线程同时执行 8.虚拟机栈1，虚拟机栈出现的背景 由于跨平台性的设计，java的指令集架构是基于栈的指令集架构。不同平台cpu架构不同，所以不能设计为基于寄存器的指令集架构。 优点是跨平台，指令集小，编译器容易实现，缺点是性能下降，实现同样的功能需要更多的指令。 2．内存中的栈与堆 栈是运行时单位，堆是存储的单位。 栈解决的是程序的运行问题，既程序如何执行，或者说如何处理数据，堆解决的是数据存储的问题，既数据怎么放，放在哪里。 栈中可以存放方法内的基本类型变量和引用变量的地址。 3．虚拟机栈基本内容 每个线程创建的时候都会创建一个ｊａｖａ虚拟机栈，其内部保存一个个的栈帧，对应着一次次方法的调用（一个栈帧对应着一个ｊａｖａ方法）。 栈顶的方法被称为当前方法。栈是线程私有的，生命周期和线程一致。 1）作用 主管ｊａｖａ程序的运行，他保存方法的局部变量（８种数据类型，对象的引用地址），部分结果，并参与方法的调用和返回。 2）栈的特点 栈是一种快速有效的分配存储方式，访问速度仅仅次于程序计数器 jvm堆栈的直接操作只有两个 ：方法执行，入栈；方法结束，出栈。 对于栈来说，不存在垃圾回收问题。 4.栈的常见异常和参数设置 jvm规范允许栈的大小是动态的或者是固定不变的。 如果采用固定大小的java栈，那每一个线程的虚拟机容量可以在线程创建的时候独立选定，如果线程请求分配的栈容量超过java虚拟机允许的最大容量，jvm会抛出StackOverFlowError（递归没有出口）。 特别的，在hotspot虚拟机中，如果在为栈分配内存时，内存不足，抛出的异常并不是oom，而是StackOverFlowError。 如果java虚拟机栈可以动态扩容，并且在尝试扩展的时候无法申请到足够的内存，或者在创建新的线程是没有足够的内存去创建对应的虚拟机栈，那java虚拟机会抛出oom。 设置栈的大小 -Xss 设置线程的最大栈空间，栈的大小直接决定了函数调用的最大可达参数。 1-Xss256k 5．栈的存储结构和运行原理 每个线程都有自己的栈，栈中的数据都是以栈帧的形式存在。 在这个线程上正在执行的每个方法都各自对应一个栈帧。 栈帧是一个内存区块，是一个数据集，维系着方法执行过程中各种数据信息。 １）栈的运行原理 一个线程中，一个时间点上，只会有一个活动的栈帧，既只有当前正在执行的方法的栈帧是有效的，这个栈帧被称为当前栈帧。与这个栈帧对应的方法是当前方法，定义当前方法的类叫做当前类。 执行引擎运行的所有字节码指令只针对当前栈帧进行操作。 如果在该方法中调用了其他方法，对应的新的栈帧被创建出来，放在栈顶，成为新的当前栈帧。 不同线程中所包含的栈帧是不允许相互引用的，既不可能在一个栈帧之中引用另外一个线程的栈帧。 如果当前方法调用了其他方法，方法返回时，当前栈帧会传回此方法的执行结果给前一个栈帧，接着，虚拟机会丢弃当前栈帧，使得前一个栈帧成为当前栈帧。 Java方法有两种返回函数的方式，一种是正常的函数返回，使用return指令，另一种是抛出异常，不管使用哪种方式，栈帧都会弹出。 ２）栈帧的内部结构 每个栈帧中存储着 局部变量表 操作数栈 动态链接（指向运行时常量池的方法引用） 方法返回地址 一些附加信息 ３）局部变量表 本地变量表：定义为一个数字数组，主要用于存储方法参数和定义在方法体内的局部变量，这些数据类型包括各种基本数据类型，对象引用，以及返回地址类型。 由于局部变量表是建立在线程的栈上的，是线程私有数据，因此不存在数据安全问题。 局部变量表所需容量大小是在编译器确定下来的，并保存在方法的Ｃｏｄｅ属性的ｍａｘｉｍｕｎ ｌｏｃａｌ ｖａｒｉａｂｌｅｓ数据项，方法运行期间变量表的大小是不会改变的。 参数值的存放总是在局部变量数组的ｉｎｄｅｘ０开始，到数组长度－１的索引结束。 局部变量表，最基本的存储单元是变量槽。 局部变量表中存放编译期间可知的各种基本数据类型，引用类型，ｒｅｔｕｒｎＡｄｄｒｅｓｓ类型。 方法嵌套的次数由栈的大小决定，局部变量表中的变量只在当前方法调用中有效，局部变量表随着栈帧销毁。 ４）关于ｓｌｏｔ的理解 参数值的存放总是在局部变量表的ｉｎｄｅｘ０开始，到数组长度－１的索引结束。 局部变量表，最基本的存储单元是变量槽 ｓｌｏｔ 局部变量表中存放着编译期可知的各种基本数据类型，引用类型，ｒｅｔｕｒｎＡｄｄｒｅｓｓ的变量。 在局部变量表里，３２位以内的类型只占用一个ｓｏｒｔ，６４占用两个ｓｏｒｔ。 ｊｖｍ会为局部变量表中的每一个ｓｌｏｔ都分配一个访问索引，通过这个索引就可以成功访问到局部变量表中指定的局部变量值。 当一个实例方法被调用的时候，他的方法参数和方法体内部定义的局部变量将会按照顺序被复制到局部变量表中的每一个slot上。 如果需要访问一个局部变量表中64bit的局部变量值时，只需要使用一个索引即可。 如果当前帧是由构造方法或者实例方法(非static)创建的，那么该对象引用this将会存放在index为0的slot处，其余的参数按照参数表顺序继续排列。 为什么静态方法不能使用ｔｈｉｓ？ 静态方法的局部变量表没有ｔｈｉｓ ｓｌｏｔ的重复利用：栈帧中的局部变量表中的槽位是可以重复利用的，如果一个局部变量过了其作用域，那么再其作用域之后声明的新的局部变量就很有可能会复用过期的局部变量的槽位，从而达到节省资源的目的。 面试题：为什么if建议写大括号？ 为了栈帧局部变量表里的slot槽能够最大可能的重复利用 6．静态变量与局部变量的对比 变量按照数据类型：基本数据类型和引用数据类型 变量按照类中声明位置：成员变量（类变量 ｓｔａｔｉｃ ，局部变量） 局部变量 成员变量 在使用前都经过默认赋值，ｉｎｔ显式赋值。 实例变量 随着对象创建会在堆空间中分配实例变量空间，并进行默认赋值。 局部变量显式赋值，否则没法使用。 在栈帧中，与性能调优最为密切的就是局部变量表，在方法执行时，虚拟机使用局部变量表完成方法的传递。 在局部变量表中的变量也是重要的垃圾回收根节点，只要被局部变量表中直接引用或间接引用的对象都不会被回收。 7．操作数栈 栈的结构可以是数组或者单向链表，操作数栈是数组实现的。 操作数栈，在方法执行过程中，根据字节码指令，往栈中写入数据或者提取数据。 某些字节码指令将数值压入操作数栈，其余的字节码指令将操作数取出栈，使用他们后再把结果压入栈。 执行复制，操作，求和。 如果被调用的方法带有返回值的话，其返回值将会被压入当前栈帧的操作数栈中，并更新ＰＣ寄存器中下一条需要执行的字节码指令。 操作数栈中元素的数据类型必须与字节码指令的序列严格匹配，这由编译器在编译期间进行验证，同时在类加载过程中的类检验阶段的数据流分析阶段要再次验证。 操作数栈，主要用于保存计算过程的中间结果，同时作为计算过程中变量的临时存储空间。 每一个操作数栈都会拥有一个明确的栈深度用于存储数值，其所需的最大深度在编译期间就定义好了，保存在方法的ｃｏｄｅ属性，为ｍａｘ＿ｓｔａｃｋ的值。 栈中的任何元素都是可以任意的ｊａｖａ数据类型。 ｊａｖａ虚拟机的解释引擎是基于栈的执行引擎，其中的栈指的就是操作数栈。 涉及操作数栈的字节码指令执行分析 8．栈顶缓存技术 由于操作数是存储在内存中，因此频繁的执行内存读写必然会影响执行速度。为了解决这个问题，HotSpot JVM的设计者们提出了栈顶缓存技术，将栈顶元素全部缓存在物理CPU的寄存器中，以此降低对内存的读写次数，提升执行引擎的执行效率。 9．动态链接 指向运行时常量池的方法引用。 每一个栈帧的内部都包含一个指向运行时常量池中该栈帧所属方法的引用。包含这个引用的目的就是为了支持当前方法的代码能够实现动态链接。 在ｊａｖａ源文件在编译到字节码文件中时，所有的变量和方法引用都作为符号引用保存在ｃｌａｓｓ文件的常量池里。比如描述一个方法调用了另外的其他方法时，就是通过常量池中指向方法的符号引用来表示的，那么动态链接的作用就是为了将这些符号引用转换为调用方法的直接引用。 为什么需要常量池？为了提供一些符号和常量，便于指令识别。 10.方法的调用 在ｊｖｍ中，将符号引用转换为调用方法的直接引用与方法的绑定机制有关。 1）动态链接与静态链接 静态链接：当一个字节码文件被装载到JVM内部时，如果被调用的 目标方法在编译期可知 ，且运行期保持不变时，这种情况下调用方法的符号引用转换为直接引用的过程称之为静态链接。 动态链接：如果 被调用的方法在编译期无法被确定下来 ，也就是说，只能够在程序运行期将调用方法的符号引用转换为直接引用，由于这种引用转换过程具备动态性，因此也就被称之为动态链接。 2）对应方法的绑定机制 早期绑定(Early Binding)和晚期绑定(Late Binding)。 绑定是一个字段(属性)、方法或者类在符号引用被替换为直接引用的过程，这仅仅发生一次。 1.早期绑定就是指被调用的 目标方法如果在编译期可知，且运行期保持不变时 ，即可将这个方法与所属的类型进行绑定，这样一来，由于明确了被调用的目标方法究竟是哪一个，因此也就可以使用静态链接的方式将符号引用转换为直接引用。 2.如果 被调用的方法在编译期无法被确定下来，只能够在程序运行期根据实际的类型绑定相关的方法 ，这种绑定方式也就被称之为晚期绑定。 Java中任何一个普通的方法其实都具备虚函数的特征，它们相当于C语言中的虚函数(C中则需要使用关键字virtual 来显示定义)。 如果在Java程序中不希望某个方法拥有虚函数的特征时，则可以使用关键字final来标记这个方法。 3）4种方法调用指令区分非虚方法与虚方法 虚方法与非虚方法 如果方法在编译期就确定了具体的调用版本，这个版本再运行时是不可变的。这样的方法称为非虚方法。 静态方法、私有方法、final方法、实例构造器、父类方法都是非虚方法。其他方法称为虚方法。 (子类对象的多态性的使用前提：①类的继承关系 ②方法的重写) 动态类型语言和静态类型语言 1、动态类型语言和静态类型语言两者的区别就在于对类型的检查是在编译期还是在运行期，满足前者就是静态类型语言，反之是动态类型语言。 2、静态类型语言是判断变量自身的类型信息 3、动态类型语言是判断变量值的类型信息。变量没有类型信息，变量值才有类型信息，这是动态语言的一个重要特征。 4）方法重写的本质与虚方法表的使用 方法重写的本质 找到操作数栈顶的第一个元素所执行的对象的实际类型，记做Ｃ。 如果在类型Ｃ中找到与常量中的描述符和简单名称都相符的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用，查找过程结束；如果不通过则返回java.lang.IllegalAccessError。 否则，按照继承关系从下往上依次对C的各个父类进行第二步的搜索和验证过程。 如果始终没有找到合适的方法，则抛出java.lang.AbstractMethodError异常。 java.lang.IllegalAccessError 程序试图访问或者修改一个属性或调用一个方法，这个属性或者方法，你没有权限访问，一般的，这个会引起编译期异常，这个错误 如果发生在运行时，就说明一个类发生了不兼容的改变。 补充：多态的本质就是指向同一个虚方法表的引用。 11.方法返回地址 存放着调用该方法的pc寄存器的值 一个方法结束，有两种方式： 1.正常执行完成 2.出现未处理的异常，非正常退出 无论通过哪种方式退出，在方法退出后都要返回到该方法被调用的位置，方法正常退出时，调用者的PC寄存器的值作为返回地址；即调用该方法的指令的下一条指令的地址，而通过异常退出的，返回的地址需要通过异常表来确定，栈帧中一般不会保存这部分信息。 当一个方法开始执行后，只有两种方式可以退出方法： 1.当执行引擎遇到任意一个方法返回的字节码指令，会有返回值传递给上层的方法调用者，简称正常完成出口。 一个方法在正常执行完成之后究竟需要使用哪一个返回指令还需要根据方法返回值的实际参数类型而定。 在字节码指令中，返回指令包含ireturn（int），lreturn（long），freturn（float），dreturn（double），areturn是引用类型，另外还有一个return指令供声明为void的方法，实例初始化方法，类和接口初始化方法使用。 在方法执行过程中遇到了异常，并且这个异常没有在方法内进行处理，也就是只要在本地方法的异常表中没有搜索到匹配的异常处理器，就会导致方法退出。简称异常完成出口。 方法执行过程中抛出异常时的异常处理，存储在一个异常处理表，方便在发生异常的时候找到处理异常的代码 总结 本质上，方法的退出就是当前栈帧出栈的过程，此时，需要回复上层方法的局部变量表，操作栈数，将返回值压入调用者栈帧的操作数栈，设置PC寄存器值等，让调用者方法能够继续执行下去。 正常完成出口和异常完成出口的区别在于：通过异常完成出口退出的不会给他的上层调用者产生任何返回值。 12.栈帧中的一些附加信息 栈帧中还允许携带与java虚拟机实现相关的一些附加信息。例如：对程序调试提供支持的信息。 13.虚拟机的五道面试题 1.栈溢出的情况？ 递归没有出口，方法无限循环调用 主要从两个方面考虑，申请固定内存但是内存不足，动态扩容内存不足。 2.调整栈的大小，就能保证栈不溢出嘛？ 不是，比如递归没有出口 3.分配的栈内存越大越好么？ 不是 4.垃圾回收是否涉及到虚拟机栈？ 不是，但是局部变量表的引用属于垃圾回收的根节点。 5.方法中定义的局部变量是否是线程安全的？ 不一定，只有内部生内部死的变量才是线程安全的，不是内部产生的（方法参数），内部产生又返回到外面的（返回值），是线程不安全的。 9.本地方法接口和本地方法栈 什么是本地方法？ 1、简单讲：一个Native Method 就是一个Java调用非Java代码的接口。一个 Native Method是这样的一个Java方法：该方法的实现由非Java语言实现，比如 C 。这个特征并非Java所特有，很多其它的编程语言都有这一机制，比如在C中，可以用 extern “C” 告知C 编译器去调用一个C 的函数。 2、“A native method is a Java method whose implementation is provided by non-java code.” 3、在定义一个native method 时，并不提供实现体(有些像定义一个Java interface )，因为其实现体是由非Java语言在外面实现的。 4、本地接口的作用是融合不同的编程语言为Java 所用，它的初衷是融合 C/C++程序。 5、标识符native可以与所有其他的Java标识符连用(除了abstract)。 为什么使用本地方法？ 1.java虽然使用方便，但是有一些层次的任务用java实现并不容易，或者我们对程序的效率很在意时，问题会出现。 2.目前该方法的使用越来越少了，除非是与硬件有关的应用，比如通过java程序驱动打印机或者java系统管理生产设备，在企业级应用中已经少见，因为现在的异构领域间的通信很发达，比如可以使用Socket通信，也可以使用WebService等。 本地方法栈的理解 java虚拟机栈用于管理java方法的使用，而本地方法栈用于管理本地方法的调用，本地方法栈也是线程私有的，它允许被实现成固定或者可动态扩展的内存大小（在内存溢出方面是相通的）。本地方法主要是使用c语言来实现的，它的具体做法是在本地方法栈中登记本地方法，在执行引擎执行的时候加载本地方法库。 本地方法栈，本地接口和本地方法库之间的联系 当某个线程调用一个本地方法时，他就进入了一个全新的并且不再受虚拟机限制的世界，他和虚拟机拥有同样的权限，本地方法可以通过本地方法接口来访问虚拟机内部的运行时数据区。他甚至可以直接使用本地处理器中的寄存器，直接从本地内存的堆中分配任意数量的内存。 并不是所有的jvm都支持本地方法，因为Java虚拟机规范并没有明确要求本地方法栈的使用语言、具体实现方式、数据结构等。如果 JVM 产品不打算支持 native 方法，也可以无需实现本地方法栈。 在hotspot虚拟机中直接将本地方法栈和java虚拟机栈合二为一。 10.堆 11.堆的核心概念一个jvm实例只存在一个堆空间，他是java内存管理的核心区域。他在jvm启动时创建，空间大小也就确定了，是jvm管理的最大一块内存空间，堆内存的大小是可以调节的，jvm规范规定，堆可以处于物理上内存不连续的空间，但是在逻辑上他应该被视为连续的。所有线程共享java堆，在这里还可以划分线程私有的缓冲区（TLAB）。 jvm规范中对java堆的描述是：几乎所有的对象实例以及数组都应当在运行时分配在堆上。数组和对象可能永远不会存储在栈上，因为栈帧中保存引用，这个引用指向对象或者数组在堆中的位置。在方法结束后，堆中对象不会马上移除，仅仅在垃圾收集的时候才会被移除。堆是GC垃圾回收的重点区域。 12.堆的内存细分 现代垃圾收集器大部分基于分代收集理论，堆空间细分为：新生代（伊甸园区，幸存者0和幸存者1），老年代，永久代（jdk8改名叫元空间） 3.设置堆内存大小与OOM 1）堆空间的大小设置java堆用于存储java实例对象，那么堆的大小在jvm启动的时候就已经设定好了，可以通过-Xms和-Xmx来设置。 1234-Xms用于表示堆的起始内存-Xmx用于表示堆区的最大内存-X是jvm的运行参数ms是memory start 一旦堆区中的内存大小超过-Xms所指定的最大内存时，将会抛出OutOfMemoryError异常。通常将起始内存和最大内存设置相同的值，其目的是为了能够在垃圾回收机制清理完堆空间后不需要重新分隔计算堆空间的大小，从而提高性能。 默认情况下，初始内存：物理内存/64，最大内存大小物理内存/4，查看设置的参数JPS 或者 jstat -gc 进程id -XX:+PrintGCDetails 打印垃圾回收细节 123456//返回java虚拟机中的堆内存总量long initMemory=Runtime.getRuntime().totalMemory()/1024/1024;//返回java虚拟机试图使用的最大堆内存量long maxMemory=Runtime.getRuntime().maxMemory()/1024/1024;System.out.println(&quot;-Xms:&quot;+initMemory);System.out.println(&quot;-Xmx:&quot;+maxMemory); 2）OutOfMemoryError集合或者数组创建过大，导致堆空间超出内存。 13.年轻代和老年代存储在jvm中的java对象可以被划分为两类 生命周期比较短的和生命周期比较长的。 java堆区进一步细分的话，可以划分为年轻代和老年代（1:2），其中年轻代又可以划分为Eden，幸存者0，幸存者1（8:1:1）。默认自适应，其实并不是8:1:1。 12-NewRatio=2 设置新生代与老年代的比例，默认是2-XX：SurvivorRatio=8 设置新生代中Eden区与Survivor区的比例 几乎所有的对象都是在Eden区被new出来的。绝大多数java对象的销毁都是在新生代进行的，80%的对象都是朝生夕死，可以使用-Xmn设置新生代最大内存，一般使用默认。 2.对象创建过程 类加载检查：虚拟机遇到一条new指令时，首先将去检查这个类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程：类加载过程中会通过jvm自带的三个类加载器按照双亲委派机制进行类加载过程，我们常见的ClassNotFoundException就是在这里发生的，我们代码中的静态代码块里面的内容也是在这个过程中执行的。 分配内存：在类加载检查通过后，虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务等同于把 一块确定大小的内存从Java堆中划分出来。 初始化零值：内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 设置对象头：初始化零值之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息(即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例)、对象的哈希码（HashCode）、对象的GC分代年龄、锁状态标志等信息。这些信息存放在对象的对象头Object Header之中。 执行方法：执行方法，即对象按照程序员的意愿进行初始化。对应到语言层面上讲，就是为属性赋值（注意，这与上面的赋零值不同，这是由程序员赋的值），和执行构造方法。 3.类的加载过程加载-&gt;链接（验证，准备，解析）-&gt;初始化 1）加载 通过全限定类名加载一个类的二进制字节流，将静态结构转化为方法区的运行时数据结构，在内存中生成一个代表这个类的Class对象 加载.class文件的方式： 从本地系统中直接加载 通过网络获取，典型场景：Web Applet 从zip压缩包中读取，成为日后jar ，war格式的基础 运行时计算生成，使用最多的是：动态代理技术 其他文件生成：典型场景JSP 从专有数据库提取.class文件，比较少见 从加密文件中获取，典型的防Class文件被反编译的保护措施 2）链接 1.验证：确保class文件内容不会危害到当前虚拟机 2.准备：为类变量分配内存并设置初始值，不会为实例变量分配空间初始化，类变量分配在方法区，实例变量分配在堆空间。 3.解析：将常量池的符号引用转换为直接引用 3）初始化 执行类构造器方法（完成静态属性和静态代码块变量的赋值操作）的过程，此方法不需要定义，是javac完成的。 clinit()不同于类的构造器,他只会加载一次。若该类具有父类，JVM会保证子类的clinit()执行前，父类的clinit()已经执行完毕。 虚拟机必须保证一个类的clinit()方法在多线程下被同步加锁。 任何一个类声明以后，内部至少存在一个类的构造器 4.类加载器 JDK自带有三个类加载器：bootstrapClassLoader(引导类加载器)、ExtClassLoader(扩展类加载器)、AppClassLoader(系统类加载器)，bootstrapClassLoader是ExtClassLoader的父类加载器(并不是集成关系，是属性关系)默认加载%JAVA_HOME%/lib下的jar包和class文件ExtClassLoader是AppClassLoader的父类加载器，默认加载%JAVA_HOME%/lib/ext文件夹下的jar包和class文件AppClassLoader是自定义类加载器，负责加载classpath下的类文件 1）引导类加载器 启动类加载器 BootStrap ClassLoader 加载java核心类库，只加载java javax sun开头的类 2）系统类加载器 extends ClassLoader java ClassLoader.getSystemClassLoader(); 加载用户自定义类，父类加载器为扩展类加载器 3）拓展类加载器 extends ClassLoader java SystemClassLoader.getParent(); 4）用户自定义类加载器 extends ClassLoader 1.为什么要用户自定义类加载器？ 1隔离加载类` `修改类加载的方式` `扩展加载源` `防止源码泄露 2.用户自定义类加载器步骤？ jdk1.2之前，继承ClassLoader重写loadClass().jdk1.2之后建议重写findClass() 也可以继承URLClassLoader,避免了自己编写findClass()以及获取自己码流的方式。 3.ClassLoader 它是一个抽象类，其后所有的类加载器都继承自ClassLoader(不包括启动类加载器) 1234getParent() 返回父类加载器loadClass(String name) 返回Class实例findClass(String name) 返回Class实例defineClass(String name,byte[] b ,int off,int len) 把字节数组b中的内容转换为一个java类，返回结果为java.lang.Class类的实例 5.双亲委派机制分为向上委派和向下查找，每一个类加载器都有各自的缓存，都会记录自己加载过的类，当一个类需要加载，AppClassLoader先查找自己的缓存有没有加载过这个类，没有加载过就会调用ExtClassLoader去查找，ExtClassLoader没有加载过，就会调用bootstrapClassLoader类去加载，如果bootstrapClassLoader没有加载过，就会去他的类加载路径查找，如果没有找到ExtClassLoader就会查找他的类加载路径，向上委派就是查找缓存，查找到bootstrapClassLoader位置，向下查找就是查找类加载路径，查找到发起的类加载器为止。 java虚拟机对class文件采用的是按需加载，而且加载某个类的class文件时，java虚拟机采用的是双亲委派机制，就是把请求交由父类加载器处理，它是一种任务委派模式 1）工作原理 如果一个类加载器收到了类加载请求，他并不会自己先去加载，而是把这个请求委托给父类的加载器去执行。 如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将达到顶层的启动类加载器。 如果父加载器可以完成类加载任务，就成功返回，倘若父加载器无法完成此类加载任务，子加载器才会尝试自己去加载。 2）举例 调用JDBC接口，接口是引导类加载器加载的，但是实现类是系统类加载器加载的。 3）优点 1、安全性，避免自己写的类替换掉java核心类； 2、避免类重复加载，相同class文件不同的类加载器加载的也是两个类。 6.沙箱安全机制自定义的String类，但是在加载自定义String类的时候会率先使用引导类加载器加载，而引导类加载器在加载的过程中会先加载jdk自带的文件，报错信息说没有main方法，就是因为加载的是jdk的String，这样可以保证对Java核心API源码的保护，这就是沙箱安全机制。 14.对象分配过程1.new的对象先放在伊甸园区，此区有大小限制。 2.当伊甸园的空间填满时，程序有需要创建对象，JVM的垃圾回收器将对伊甸园区进行垃圾回收，将伊甸园区的不在被其他对象所引用的对象销毁，在加载新的对象放在伊甸园区。 3.然后将伊甸园区剩余的对象移动到幸存者0. 4.如果再次触发垃圾回收，此时上次幸存下来的放到幸存者0区的，接着再去幸存者1区。 5.啥时候去养老区呢？可以设置次数，默认是15次。-XX:MaxTenuringThreshold= (伊甸园区满了会触发ygc，将幸存者区域和伊甸园区都回收一下，但是幸存者区满了不会触发垃圾回收) 总结：针对幸存者S0，S1区的总结：复制之后有交换，谁空谁是to。关于垃圾回收，频繁在新生区收集，很少在养老区收集，几乎不在永久区或者元空间收集。 15.常用调优工具12345671.JDK命令行2.Jconsole3.VisualVM4.Jprofiler5.Java Flight Recorder6.GCViewer7.GC Easy 16.Minor GC,Major GC,fULL GCjvm在进行GC时，并非每次都对上面三个内存区域一起回收的，大部分回收都是指新生代。 针对hotSpotVM的实现，它里面的GC按照回收区域分为两大类型：一种是部分收集，一种是FullGC 1.部分收集：不是完整收集整个Java堆的垃圾回收，又分为： 1）新生代收集Minor GC：只是新生代的垃圾收集 2）老年代收集Major GC：只是老年代的垃圾收集 目前只有CMSGC拥有单独收集老年代的行为，很多时候Minor GC会和FullGC混淆使用，需要具体分辨老年代回收还是整堆回收。 3）混合收集：收集整个新生代以及部分老年代的垃圾 2.整堆收集：收集整个java堆和方法区的垃圾 17.分代式GC策略触发条件1）年轻代触发机制1.当年轻代空间不足时，就会出发minorGC，这里的年轻代指的是Eden满，Survivor满不会触发GC 2.因为java对象大多数存活时间比较短暂，所以MinorGC非常频繁，一般回收速度也比较快。 3.MinorGC会引发STW，暂停其他用户的线程，等垃圾回收结束，用户线程才恢复运行。 2）老年代GC触发机制1.指的是发生在老年代的GC，对象从老年代消失时，我们说的MajorGC和FullGC发生了。 2.出现了MajorGC，经常会伴随至少一次的MinorGC，也就是在老年代空间不足的时候，会先尝试触发minorGC，如果之后空间还是不足，则会触发MajorGC。 3.MajorGC的速度一般会比MinorGC慢10倍以上，STW时间更长。 4.如果MajorGC后，内存还是不足，就会OOM。 3）FullGC触发条件1.System.gc() 不是一定执行的。 2.老年代空间不足 3.方法区空间不足 4.通过Minor GC后进入老年代的平均大小大于老年代的可用内存。 5.Eden区，survivor0 区向survivor1区复制时，对象大小大于To Space可用区域，则把该对象转存到老年代，且老年代的可用内存小于该对象的内存就会触发GC。 Full GC是开发或者调优中尽量要避免的，这样暂停时间会短一些。 18.堆空间分代思想研究表明，堆空间百分之七十以上的对象是临时对象，其实不分代完全可以，分代的唯一理由就是优化GC性能，如果没有分代，所有对象都在一块，GC的时候要找到哪些对象没用，这样就会造成整堆扫描，而很多对象都是朝生夕死的，如果分代的话，把新创建的对象放到某一块区域，当GC时先把这块区域存储朝生夕死对象的区域进行回收，这样就会腾出很大空间。 19.对象分配原则针对不同年龄段对象分配原则如下 优先分配到伊甸园区，大对象直接分配到老年代，尽量避免程序中出现过多的大对象，长期存活的对象分配到老年代，动态对象年龄判断，如果幸存者区中相同年龄的所有对象大小的总和大于幸存者空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代，无需等到达到16. 空间分配担保： -XX：HandlePromotionFailure 20.TLAB堆空间为每个线程分配的TLAB 堆空间是线程共享区域，任何线程都可以访问到堆区中的共享资源。由于对象实例的创建在JVM中非常频繁，因此在并发环境下从堆区中划分内存空间是线程不安全的。为了避免多个线程操作同一地址，需要使用加锁等机制，进而影响分配速度。 从内存模型而不是垃圾收集的角度，对Eden区域继续进行划分，JVM为每一个线程分配了一个线程私有的缓存区域，它包含在伊甸园区内。 多线程同时分配内存时，使用TLAB可以避免一系列的非线程安全问题，同时还能提升内存分配的吞吐量，因此我们可以将这种内存分配的方式称为快速分配策略 尽管不是所有的对象实例都能够在TLAB中成功分配内存，但是JVM确实是将TLAB作为内存分配的首选。 在程序中，可以通过-XX:UseTLAB设置是否开启TLAB空间。 默认情况下，TLAB空间的内存非常小，仅仅占整个Eden空间的百分之一，当然我们可以通过选项-XX：TLABWasteTargetPercent设置Tlab空间所占用Eden空间的百分比大小。 一旦对象在TLAB空间分配内存失败时，JVM会尝试通过加锁机制确保数据操作的原子性，从而直接在Eden空间中分配内存。 21.堆空间的参数设置1234567891011-XX:+PrintFlagsInitial 查看所有的参数的默认值-XX:+PrintFlagsFinal 查看所有的参数的最终值-Xms: 初始化堆空间大小-Xmx: 最大堆空间内存-Xmn: 设置新生代的大小-XX:NewRatio 配置新生代与老年代在堆结构的占比-XX:SurvivorRatio:设置新生代中Eden和s0/s1空间的比例-XX:MaxTenuringThreshold 设置新生代垃圾的最大年龄-XX:+PrintGCDetails 输出详细的GC处理日志打印gc简要信息：XX:+PrintGC -verbose:gc-XX:HandlePromotionFailure 是否设置空间分配担保 在发生MinorGC之前，虚拟机会检查老年代最大可用的连续空间是否大于新生代所有对象的总空间，如果大于，则此次GC是安全的。如果小于，虚拟机会查看空间分配担保的策略参数是否为true，如果为true，那么会继续检查老年代最大可用连续空间是否大于历次晋升到老年代的对象的平均大小。如果大于，则尝试进行老年代GC，但是这次GC依然是有风险的；如果小于，则改为进行FullGC。如果空间分配担保的策略参数为false，直接FullGC。 22.从逃逸分析角度分析对象内存分配1）堆是分配对象存储的唯一选择嘛？在java虚拟机中，对象在java堆中分配内存的，这是一个普遍的常识。但是有一个特殊的情况，那就是如果经过逃逸分析后发现，一个对象并没有逃逸出方法的话，那么就可能优化成栈上分配。这样就无需再堆上分配内存，也无须进行垃圾回收了。这是最常见的堆外存储技术。 2）逃逸分析1.如果将堆上的对象分配到栈，需要使用逃逸分析手段。 2.这是一种可以有效减少java程序中同步负载和内存堆分配压力的跨函数全局数据流分析算法。 3.通过逃逸分析，hotspot编译器能够分析出一个新的对象的引用的适用范围从而决定是否要将这个对象分配到堆上。 4.逃逸分析的基本行为就是分析对象动态作用域： 当一个对象在方法中被定义后，对象只在方法内部使用，则认为没有发生逃逸。 当一个对象在方法中被定义后，它被外部方法所引用，则认为发生逃逸。例如作为调用参数传递到其他地方中。 5.没有发生逃逸的对象，则可以分配到栈上，随着方法的执行结束，栈空间就被移除，栈空间指向堆空间对象的引用就没了，等到年轻代GC，对象就会被回收。 3）补充其实主要就是解决了循环引用，没有逃逸分析，这个对象可能一直不被回收，但是有了逃逸分析，栈帧销毁，这个对象就是垃圾了。 23.使用逃逸分析堆代码进行优化1）栈上分配将堆分配转换为栈分配，如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配。 JIT编译器在编译期间根据逃逸分析的结果，发现如果一个对象并没有逃逸出方法的话，就可能被优化成栈上分配。分配完成后，继续在调用栈内执行，最后线程结束后，栈空间被回收，局部变量对象也被回收。这样就无需进行垃圾回收了。 常见的栈上分配场景：在逃逸分析中，已经说明了。分别是给成员变量赋值，方法返回值，实例引用传递。 1-XX:+DoEscapeAnalysis //默认开启 2）同步省略如果一个对象被发现只能从一个线程被访问到，那么这个对象的操作可以不考虑同步。线程同步的代价是相当高的，同步的后果是降低并发和性能。在动态编译同步代码块的时候，jit编译器可以借助逃逸分析来判断同步块所使用的锁的对象是否只能被一个线程访问而没有被发布到其他线程，如果没有，那么JIT编译器在编译这个同步块的时候就会取消对这部分代码的同步。这样就能大大提高并发性能，这个取消的性能就叫做同步省略，也叫锁消除。 3）分离对象或标量替换有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分可以不存储在内存，而是存储在CPU寄存器中 1.标量是指一个无法在分解成更小的数据的数据，Java中的原始数据类型就是标量。相对的，那些还可以分解的数据叫做聚合量，java中的对象就是聚合量，因为他可以分解成其他聚合量和标量。在JIT阶段，如果经过逃逸分析，发现一个对象不会被外界访问的话，那么经过JIT优化，就会把这个对象拆解成若干个其中包含的若干个成员变量来代替。这个过程就是标量替换。 2.标量替换的参数设置 参数-XX：+EliminateAllocations开启了标量替换，允许将对象打散分配在栈上。 3.逃逸分析技术并不成熟，其根本原因就是无法保证逃逸分析的性能消耗一定高于他的消耗。虽然经过逃逸分析可以做标量替换，栈上分配和锁消除。但是逃逸分析自身也是需要进行一系列复杂的分析的，这其实也是一个相对耗时的过程，虽然并不成熟，但是他也是即时编译器优化技术中一个十分重要的手段。 通过逃逸分析，jvm会在栈上分配那些不会逃逸的对象，这在理论上是可行的，但是取决于jvm设计者的选择。HotSpot虚拟机中并未这么做，所以可以明确所有的对象实力都是创建在堆上。 intern字符串的缓存和静态变量曾经都被分配到永久带上，而永久代已经被元数据区取代。但是，intern字符串缓存和静态变量并不是被转移到元数据区，而是直接在堆上分配 ，所以这一点同样符合前面的一点结论：对象实例都是分配在堆上。 24.栈，堆，方法区的交互关系 1234Person person=new Person();Person这个运行时的大Class实例放在方法区person这个在java虚拟机栈的某个栈帧的局部变量表new Person();结构在堆中。 25.方法区的理解尽管所有方法区在逻辑上是属于堆的一部分，但一些简单的实现可能不会选择去进行垃圾回收或者压缩，对于hotspot而言，方法区还有一个别名叫非堆，目的就是要和堆分开。所以方法区可以看做是一块独立于java堆的内存空间。方法区与堆空间一样，是多线程共享的，方法区在jvm启动的时候被创建，并且他的实际物理内存空间中和java堆区一样都可以是物理上不连续的。方法区的大小和堆一样可以选择固定大小和扩展。方法区的大小决定了系统可以保存多少个类，如果系统定义了太多的类，导致方法区溢出，虚拟机同样会OOM。关闭jvm就会释放这个区域的内存。 加载大量第三方jar tomcat部署的工程太多 大量动态的生成反射类 关闭jvm就会释放这个区域的内存 26.设置方法区大小的参数方法区的大小不必是固定的，jvm可以根据应用需要动态调整。 1.元数据区大小可以使用参数-XX:MetaspaceSize和-XX:MaxMetaspaceSize指定，替代上述原有的两个参数。 2.默认值依赖于平台，win下，元空间默认大小是21M，最大值是-1，代表没有限制。 3.与永久代不同，默认情况下，如果指定大小，虚拟机会耗尽所有的可用系统内存。如果元数据去发生溢出，虚拟机一样会OOM。 4.-XX:MetaspaceSize：设置初始元空间大小。这就是一个水平线，达到这个线就会触发FullGC，如果设置小了就会频繁GC，所以尽量设置大点，每次GC，会卸载没用的类(即对应的类加载器不在存活)，然后这个水平线将会被重置。新的水位线取决于GC释放了多少空间。如果释放空间不足，那么在不超过最大值时，他会适当调高，如果释放空间过多，则适当降低该值。 27.如何解决OOM1.要解决OOM异常或heap space的异常，一般的手段是首先通过内存映像分析工具堆dump出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，也就是要先分清楚到底是出现了内存泄漏还是内存溢出。 2.如果是内存泄漏，可以进一步通过工具查看泄露对象到GC ROOTS的引用链，于是就能找到泄露对象到底是通过怎么样的路径与GC ROOTS相关联导致垃圾收集器无法自动回收他们的。掌握了泄露对象的类型信息，以及GC ROOTS引用链的信息，就可以比较准确的定位出泄露代码的位置。 3.如果不存在内存泄漏，换句话说就是内存中的对象却是还必须存活着，那就应当检查虚拟机堆参数，与机器物理内存对比看是否还可以调大，从代码上检查是否存在某些生命周期过长，持有状态时间过长的情况，尝试减少程序运行期的内存消耗。 28.方法区的内部结构 他用于存储已经被虚拟机加载的类型信息，常量，静态变量，即时编译器编译后的代码缓存等。 1）类型信息对每个加载的类型，jv必须在方法区存储一下类型信息。 全限定类名，直接父类的全限定类名，这个类型的修饰符，这个类型直接接口的一个有序列表 2）域信息jvm必须在方法区中保存类型的所有域的信息以及域的声明顺序。 域的相关信息包括：域名称，域类型，域修饰符。 3）方法信息jvm必须保存所有方法的以下信息，同域信息一样包括声明顺序 方法名称，返回类型，参数的数量和类型，顺序，方法的修饰符，方法的字节码，操作数栈，局部变量表以及大小，异常表 每个异常处理的开始位置，结束位置，代码处理在程序计数器中的偏移地址，被捕获的异常类的常量池索引。 12345678910111213141516171819202122232425262728/** * @author yinhuidong * @createTime 2020-08-20-12:09 * 1.静态变量和类关联在一起，随着类的加载而加载，他们成为类数据在逻辑上的一部分。 * 2.类变量被类的所有实例共享，即使没有实例时你也可以放问他。 * 3.全局常量 static final * 被声明为final的类变量的处理方法则不同，每个全局常量在编译的时候就会被分配了。 *图：final-static * javap -v Order.class * javap -v -p DemoE.class &gt; DemoE.txt */public class DemoD &#123; public static void main(String[] args) &#123; Order order=null; order.hello(); &#125;&#125;class Order&#123; public static int a=1; public static final int b=2; static &#123; a=3; &#125; public final static void hello()&#123; System.out.println(&quot;a = &quot; + a+&quot;----&quot;+&quot;b = &quot; + b); System.out.println(&quot;hello&quot;); &#125;&#125; 4）class文件中常量池的理解方法区内部包含了运行时常量池，字节码文件内部包含了常量池。 为什么需要常量池？ java中的字节码需要数据支持，通常这种数据会很大以至于不能直接存到字节码里，换另一种方式，可以存到常量池，这个字节码包含了指向常量池的引用，在动态链接的时候就会用到运行时常量池。 一个有效的字节码文件中除了包含类的版本信息，字段，方法以及接口等描述信息外，还包含一项信息那就是常量池表，包含各种字面量和对应类型，域和方法的符号引用。 总结：常量池，可以看做是一张表，虚拟机指令根据这张常量表找到要执行的类名，方法名，参数类型，字面量等类型。 5）运行时常量池方法区的一部分，class文件的常量池被类加载器加载到运行时数据区就会在方法区生成对应的运行时常量池，JVM为每一个已经加载的类或接口都维护了一个常量池。池子中的数据就像数组一样，都是通过索引访问的，运行时常量池包括多种不同的变量，包括编译期就已经明确的数值字面量，也包括到运行期解析后才能获得的方法或者字段引用。此时不再是常量池中的符号地址了，这里换位真实地址。 运行时常量池动态性，当创建类或者接口的运行时常量池时，如果构造运行时常量池所需的内存空间超过了方法区所能提供的最大值，则会OOM。 29.图解常量池操作 123456789public class DemoE &#123; public static void main(String[] args) &#123; int x=500; int y=100; int a=x/y; int b=50; System.out.println(a+b); &#125;&#125; 1）运行时常量池运行时常量池是方法区的一部分。 常量池表用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池。 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是说，并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可以将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。 30.方法区的演进 只有hotspot才有永久代。hotspot中方法区的变化： 123jdk1.6 有永久代，静态变量存放在永久代jdk1.7 有永久代，但已经逐步‘去永久带’，字符串常量池，静态变量移除，保存在堆jdk1.8 无永久代，类型信息，字段，方法，常量保存在本地内存的元空间，但是字符串常量池，静态变量仍在堆。 永久代为什么要被元空间替换？ 以前使用虚拟机内存，现在使用本地内存 ①为永久代设置空间大小很难确定 ②对永久代调优困难 方法区的垃圾收集主要分两部分 常量池中废弃的常量和不再使用的类型 31.StringTable为什么调整位置jdk7将StringTable放到了堆空间。因为永久代的垃圾回收效率低，在full gc的时候才会触发。而full gc是老年代空间不足，永久代不足的时候才会触发。这就导致了StringTable的回收效率不高。而我们在开发的时候会有大量的字符串被创建，回收效率低，导致永久代内存不足。放到堆里，能及时回收内存。 32.静态变量放在哪里12345678910111213141516171819202122/** * @author yinhuidong * @createTime 2020-08-20-15:38 * 分析： * 1.new出来的结构，也就是对象实例都在堆空间 * 2.1.6：obj1随着DemoG的类型信息放在方法区；1.7：放在堆空间 * 3.obj2是实例变量，在堆空间 * 4.obj3在foo方法对应的栈帧的局部变量表 */public class DemoG &#123; static class Test&#123; static Order obj1=new Order(); //jdk7以前放在永久代，7开始放在堆空间 Order obj2=new Order(); //实例变量 堆空间 void foo()&#123; Order obj3=new Order(); //方法内部局部变量 栈帧里面的局部变量表 &#125; &#125; static class Order&#123; &#125;&#125; 33.方法区的垃圾回收1.一般来说，方法区的回收效果不好，特别是类型的卸载，但是有时候回收又是必要的 2.方法区的垃圾回收主要是两部分：废弃的常量和不再使用的类型 3.方法区常量池主要存放：字面量和符号引用 符号引用包括：1.类和接口的全限定类名2.字段的名称和描述符3.方法的名称和描述符 4.只要常量池中的常量没有被任何地方引用，就可以被回收 5.判断一个类是否可以被回收 1.该类所属的实例都已经被回收 2.加载该类的类加载器已经被回收 3.该类对应的Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的对象 34.创建对象的方式1）newnew（直接new ，工厂模式，构建者模式） 2）Class.forName().newInstance()反射的方式，只能调用空参构造器，权限是public 3）Constructor.newInstance(xxx)反射的方式，可以调用空参，带参的构造器，权限没要求 4）使用clone（）不调用任何构造器，当前需要实现Cloneable接口，实现clone（）；分为深克隆和浅克隆 对象嵌套 5）使用反序列化从文件，网络中获取一个对象的二进制流 6）第三方库Objenesis35.创建对象的步骤1）首先判断对应的类是否已经加载，链接，初始化2）为对象分配内存内存规整-指针碰撞 所有用过的内存在一边，空闲的内存在另一边，中间放着一个指针作为分界点的指示器，分配内存就仅仅是吧指针想空闲那边挪动一段与对象大小相等的距离罢了，如果垃圾收集器选择的是基于压缩算法的，虚拟机采用这种分配方式。一般使用带有整理过程的收集器时，使用指针碰撞。 内存不规整-空闲列表分配 如果内存不是规整的，已使用的内存和未使用的内存相互交错，那么虚拟机将采用的是空闲列表法来为对象分配内存。虚拟机维护一个列表，记录哪块内存可用，有多大，在分配的时候找到一块内存足够大的空间划分给对象实例，并更新列表上的内容。这种分配方式称为空闲列表。选择哪种分配方式由java堆是否规整决定，而java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 3）处理并发安全问题采用cas配上失败重试保证更新的原子性，每个线程预先分配一块tlab 4）初始化分配到空间所有属性设置默认值，保证对象实例字段在不赋值时可以直接使用 给对象属性赋值的步骤：显式初始化/代码块初始化 ，构造器初始化 5）设置对象头 6）执行init方法初始化属性的显示初始化，代码块初始化，构造器初始化 36.对象访问定位JVM是如何通过栈帧中的对象引用访问到其内部的对象实例呢？ 定位，通过栈上reference访问。创建对象的目的是为了使用它。对象访问方式主要有两种： 1）句柄访问 优点：引用中存储稳定句柄地址，对象被移动时只会改变句柄中实例数据指针即可，引用本身不需要被修改。 缺点：需要额外维护一个句柄，效率低。 2）直接指针(HotSpot采用) 优点：效率高 37.直接内存不是虚拟机运行时数据区的一部分，也不是jvm规范中定义的内存区域。 直接内存是在java堆外，直接向系统申请的内存空间。 来源于NIO，通过存在堆中的DirectByteBuffer操作Native内存，通常，访问直接内存的速度会优于java堆，即读写性能高，因此出于性能考虑，读写频繁的场合可能会考虑使用直接内存，java的NIO库允许java程序使用直接内存，用于数据缓冲区。 12345678910public class DemoH &#123; private static Integer BUFFER=10*1024*1024; public static void main(String[] args) &#123; Scanner sc = new Scanner(System.in); ByteBuffer byteBuffer=ByteBuffer.allocate(BUFFER); sc.next(); byteBuffer=null; System.gc(); &#125;&#125; 也可能导致OOM异常，由于直接内存在java堆外，因此它的大小不会受限于-Xmx指定的最大堆的大小，但是系统内存是有限的，java堆和直接内存的总和依然受限于操作系统能给出的最大内存。 缺点：分配回收成本高，不受JVM内存回收管理，直接内存大小可以通过MaxDirectMemorySize设置，如果不指定，默认与堆的最大值-Xmx参数值一致，简单理解java进程内存=java堆+本地内存。 38.执行引擎1.虚拟机是一个相对于物理机的概念，这两种机器都有代码执行能力，其区别是物理机的执行引擎是直接建立在处理器，缓存，指令集和操作系统层面的，而虚拟机的执行引擎是由软件自行实现的，因此可以不受物理条件制约的定制指令集与执行引擎的结构体系，能够执行那些不被硬件直接支持的指令集格式。 2.jvm的主要作用负责装在字节码到其内部，但是字节码不能直接运行在操作系统之上，执行引擎就是将字节码指令编译为对应平台上的本地机器指令。 3.外观上看：jvm的执行引擎输入输出都是一致的，输入的是字节码二进制流，处理过程是字节码解析执行的过程，输出的是执行结果。 39.java代码的编译和执行过程 大部分的程序代码转换为物理机的目标代码或虚拟机能执行的指令集之前，都需要经过上图的步骤。 为什么java称为半解释型半编译型语言？ 因为jvm的执行引擎是解释器和jit即时编译器交互工作的。 什么是解释器？什么是JIT编译器？ 解释器：将字节码指令逐行翻译成机器指令并执行 编译器：将源代码直接编译成对应的机器指令。 40.机器码，指令，汇编语言 1.机器码：二进制编码方式表示的机器指令 2.指令，指令集：将机器中特定的0和1简化成对应的指令。每个平台所支持的指令就叫做指令集 3.汇编语言：用助记符代替机器指令的操作码，所以汇编编写的程序还需要翻译成机器指令码 4.高级语言：需要把程序解释和编译成机器的指令码 5.字节码：一种中间状态的二进制文件，实现特定软件运行和软件环境，与硬件环境无关 41.解释器 解释器其实就是一个运行时翻译者，将字节码文件中的内容翻译为对应平台的本地机器指令执行。当一条字节码指令被解释执行完成后，接着再根据PC寄存器中记录的下一条需要被执行的字节码指令执行解释操作。 解释器分类 1.字节码解释器：纯软件代码模拟字节码的执行 2.模板解释器：将每一条字节码指令和一个模板函数相关联，模板函数中直接产生这条字节码执行时的机器码 基于解释器执行已经沦落为低效的代名词，JIT编译的目的是为了避免函数被解释执行，而是将整个函数体编译成为机器码，每次函数执行，只执行编译后的机器码即可。 42.JIT即时编译器HotSpot采用解释器与即时编译器共存的架构。由JVM决定何时使用哪种方式执行。 解释器可以边解释边执行，这样程序启动时间就会变快，及时编译器是都编译好了在执行，程序启动时间慢。但是一旦jit编译器把越来越多的代码编译成本地代码，执行效率立马起飞。 1）热点代码以及探测方式判断是否启动jit编译器将字节码直接编译为对应平台的本地机器指令，需要根据执行的频率而定。热点代码就是需要被编译为本地代码的字节码，jit在运行时会针对频繁调用的热点代码直接编译为对应平台的本地机器指令，提升性能。 2）OSR编译一个方法被多次调用，或者一个方法体内部循环次数较多的循环体都可以被称为热点代码，因此都可以通过jit编译器编译为本地机器指令，也就是栈上替换.hotspot采用的热点探测方式是基于计数器的热点探测。 为每个方法建立2个不同类型的计数器，分别为方法调用计数器和回边计数器 1.方法调用计数器：统计方法调用次数，默认client模式下1500，server模式下100002.回边计数器：统计循环执行次数 当一个方法被调用时，先判断有没有jit编译过，有的话直接使用jit编译后的机器指令，没有的话计数器+1，然后判断两个计数器之和是否超过方法调用计数器的阈值。如果超过，就会向jit发出即时编译申请。 热度衰减是在虚拟机进行垃圾回收的时候顺便进行的，也就是在一段时间方法一直没有执行，计数器的计数就会减半。 可以自己手动设置虚拟机采用哪种编译模式 3）JDK9引入了AOT编译器aot编译器在程序执行之前，就将字节码转换为机器码过程。 好处：可以直接运行，不必预热。 坏处：由于提前编译成了机器指令，无法实现java一次编译到处运行。 4）JDK10的Graal编译器全新的即时编译器，实验阶段，需要手动开启，前景大好 43.StringTable44.String的基本特征String字符串使用一对引号引起来表示 1234String s1=“yhd”;String s2=new String(“yhd”);jdk8:private final char value[];jdk9:private final byte value[]; String声明为final，不可被继承。 String类实现了序列化接口，可以跨jvm进行传输，实现了Conparable接口，支持比较大小。 String代表不可变的字符序列，简称不可变性。 1）当对字符串重新赋值，需要重新指定内存区域赋值，不能使用原有的value值。 2）当对现有的字符串进行连接操作时，也需要重新指定内存区域赋值，不能使用原有value赋值。 3）当调用String的replace方法修改指定的字符或者字符串时，也需要重新指定内存区域赋值。 一方面数组的长度一旦确定了，就不能再改变，一方面存储在字符串常量池的数据不可发生变化。 通过字面量的方式给一个字符串赋值，此时的字符串值声明在字符串常量池。 字符串常量池不会存储两个相同内容的字符串。 String的底层是一个固定大小的hashtable，默认长度是1009，如果放入StringPool的String特别多，就会造成Hash冲突严重，从而导致链表会很长，而链表长了会影响String.intern的性能。 使用-XX:StringTableSize设置StringTable的长度。 jdk6 StringTable长度固定为1009，字符串多就会造成性能下降。 jdk7 StringTable的长度默认值是60013,jdk8开始1009是可设置的最小值。 45.String内存的分配常量池就类似一个Java系统级别提供的缓存。8种基本数据类型的常量池都是系统协调的，String类型的常量池比较特殊。它的主要使用方法有两种： 1）直接使用双引号声明出来的String对象会直接存储在常量池中 2）如果不是用双引号声明的String对象，可以使用String提供的intern()方法。 所有的字符串都保存在堆中，这样调优的时候仅仅需要调整堆的大小就可以了。 StringTable为什么要调整？ 1）permSize默认比较小 2）永久代垃圾回收频率低 46.String的基本操作java语言规范里要求完全相同的字符串字面量，应该包含同样的Unicode字符串序列，并且必须是指向同一个String类实例。 47.字符串拼接1）常量与常量的拼接结果在常量池，原理是编译期优化 2）.常量池中不会存在相同内容的常量 3）只要其中有一个是变量，结果就在堆中。变量拼接的原理是StringBuilder 4）如果拼接的结果调用intern()方法，则主动将常量池中还没有的字符串对象放入池中，并返回此对象的地址。 5）字符串拼接底层原理分析 1234567891011121314public static void main(String[] args) &#123; String s1=&quot;a&quot;; String s2=&quot;b&quot;; String s3=&quot;ab&quot;; String s4=s1+s2; System.out.println(s3==s4); /** * 如下的s1+s2的执行细节： * ① StringBuilder s= new StringBuilder(); * ② s.append(&quot;a&quot;); * ③ s.append(&quot;b&quot;); * ④ s.toString(); //类似于new String(&quot;ab&quot;); */ &#125; 12345678910111213141516public class DemoA &#123; /** * 1.字符串拼接操作不一定使用的是StringBuilder! * 如果拼接符号左右两边都是字符串常量或常量引用。则仍然使用编译器优化，即非 * StringBuilder的方式。 * 2.针对于final修饰类，方法，基本数据类型，引用数据类型的结构时，能使用上final * 的时候建议使用上。 * @param args */ public static void main(String[] args) &#123; final String s1=&quot;a&quot;; final String s2=&quot;b&quot;; String s3=&quot;ab&quot;; String s4=s1+s2; System.out.println(s3==s4);//true &#125; 12345拼接操作和append操作的效率对比StringBuilder s= new StringBuilder();//每次直接编译器优化s.append(&quot;a&quot;);src+=&quot;a&quot;;//每次都会创建一个新的StringBuilder 而且还会new String()总结：实际开发，尽量少用空参的list和StringBuilder，防止不断扩容。 48.intern（）的使用intern()方法就是确保字符串在内存中只有一份，这样可以节约内存空间，加快字符串操作任务的执行速度，这个值会被存放在字符串内部池。 如何保证变量s指向的是字符串常量池的数据呢？ 12345方式1：String s=“shkstart”;//字面量定义的方式方式2：String s=new String(“shkstart”).intern();String s=new StringBuilder(“shkstart”).toString().intern(); 1String a=new String(“a”);//底层创建了两个对象，一个在堆空间，一个在字符串常量池 1234567891011121314new String(“a”)+new String(“b”);//创建了几个对象/* 对象1：new StringBuilder(); 对象2：new String(“a”); 对象3：常量池中的a 对象4：new String(“b”); 对象5：常量池中的b 对象6：new Sring(“ab”);*/ toString()的调用，在字符串常量池中，没有生成”ab”。 1234567891011121314151617181920212223void contextLoads() &#123; String str=new String(&quot;1&quot;); //str指向堆空间的引用地址 String intern = str.intern(); //intern 指向字符串常量池的引用地址 String str2=&quot;1&quot;; //指向常量池的地址 System.out.println(str==str2);//false System.out.println(intern==str2);//true String str3=new String(&quot;1&quot;)+new String(&quot;1&quot;); //str3指向堆空间 //new StringBuilder().append().append().toString(); String intern1 = str3.intern();//intern1指向字符串常量池 String str4=&quot;11&quot;; // 常量池 System.out.println(str3==str4);//false System.out.println(intern1==str4);//true String str5=new String(&quot;a&quot;)+new String(&quot;a&quot;);//str5指向堆空间 String str6=&quot;aa&quot;; //str6常量池 String intern2 = str5.intern(); //intern2指向常量池的副本地址 System.out.println(str5==str6);//false System.out.println(str6==intern2); //true &#125; 总结String的intern()的使用： jdk1.6中，将这个字符串对象尝试放入字符串常量池。 1）如果字符串常量池有，则并不会放入。返回已经有的字符串常量池中的对象的地址。 2）如果没有，会把此对象复制一份，放入串池，并返回串池中的对象地址。 jdk1.7起，将这个字符串对象尝试放入字符串常量池。 1）如果字符串常量池有，则并不会放入。返回已经有的字符串常量池中的对象的地址。 2）如果没有，则会把对象的引用地址复制一份，放入串池，并返回字符串常量池中的引用地址，所以相当于指向的是对象的引用。 练习 123456789@Testpublic void test1()&#123; String s=new String(&quot;a&quot;)+new String(&quot;b&quot;);//堆 //执行完上一行代码，字符串常量池中并没有 ab String s2=s.intern();//6：字符串常量池 8：没有创建字符串，而是创建一个引用指向堆中的ab //ab 常量池 System.out.println(s2==&quot;ab&quot;); //8：true 6：true System.out.println(s==&quot;ab&quot;); //8：true 6：false&#125; 12345678910@Testpublic void test1()&#123; String x=&quot;ab&quot;; //常量池 String s=new String(&quot;a&quot;)+new String(&quot;b&quot;);//堆 String s2=s.intern();//6：字符串常量池 8：指向常量池ab的引用 //ab 常量池 System.out.println(s2==&quot;ab&quot;); //8：true 6：true System.out.println(s==&quot;ab&quot;); //8：false 6：false&#125; 12345678@Testpublic void test2()&#123; String s1=new String(&quot;ab&quot;); //会在字符串常量池生成 ab，但是str指向的是堆 String s=new String(&quot;a&quot;)+new String(&quot;b&quot;); //不会再字符串常量池生成 ab s1.intern(); String s2=&quot;ab&quot;; //常量池 System.out.println(s1==s2); //false&#125; 开发中推荐使用intern（），节省内存空间。 49.StringTable的垃圾回收1234//-Xms15m -Xmx15m -XX:+PrintStringTableStatistics -XX:+PrintGCDetailspublic static void main(String[] args) &#123; IntStream.range(0, 100).forEachOrdered(i -&gt; String.valueOf(i).intern());&#125; 50.G1中的String去重操作12String s1=new String(&quot;a&quot;);String s2=new String(&quot;a&quot;); 此时s1和s2在堆空间new了两个一模一样的对象，两个对象在同时指向字符串常量池的同一个字符串a，此时G1就会删除一个堆空间的对象，让s1和s2都指向同一个堆空间的对象。 实现 1）当垃圾收集器工作的时候，会访问堆上存活的对象。对每一个访问的对象都会检查是否是候选的要去重的String对象。 2）如果是，把这个对象的一个引用插入到队列中等待后续的处理。一个去重的线程在后台运行，处理这个队列。处理队列的一个元素意味着从队列删除这个元素，然后尝试去重它引用的String对象。 3）使用一个hashtable来记录所有的被String对象使用的不重复的char数组。当去重的时候，会检查这个hashtable，来看堆上是否已经存在一个一模一样的char数组。 4）如果存在，String对象会被调整引用那个数组，释放对原来的数组的引用，最终会被垃圾收集器回收掉。 5）如果查找失败，char数组会被插入到hashtable，这样以后的时候就可以共享这个数组了。 命令行选项 123UseStringDeduplication //开启String去重，默认不开启PrintStringDeduplicationStatistics //打印详细的去重统计信息StringDeduplicationAgeThreshold //打到这个年龄的String对象被认为是去重的候选对象 51.什么是垃圾？在ｊｖｍ进行垃圾回收之前，会先判断哪些对象是垃圾，也就是说，要判断哪些对象可以被销毁了，其占有的空间是可以被回收的。根据ｊｖｍ的架构划分，ｊａｖａ中几乎所有的对象实例都在堆空间中存放，所以垃圾回收也主要是针对堆空间进行垃圾回收。 在ｊｖｍ眼中，垃圾就是指那些在堆空间中存在的，已经死亡的对象，而对于死亡的定义，我们可以简单的将他理解为不可能再被任何途径使用的对象。那怎么才能确定一个对象是存活还是死亡呢？这就涉及到了垃圾判断算法，其主要包括引用计数法和可达性分析算法。 52.为什么需要GC？对于高级语言来讲，如果不进行GC，内存迟早会消耗殆尽，除了释放没用的对象，垃圾回收也可以清理内存里的记录碎片。碎片整理将所占用的堆内存移动到堆的一端，以便JVM将整理出的内存分配给新的对象，没有GC不能保证程序的正常运行。 53.早期的垃圾回收new：申请内存 delete：释放内存 优点：灵活控制内存释放的时间 缺点：频繁申请和释放内存的管理负担，一旦忘记释放，可能会引发内存泄漏从而导致内存溢出，使程序崩溃。 54.Java垃圾回收机制1）自动内存管理，降低内存泄漏和内存溢出的风险 2）使程序员更专注与业务代码的开发 55.垃圾判断算法１）引用计数法在这种算法中，假设堆中每个对象都有一个引用计数器。当一个对象被创建并且初始化赋值以后，对象的计数器就会设置为１，每当有一个地方引用他，计数器的值就会＋１，例如将对象Ｂ赋值给对象Ａ，那么Ｂ被引用，Ｂ的引用计数器就会＋１. 反之，当引用失效的时候，比如一个对象的某个引用被设置了新的值，则之前被引用的对象的计数器就会－１.而那些引用计数为０的对象，就可以称之为垃圾，可以被收集。 特别的，当一个对象被当做垃圾收集时，他引用的任何对象的计数器的值都－１. 优点：实现简单，对程序不被长时间打断的实时环境比较有利 缺点：需要额外的空间来存储计数器，难以检测对象之间的循环依赖 java并没有选择引用计数，是因为其存在一个基本难题，也就是很难处理循环引用关系。 Python如何解决循环引用？手动解决（在合适的时机，接触引用关系）使用弱引用weakref（weakref是Python提供的标准库，为了解决循环依赖） ２）可达性分析算法可达性是指，如果一个对象会被至少一个在程序中的变量通过直接或间接的方式被其他可达的对象引用，则称该对象就是可达的。 对象成为可达对象的两个条件： 对象属于跟集中的对象 对象被一个可达的对象引用 在java语言中，GC ROOTS 包括以下几类元素： 虚拟机栈中引用的对象（各个线程被调用的方法中使用到的参数，局部变量等） 本地方法栈内JNI（通常说的本地方法）引用的对象 方法区中类静态属性引用的对象（java类的引用类型静态变量） 方法区中常量引用的对象（字符串常量池（String Table）里的引用） 所有被同步锁synchronized持有的对象 java虚拟机内部的引用（基本数据类型对应的class对象，一些常驻的异常对象，系统类加载器） 本地代码缓存 临时的（分代收集，局部回收） 如何判断一个root 由于root采用栈方式存放变量和指针，所以如果一个指针，他保存了堆内存里面的对象，但是自己又不存放在堆内存里面，那他就是一个root。 如果要使用可达性分析算法来判断内存是否可回收，那么分析工作必须在一个能保障一致性的快照中进行。这点不满足的话分析结果的准确性就无法保证。这点也是导致GC进行时必须Stop the World的一个重要原因。 优点：可以解决循环引用的问题，不需要占用额外的空间 缺点：多线程场景下，其他线程可能会更新已经访问过的对象的引用。 56.对象的finalization机制java语言提供了对象终止机制来允许开发人员提供对象被销毁之前的自定义处理逻辑。当垃圾回收器发现没有引用指向一个对象，即：垃圾回收此对象之前，总会先调用这个对象的finalize方法。finalize方法允许在子类中被重写，用于在对象被回收时进行资源释放。通常在这个方法中进行一些资源释放和清理的工作。 应该交给垃圾回收机制调用（永远不要主动调用某个对象的finalize方法） 1.在finalize时可能会导致对象复活 2.finalize（）方法的执行时间是没有保障的，他完全由GC线程决定，极端情况下，若不发生GC，则finalize（）将没有执行的机会。 3.一个糟糕的finalize（）会严重影响GC的性能。 从功能上来说，finalize（）与c中的析构函数比较相似，但是java采用的是基于垃圾回收期的自动内存管理机制，所以finalize（）方法在本质上不同于c中西沟函数。 由于finalize（）方法的存在，虚拟机中的对象一般处于三种可能的状态。 可触及的：从根节点开始，可以到达这个对象。 可复活的：对象的所有引用都被释放，但是对象有可能在finalize（）中复活。 不可触及的：对象的finalize（）方法被调用，并且没有复活，那么就会进入不可触及状态。不可触及的对象不可能被复活，因为finalize（）只会被调用一次。 判定一个对象是否可回收，至少需要经历两次标记过程 1）如果该对象到GC Roots没有引用链，则进行第一次标记。 2）进行筛选，判断该对象是否有必要执行finalize() ①如果对象没有重写该方法，或者该方法已经被虚拟机调用过，则虚拟机认为没有必要执行，该对象为不可触及 ②如果该对象重写了该方法，且没有执行过，那么对象会被插入到一个队列中，由一个虚拟机自动创建的，低优先级的Finalizer线程触发finalize（）执行。 ③finalize()是对象逃脱死亡的最后机会，GC线程会对队列中的对象进行第二次标记，如果对象在finalize()中与引用链上的任何一个对象建立了联系，那么在第二次标记时，对象会被移出即将回收集合。之后，对象会再次出现没有引用存在的情况，在这个情况下，finalize()不会被再次调用，对象会直接变成不可触及的状态，也就是说，一个对象的finalize()只会被调用一次。 57.使用MAT查看GC RootMAT是一款功能强大的java堆内存分析器，用于查找内存泄漏以及内存消耗情况。 1）获取dump文件①命令行使用jmap12345678910C:\\Users\\ASUS\\Ideaproject\\project03\\gc&gt;jps16912 Launcher5056 RemoteMavenServer3613636 Jps14876 GetDump15132C:\\Users\\ASUS\\Ideaproject\\project03\\gc&gt;jmap -dump:format=b,live,file=test1.bin 14876Dumping heap to C:\\Users\\ASUS\\Ideaproject\\project03\\gc\\test1.bin ...Heap dump file created ②JvisualVM 2）使用MAT分析dump文件①查看GC Roots 58.使用Jprofiler进行GCRoots淑源59.使用Jprofiler分析OOM1-XX:+HeapDumpOnOutOfMemoryError //发生OOM时生成dump文件日志 60.标记清除算法当成功区分出内存中存活对象和死亡对象后，GC接下来的任务是执行垃圾回收，释放掉无用对象所占用的内存空间，以便有足够的可用内存空间为新对象分配内存。目前在JVM中比较常见的三种垃圾收集算法是标记-清除算法，复制算法，标记-压缩算法。 当堆中的有效内存空间被耗尽时，就会停止整个程序，然后进行两项工作，第一项是标记，第二项则是清除。 标记：从引用跟节点开始遍历，标记所有被引用的对象。一般是在对象的Header中记录为可达对象。 清除：对堆内存从头到尾进行线性遍历，如果发现某个对象在其header中没有标记为可达对象，则将其回收。 优点：不需要进行对象的移动，并且仅对不存活的对象进行处理，存活对象比较多的情况下极为高效。 缺点：标记和清除过程的效率都不高，这种方法需要使用一个空闲列表来记录所有的空闲区域以及大小，对空闲列表的管理会增加分配对象时的工作量；标记清除后会产生大量不连续的内存碎片，虽然空闲区域的大小是足够的，但却可能没有一个单一的区域能满足这次分配所需大小，分配还会失败，不得不触发再一次的垃圾回收。 何为清除：所谓清除，并不是并不是真的置空，而是把需要清除的对象地址保存在空闲的地址列表里，下次有新的对象需要加载时，判断垃圾的位置空间是够够，够就存放。 61.标记整理算法算法标记的过程与标记清除算法中的标记过程一样，但是对标记后出的垃圾对象的处理情况有所不同，他不是直接对可回收对象进行清理，而是让所有的对象都像一端移动，然后直接清理掉端边界以外的内存。在基于标记整理算法的收集齐实现中，一般增加句柄和句柄表。 优点：经过整理之后，新对象的分配只需要通过指针碰撞便能完成，比较简单；使用这种方法，空闲区域的位置是始终可知的，也不会再有碎片的问题了。 缺点：ＧＣ暂停的时间会增长，因为你需要将所有的对象都拷贝到一个新的地方，还得更新他们的引用地址。 62.复制算法复制算法主要是为了克服句柄的开销和解决堆碎片的垃圾回收。它将内存按照容量分为大小相等的两块，每次只使用其中的一块，当这一块的内存用完了，就将还活着的对象复制到另一块内存上面（空闲面），然后再把已使用过的内存空间一次清理掉。 复制算法比较适合于新生代（短生存期的对象），在老年代（长生存期的对象）中，对象存活率比较高，如果执行较多的复制操作，效率将会变低。所以老年代一般会选用其他算法，如标记整理算法。一种典型的基于复制算法的垃圾回收是stop-and-copy算法，他将堆分为对象区和空闲区，在对象区与空闲区的切换过程中，程序暂停执行。 优点：标记阶段和复制阶段可以同时进行，每次只对一块内存进行回收，运行高效，只需要移动栈顶指针，按顺序分配内存即可，实现简单，内存回收时，不用考虑内存碎片的出现。 缺点：需要一块能容纳下所有存活对象的额外的内存空间。因此，可一次性分配的最大内存缩小了一半。 63.分代收集算法将堆内存划分为新生代，老年代和永久代。新生代又被进一步划分为伊甸园区和幸存者0，幸存者1区。所有通过new创建的对象的内存都在堆中分配，其大小可以通过-Xms和-Xmx来控制。分代收集，是基于这样一个事实：不同的对象的生命周期是不一样的。因此可以将不同生命周期的对象分代，不同的代采取不同的回收算法进行垃圾回收，以便提高回收效率。 新生代：几乎所有新生成的对象首先都是放在年轻代的。新生代内存按照 8:1:1 的比例分为一个 Eden 区和两个 Survivor（Survivor0，Survivor1）区。大部分对象在 Eden 区中生成。当新对象生成，Eden 空间申请失败（因为空间不足等），则会发起一次 GC（Scavenge GC）。回收时先将 Eden 区存活对象复制到一个 Survivor0 区，然后清空 Eden 区，当这个 Survivor0 区也存放满了时，则将 Eden 区和 Survivor0 区存活对象复制到另一个 Survivor1 区，然后清空 Eden 和这个 Survivor0 区，此时 Survivor0 区是空的，然后将 Survivor0 区和 Survivor1 区交换，即保持 Survivor1 区为空， 如此往复。当 Survivor1 区不足以存放 Eden 和 Survivor0 的存活对象时，就将存活对象直接存放到老年代。当对象在 Survivor 区躲过一次 GC 的话，其对象年龄便会加 1，默认情况下，如果对象年龄达到 15 岁，就会移动到老年代中。若是老年代也满了就会触发一次 Full GC，也就是新生代、老年代都进行回收。新生代大小可以由-Xmn来控制，也可以用-XX:SurvivorRatio来控制 Eden 和 Survivor 的比例。 老年代：在新生代中经历了 N 次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。内存比新生代也大很多（大概比例是 1:2），当老年代内存满时触发 Major GC 即 Full GC，Full GC 发生频率比较低，老年代对象存活时间比较长，存活率高。一般来说，大对象会被直接分配到老年代。所谓的大对象是指需要大量连续存储空间的对象，最常见的一种大对象就是大数组。当然分配的规则并不是百分之百固定的，这要取决于当前使用的是哪种垃圾收集器组合和 JVM 的相关参数。 永久代：用于存放静态文件（class类、方法）和常量等。永久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如 Hibernate 等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。对永久代的回收主要回收两部分内容：废弃常量和无用的类。永久代在 Java SE8 特性中已经被移除了，取而代之的是元空间（MetaSpace），因此也不会再出现java.lang.OutOfMemoryError: PermGen error的错误了。 特别的，在分代收集算法中，对象的存储具有以下特点： 1.对象优先在伊甸园区分配 2.大对象直接进入老年代 3.长期存活的对象将进入老年代，默认为15岁 对于晋升老年代的年龄阈值，为什么是15岁？ 实际上，HotSpot虚拟机的对象头其中一部分用于存储对象自身的运行时数据，如哈希码，GC分代年龄，锁状态标志，线程持有的锁，偏向线程ID，偏向时间戳等，这部分数据的长度在32位和64位的虚拟机中分别为32bit和64bit，官方称为mark word。在 32 位的 HotSpot 虚拟机中，如果对象处于未被锁定的状态下，那么Mark Word的 32bit 空间中 25bit 用于存储对象哈希码，4bit 用于存储对象分代年龄，2bit 用于存储锁标志位，1bit 固定为 0，其中对象的分代年龄占 4 位，也就是从0000到1111，而其值最大为 15，所以分代年龄也就不可能超过 15 这个数值了。 GC的分类 新生代GC Minor GC ：发生在新生代的垃圾收集动作，因为java对象大多具有朝生夕灭的特性，因此MinorGC非常频繁，一般回收速度也比较快。在新生代中，每次垃圾收集时都会发现有大量对象死去，只有少量存活，因此可以选用复制算法。 老年代GC Major GC：发生在老年代的垃圾回收动作。Major GC 经常会伴随至少一次Minor GC。由于老年代中的对象的生命周期比较长，因此Major GC并不频繁，一般都是等待老年代满了之后才进行Full GC，而且其速度一般会比Minor GC慢10倍以上。另外，如果分配了Direct Memory，在老年代中进行 Full GC 时，会顺便清理掉 Direct Memory 中的废弃对象。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清除”算法或“标记-整理”算法来进行回收。新生代采用空闲指针的方式来控制 GC 触发，指针保持最后一个分配的对象在新生代区间的位置，当有新的对象要分配内存时，用于检查空间是否足够，不够就触发 GC。当连续分配对象时，对象会逐渐从 Eden 到 Survivor，最后到老年代。 64.增量收集算法上述现有的算法，在垃圾回收过程中，应用软件将处于一种STW的状态，在该状态下，应用程序所有的线程都会挂起，暂停一切正常的工作，等待垃圾回收完成。如果垃圾回收时间过长，应用程序会被挂起很久，将严重影响用户体验或者系统稳定性。为了解决这个问题，即对实时垃圾收集算法的亚久直接导致增量收集算法的产生。 如果一次性将所有的垃圾进行处理，需要造成系统长时间的停顿，那么就可以让垃圾收集线程 和应用程序线程交替执行。每次，垃圾收集线程值收集一小片区域的内存空间，接着切换到应用程序线程，依次反复，一直到垃圾收集完成。 总的来说，增量收集算法的基础仍然是传统的标记-清除和复制算法。增量收集算法通过对线程间冲突的妥善处理，允许垃圾收集线程以分阶段的方式完成标记，清理或复制工作。 缺点：线程切换和上下文的转换消耗性能，会使得垃圾回收的总体成本上升，造成系统吞吐量的下降。 65.分区算法分代算法将按照对象的生命周期长短划分成两个部分，分区算法将整个堆空间划分成连续的不同的小空间。每一个小空间都独立使用，独立回收。这种算法的好处是可以控制一次回收多少个小区间。 66.System.gc()的理解在默认情况下，通过system.gc()或者Runtime.getRuntime().gc()的调用，会显式触发Full GC，同时对老年代和新生代进行垃圾回收，尝试释放被丢弃对象占用的内存。然而，System.gc()无法保证一定对垃圾收集器的调用。 12345678910public class SystemGCTest &#123; public static void main(String[] args) &#123; new SystemGCTest(); //提醒jvm的垃圾回收期进行垃圾收集，但是不一定马上进行垃圾回收 // 与Runtime.getRuntime().gc();的作用是一样的。 System.gc(); //调用finalize()方法 System.runFinalization(); &#125;&#125; 67.内存溢出与内存泄漏1）内存溢出内存溢出是相对于内存泄漏来说的，尽管更容易被理解，但是同样的，内存溢出也是引发程序崩溃的罪魁祸首之一。大多数情况下，GC会进行各种年龄段的垃圾回收实在不行了就放大招，来一次Full GC操作，这时候会回收大量内存，供应用程序继续使用。javadoc对OOM的解释是：没有空闲内存，并且垃圾收集器也无法提供更多内存。 堆内存不够的原因有两个：java虚拟机的堆内存设置不够。代码中创建了大量的大对象，并且长时间不能被垃圾收集器收集（存在被引用） OOM之前，通常垃圾收集器会先进行GC，当然也不是任何情况下垃圾收集器都会被触发：比如我们创建一个超过堆空间大小的对象。 2）内存泄漏只有对象不在被程序用到了，但是GC又不能回收他们的情况，才叫内存泄漏。实际上一些导致对象生命周期变得很长甚至OOM的操作，也称为内存泄漏。 尽管内存泄漏并不会立刻引起程序崩溃，但是一旦发生内存泄漏，程序中的可用内存就会被逐步蚕食，直至耗尽所有内存，最终出现OOM，导致程序崩溃。 这里的存储空间并不是指物理内存，而是指虚拟内存大小，这个虚拟内存取决于磁盘交换区设定的大小。 Example： 1.单例模式，单例的生命周期默认和应用程序一样长，所以单例程序中，如果持有对外部对象的引用的话，那么这个外部对象是不能被回收的，否则会导致内存泄漏的产生。 2.一些提供close的资源未关闭导致内存泄漏（数据库连接，网络连接，io连接） 68.Stop The World指的是GC事件发生过程中，会产生应用程序的停顿。停顿产生时整个应用程序线程都会被停掉，没有任何响应。 STW事件和采用哪款垃圾收集器无关，所有GC都有这个事件。它是由JVM在后台自动发起和自动完成的。 69.垃圾回收的并行与并发1）并行当系统有一个以上cpu，当一个cpu执行一个进程时，另一个cpu可以执行另一个进程，两个进程互不抢占cpu资源，可以同时进行，我们称之为并行。 其实决定并行因素不是cpu数量，而是cpu的核心数，比如一个cpu多个核也可以并行 2）并发一个cpu核心在一个时间段同时处理多个任务，某一个时间点只处理一个任务。 两者对比 并发指的是多个事情在同一时间段同时发生了。 并行指的是多个事情在同一时间点上同时发生了。 并发的多个任务之间是互相抢占资源的。 并行的多个任务之间是不互相抢占资源的。 只有在多cpu或者一个cpu多核的情况下，才会发生并行。否则，看似同时发生的事情，其实都是并发执行的。 垃圾回收的并发与并行 并行：指多条垃圾收集线程并行工作，但此时用户线程仍处于等待状态 串行：相较于并行的概念，单线程执行。如果内存不够，则程序暂停，启动垃圾回收器进行垃圾收集。回收完，在启动程序的线程。 并发：指用户线程和垃圾回收线程同时执行。垃圾回收线程在执行时不会停顿用户程序的运行。 70.安全点与安全区域1）安全点程序执行过程中并不是在所有地方都能停顿下来开始GC，只有在特定的位置才能停顿下来开始GC，这些位置称为安全点。 安全点太少可能导致GC等待时间太长，太多可能导致程序运行时的性能问题。 方法调用，循环跳转，异常跳转。 如何在GC发生时，检查所有线程都跑到最近的安全点停下来呢？ 抢先式中断（目前没有虚拟机采用了）中断所有线程，哪个没到安全点就恢复线程，让线程跑到安全点。 主动式中断设置一个中断标志，各个线程运行到安全点的时候主动轮询这个标志，如果中断标志为真，则将自己进行中断挂起。 2）安全区域安全点机制保证了程序执行时，在不太长的时间内就会遇到可进入的GC安全点，但是假如程序处于sleep状态，这时候线程无法响应jvm的中断请求，走到安全点去中断挂起，jvm也不太可能的等待线程被唤醒。对于这种情况，就需要安全区域来解决。 安全区域是指在一段代码片段中，对象的引用关系不会发生变化，在这个区域中的任何位置开始GC都是安全的 实际执行时 当线程运行到安全区域的代码时，首先标识已经进入了安全区域，如果这段时间内发生GC，JVM会忽略标识为安全区域状态的线程。 当线程即将离开安全区域时，会检查jvm是否已经完成GC，如果完成了，则继续运行，否则线程必须等待直到收到可以安全离开安全区域的信号为止。 71.引用强引用：Object obj=new Object(); 只要强引用还存在，垃圾回收器就永远不会回收掉引用的对象。 软引用：可能还有用，但并非必须的对象，系统内存不足时，这类引用关联的对象将被回收。 弱引用：被弱引用关联的对象只能存活到下一次垃圾回收。 虚引用：就是为了在这个对象被垃圾回收的时候能够获得一个系统通知。 终结器引用：用来实现对象的finalize（）无需手动编码，内部配合引用队列使用。 在GC时，终结器引用入队，由Finalizer线程通过终结器引用找到被引用对象并调用他的finalize（），第二次GC时才能回收被引用对象。 72.垃圾回收分类按照垃圾回收线程分：串行垃圾回收器和并行垃圾回收器 串行回收是指在同一时间段内只允许有一个cpu执行垃圾回收操作，此时工作线程被暂停，直至垃圾回收结束。 并行则是允许运用多个cpu同时执行垃圾回收操作。 按照工作模式分：并发式垃圾回收器和独占式垃圾回收器 并发式垃圾回收器与应用程序线程交替，尽可能减少应用程序暂停时间。 独占式垃圾回收器一旦运行，就禁止应用程序中的所有用户线程，直到垃圾回收过程完全结束。 按照碎片处理方式分类，压缩垃圾回收器和非压缩垃圾回收器 压缩式垃圾回收器会在回收完成后，对存活对象进行压缩整理，消除回收后的碎片。 非压缩式的垃圾回收器不进行这步操作。 按照工作的内存区间分：又可分为年轻代的垃圾回收器和老年代的垃圾回收器。 73.评估GC的性能指标吞吐量：运行用户代码的时间占总运行时间的比例。（总运行时间=程序运行时间+垃圾回收时间） 暂停时间：执行垃圾回收时，程序的工作线程被暂停的时间。 收集频率：相对于应用程序的执行，收集发生的频率。 内存占用：java堆区所占内存大小。 三者总体表现会随着技术进步越来越好，主要抓住两点：吞吐量和暂停时间。 如果以吞吐量优先，那么必然需要降低内存回收的执行效率，但是这样会导致GC需要更长的时间来执行内存回收。如果选择低延迟优先，为了降低每次内存回收时的暂停时间，也只能频繁的执行内存回收，但又引起了年轻代内存的缩减和导致程序吞吐量的下降。 标准：在最大吞吐量优先的情况下，降低停顿时间。 74.不同的垃圾回收器概述1）垃圾收集器发展历史1JDK1.3发布Serial GC 他是第一款GC。ParNew是Serial 的多线程版本。 1JDK1.4发布Parallel GC 和 Concurrent Mark Sweep GC。 1JDK6之后Parallel GC称为HotSpot默认垃圾回收器。 1JDK1.7引入G1。 1JDK9把G1变为默认垃圾收集器，替代CMS。 1JDK10中G1垃圾收集器的并行完整垃圾回收，实现并行性来改善最坏情况下的延迟。 1JDK11引入Epsilon GC。同时引入ZGC。 1JDK12增强G1，自动返回未使用堆内存给操作系统，同时引入Shenandoah GC。 1JDK13增强ZGC。 1JDK14删除CMS，并拓展ZGC的平台兼容性。 2）垃圾收集器分类串行回收器：Serial，Serial Old 并行回收器：ParNew，Parallel Scavenge，Parallel Old 并发回收器：CMS,Gl 新生代收集器：Serial，ParNew，Parallel Scavenge 老年代收集器：Serial Old，Parallel Old，CMS 整堆收集器：G1 为什么要有很多收集器？ 因为java使用场景很多，移动端，服务端等。所以就需要针对不同的场景，提供不同的垃圾回收器，提高垃圾收集的性能。 对垃圾收集器进行比较只是对具体应用场景选择最合适的收集器。 如何查看默认的垃圾收集器？ 12345-XX:+PrintCommandLineFlags 查看命令行相关参数（包含垃圾收集器）使用命令行指令：jinfo -flag 相关垃圾回收器参数 进程IDParallel Scavenge 和 Parallel Old 75.Serial 回收器（串行回收器）jdk1.3之前回收新生代的唯一选择（HotSpot在Client模式下的默认新生代垃圾收集器） Serial 收集器采用复制算法，串行回收和STW机制的方式执行内存回收。 除了年轻代，Serial收集器还提供用于执行老年代垃圾回收的Serial Old收集器。Serial Old 收集器同样也采用了串行回收和STW机制，只不过内存回收算法使用的是标记-压缩算法。 Serial Old是运行在Client模式下默认的老年代垃圾回收器。 Serial Old在Server模式下主要有两个用途：1.与新生代的Parallel Scavenge配合使用 2.作为老年代CMS收集器的后备垃圾收集方案。 这个收集器是一个单线程的收集器，但他的单线程的意义并不仅仅说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到他收集结束。 优势：简单而高效，对于限定单个CPU的环境来讲，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。（运行在Client模式下的虚拟机是个不错的选择） 在HotSpot虚拟机中，使用-XX:UseSerialGC 参数可以指定年轻代和老年代都是用串行收集器。（等价于新生代使用Serial GC，老年代使用Serial Old GC），一般在java web程序中是不会使用这种垃圾回收器的。 76.ParNew回收器（并行回收）如果说Serial GC是年轻代中的单线程垃圾收集器，那么ParNew收集器则是Serial收集器的多线程版本。（Par是Parallel的缩写 New只能处理新生代） ParNew收集器除了采用并行回收方式执行内存回收外，两款垃圾收集器之间几乎没有差别，也是STW，也是复制算法。 ParNew是很多JVM运行在Server模式下新生代的默认垃圾收集器。 对于新生代，回收次数频繁，使用并行高效。对于老年代，回收次数少，使用串行方式节省资源。（CPU并行需要切换线程，串行可以省去切换线程的资源） 由于ParNew收集器是并行回收，那么是否可以断定在任何场景下他的回收效率都比Serial收集器更高效？ 1.多核系统下，充分利用系统资源，可以快速完成垃圾收集，提升程序吞吐量。 2.但是单个CPU下，他不一定有Serial收集器更高效。避免了线程切换。 除了Serial 外，目前只有ParNew 能与CMS收集器配合工作。 在程序中，开发人员可以通过选项-XX:UseParNewGC手动指定使用ParNew收集器执行内存回收任务 。他表示年轻代使用并行收集器并不影响老年代。 -XX:ParallelGCThreads限制线程数量，默认开启和CPU数相同的线程数。 77.Parallel Scavenge 回收器（吞吐量优先）1）概述HotSpot的年轻代除了拥有ParNew收集器是基于并行回收的以外，Parallel Scavenge收集器同样也采用了复制算法，并行回收和STW机制。 那么他的出现是否多此一举呢？ 1.和ParNew收集器不同，Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量，他也被称为吞吐量优先的垃圾收集器。 2.自适应调节策略也是Parallel Scavenge与ParNew一个重要区别。 高吞吐量可以高效率的利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。因此，常见在服务器环境中使用。例如：那些执行批量处理，订单处理，工资支付，科学计算的应用程序。 Parallel 收集器在jdk1.6的时候提供了用于执行老年代垃圾搜集的Parallel Old收集器，用来替代老年代的Serial Old收集器。Parallel Old收集器采用了标记-压缩算法，但是同样也是基于并行回收和STW机制。 在程序吞吐量优先的场景中，Parallel收集器和Parallel Old收集器的组合，在Server模式下的内存回收性能很不错。在Java8中，默认是此垃圾回收器。 2）参数配置-XX:UseParallelGC 手动指定年轻代使用Parallel并行收集器执行内存回收任务。 -XX:+UseParallelOldGC 手动指定老年代都是使用并行回收器。（分别适用于新生代和老年代。默认是JDK8开启的） -XX:ParallelGCThreads 设置年轻代并行收集器的线程数。一般的最好与CPU数量相等，避免线程数影响收集器性能。（默认情况下，CPU数小于8，ParallelGCThreads的值等于CPU数量，当cpu数大于8个的时候，ParallelGCThreads值=3+[5*cpu数]/8 ） -XX:MaxGCPauseMillis 设置垃圾收集器最大停顿时间（STW的时间，单位是ms） 为了尽可能把停顿时间控制在MaxGCPauseMills以内，收集器在工作时会调整Java堆大小或者其他一些参数。对于用户来讲，停顿时间越短，体验越好。但是在服务器端，我们注重高并发，整体的吞吐量，所以服务端适合Parallel，进行控制。（该参数谨慎使用） -XX:GCTimeRatio 垃圾收集时间占总时间比例（=1/(N+1)）用于衡量吞吐量大小。 -XX:UseAdaptiveSizePolicy 设置Parallel Scavenge收集器具有自适应调节策略 在这种模式下，年轻代的大小，Eden和幸存者的比例，晋升老年代的对象年龄等参数会被自动调整，已经达到在堆大小，吞吐量和停顿时间之间的平衡点。 在手动调优比较困难的场合，可以直接使用这种自适应的方式，仅仅指定虚拟机的最大堆，目标吞吐量和停顿时间，让虚拟机自己完成调优工作。 78.CMS回收器（低延迟）1）概述JDK1.5，这款收集器是HotSpot虚拟机第一款真正意义上的并发收集器，第一次实现了让垃圾收集线程与用户线程同时工作。 CMS的关注点是尽可能缩短垃圾收集时用户线程的停顿时间。停顿时间越短就越适合于用户交互的程序，良好的响应速度能提升用户体验。 目前很大一部分的java应用集中在互联网或者B/S架构系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停段时间最短，以给用户较好的体验感。 CMS的垃圾收集算法采用标记-清除算法，并且也会STW。 不幸的是，CMS作为老年代的收集器，却无法与JDK1.4中已经存在的新生代收集器Parallel Scavenge 配合工作，所以在JDK1.5中使用CMS来收集老年代的时候，新生代只能选择ParNew或者Parial收集器中的一个。 在G1出现之前，CMS还是非常广泛的，一直到今天，仍然有许多系统使用CMS GC。 2）CMS工作原理1初始标记：标记出GCROOTS能直接关联到的对象（速度快） 1并发标记：从GCROOTS的直接关联对象开始遍历整个对象图的过程（耗时长，但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行） 1重新标记：修正并发标记期间，因为用户程序继续运作而导致标记产生变动的那一部分对象的标记记录（时间比初始标记稍微长） 1并发清除：清理删除掉标记阶段判断的已经死亡的对象，释放内存空间。（由于不需要移动存活的对象，所以这个阶段也是可以与用户线程并发执行的） 初始化标记和再次标记仍然要STW机制，目前所有的来收集器都做不到完全不需要STW，只是尽可能的缩短暂停时间。由于最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低停顿的。 在CMS回收过程中，还应该确保应用程序线程有足够的内存可用。因此，CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了在进行收集，而是当堆内存使用率达到某一阈值时，便开始进行回收，以确保应用程序在CMS工作过程中依然有足够的空间支持应用程序在运行。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次”Concurrent Mode Failure”失败，这时虚拟机将启动预备方案，临时启动Serial Old收集器来重新收集老年代的垃圾收集，这样停顿时间就很长了。 CMS的垃圾收集算法使用的是标记清除算法，不可避免的会产生内存碎片，无法使用指针碰撞，只能选择空闲列表。 标记清除算法会造成内存碎片，为什么不把算法换成标记整理算法呢？ 因为当并发清除时，用标记整理算法整理内存的话，原来的用户线程使用的内存无法继续使用，要保证用户线程还能继续执行，前提是他运行的资源不受影响。 优点：并发收集，低延迟 缺点：会产生内存碎片，对CPU资源敏感（并发阶段，占用了一部分线程导致应用程序变慢，总吞吐量降低），无法处理浮动垃圾（在并发阶段如果产生新的垃圾对象，CMS无法对这些垃圾对象进行标记，最终会导致这些新产生 的垃圾对象没有及时回收，只能在下一次GC的时候释放这些之前未被回收的内存空间） 3）参数设置-XX:+UseConcMarkSweepGC 手动指定使用CMS收集器执行内存回收任务 -XX:CMSlnitiatingOccupanyFraction 设置堆使用率的阈值，一旦达到阈值，便开始垃圾回收 -XX:+UseCMSCompactAtFullCollection 用于指定在执行完Full GC后堆内存空间进行压缩整理，以此避免内存碎片的产生。不过由于内存压缩整理过程无法并发执行，所带来的的问题就是停顿时间变得更长了。 -XX:+CMSFullGCsBeforeCompaction 设置在执行了多少次Full GC后对内存进行压缩整理。 -XX:ParallelCMSThreads 设置CMS线程数。CMS默认启动线程数是 （ParallelGCThreads+3）/4，ParallelGCThreads 是年轻代并行收集器的线程数。当CPU资源紧张时，受到CMS收集器线程的影响，应用程序的性能在垃圾回收节点可能会非常糟糕。 4）小技巧HotSpot有这么多的垃圾回收器，那么如果有人问，Serial GC，Parallel GC，Concurrent Mark Sweep GC这三个GC有什么不同呢？ 最小化的使用内存和并行开销 Serial GC 最大化应用程序的吞吐量 Parallel GC 最小化GC的中断或停顿时间 CMS GC 5）后续版本变化JDK9 声明CMS为过时，JDK14直接删掉了，如果使用不会报错，只是会给出警告，并使用默认的垃圾收集器。 79.G1回收器（区域分代化）1）概述G1是在java7引入的垃圾回收器，为了适应不断扩大的内存和不断增加的处理器数量，进一步降低暂停时间，同时兼顾良好的吞吐量。 为什么叫G1回收器？ 因为G1是一个并行回收器，他把堆内存分割成很多不相关的区域。G1有计划的避免在整个java堆中进行全区域的垃圾收集。G1跟踪各个区域里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的区域。 由于这种方式的侧重点在于回收垃圾最大量的区间，所以我们给G1一个名字，垃圾优先。 G1主要针对配备多核CPU以及大容量内存的机器，是JDK9以后的默认垃圾回收器，在JDK8还不是默认的垃圾回收器，需要使用-XX:UseGmentGC来启用。 与其他垃圾回收器相比，G1使用了全新的分区算法： 2）特点①并行与并发并行性：G1在回收期间，可以有多个GC线程同时工作，有效利用多核计算能力。此时用户线程STW。 并发性：G1拥有与应用程序交替执行的能力，部分工作可以和应用程序同时执行，因此，一般来说，不会再整个回收阶段发生完全阻塞应用程序的情况。 ②分代收集从分代上看，G1依然属于分代型垃圾回收器，他会区分年轻代和老年代，年轻代依然有Eden区和幸存者区。但从堆的结构上看，他不要求整个Eden区，年轻代或者老年代都是连续的，也不再坚持固定大小和固定数量。 将堆空间分为若干个区域，这些区域中包含了逻辑上的年轻代和老年代。 和之前的各类回收器不同，他同时兼顾年轻代和老年代。 ③空间整合CMS：标记清除算法，内存碎片，若干次GC后进行一次碎片整理。 G1将内存划分为一个个小区域，内存的回收是以一个个小区域为单位的。区域之间是复制算法，但是整体上可以看成标记压缩算法，两种算法都可以避免内存碎片。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。尤其是当java堆非常大的时候，G1的优势更加明显。 ④可预测的停顿时间每次根据允许的收集时间，优先回收价值最大的区域，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。 3）缺点在用户程序运行过程中，G1无论是为了垃圾收集产生的内存占用还是程序运行时的额外执行负载都要比CMS高。 经验上来讲：在小内存应用上CMS的表现大概率会优先于G1，而G1在大内存应用上则发挥其优势。平衡点在6-8G之间。 4）参数设置-XX:+UseG1GC 手动指定使用G1垃圾收集器执行内存回收任务 -XX:G1HeapRegionSize 设置每个区域的大小。值是2的幂，范围是1M-32M之间。 -XX:MaxGCPauseMillis 设置期望达到的最大GC停顿时间指标 -XX:ParallelGCThread 设置STW工作线程数，最多设置为8 -XX:ConcGCThreads 设置并发标记的线程数 -XX:InitiatingHeapOccupancyPercent 设置触发并发GC周期的java堆占用率阈值。超过此值，就会出发GC。 5）G1调优的步骤开启垃圾收集器 设置堆的最大内存 设置最大停顿时间 G1提供了三种垃圾收集模式：YoungGC，Mixed GC和Full GC，在不同的条件下被触发。 6）适用场景面向服务端应用，针对具有大内存，多处理器的机器 需要低GC延迟，并具有大堆的应用程序提供解决方案 用来替换掉jdk5的cms HotSpot垃圾收集器，除了G1以外，其他的垃圾收集器使用内置的JVM线程执行GC的多线程操作，而G1 GC可以采用应用线程承担后台运行的GC工作，即当JVM的GC线程处理速度慢时，系统会调用应用程序线程帮助加速垃圾收集过程。 所有的区域都大小相同，并且在JVM生命周期内不会被改变，虽然还保留新生代和老年代的概念，但是新生代和老年代不再是物理隔离了，他们都是一部分区域的集合。通过区域的动态分配的方式实现逻辑上的连续。 7）HumongousG1垃圾收集器还增加了一种新的内存区域，叫做Humongous，主要存储大对象，如果超过1.5个区域，就放到H。 设置H的原因：对于堆中的大对象，默认直接会被分配到老年代，但是如果他是一个短期的大对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1划分了一个H区，他专门用来存放大对象。如果一个H区装不下一个大对象，那么G1会寻找连续的H区来存储。为了能找到连续的H区，有时候不得不启动Full GC。G1的大多数行为都把H区作为老年代的一部分来看待。 8）回收过程G1 GC的垃圾回收过程主要包括如下三个环节 年轻代GC 老年代并发标记过程 混合回收 如果需要，单线程，独占式，高强度的Full GC还是继续存在的。他针对GC的评估失败提供了一种失败保护机制，即强力回收。 应用程序分配内存，当年轻代的Eden区用尽时开始年轻代回收过程；G1的年轻代收集阶段是一个并行的独占式收集器。在年轻代回收期，G1 GC暂停所有应用程序线程，启动多线程执行年轻代回收。然后从年轻代区间移动存活对象到Survivor区间或者老年代区间，也有可能是两个区间都会涉及。 当堆内存使用达到一定值的时候，开始老年代并发标记过程。 标记完成马上开始混合回收过程。对于一个混合回收期，G1 GC从老年区间移动存活对象到空闲区间，这些空闲区间也就成为了老年代的一部分。和年轻代不同，老年代的G1回收器和其他GC不同，G1的老年代回收器不需要整个老年代被回收，一次只需要扫描/回收一小部分老年代的区域就可以了。同时，这个老年代的区域是和新生代一起被回收的。 ①记忆集与写屏障一个对象被不同区域引用的问题 一个区域不可能是孤立的，一个区域中的对象可能被其他任意区域中的对象引用，判断对象存活时，是否需要扫描整个Java堆才能保证准确？ 在其他的分代收集器，也存在这样的问题 回收新生代也不得不同时扫描老年代？ 这样的话会降低Minor GC的效率 解决方法 无论G1还是其他垃圾收集器，Jvm都是使用Remembered Set 来避免全局扫描。 每个区域都有一个对应的Remembered Set ； 每次Reference类型数据写操作时，都会产生一个Write Barrier暂时中断操作； 然后检查将要写入的引用指向的对象是否和该Reference类型数据在不同的区域 如果不同，通过CardTable把相关引用信息记录到引用指向对象的所在区域对应的Remembered Set 中； 当进行垃圾收集时，在GC跟节点的枚举范围加入Remembered Set；就可以保证不进行全局扫描，也不会有遗漏。 ②年轻代GCJVM启动时，G1先准备好Eden区，程序在运行过程中不断创建对象到Eden区，当Eden空间耗尽时，G1会启动一次年轻代垃圾回收过程。 年轻代垃圾回收只会回收Eden区和幸存者区。 首先G1停止应用程序的执行，G1创建回收集，回收集是指需要被回收的内存分段的集合，年轻代回收过程的回收集包含年轻代Eden区和幸存者区所有的内存字分段。然后开始进行如下回收：扫描根 更新RSet 处理RSet 复制对象 处理引用 第一阶段，扫描根。 根是指static变量指向的对象，正在执行的方法调用链条上的局部变量等。根引用连同RSet记录的外部引用作为扫描存活对象的入口。 第二阶段，更新RSet。 处理dirty card queue( 见备注)中的card，更新RSet。 此阶段完成后，RSet可 以准确的反映老年代对所在的内存分段中对象的引用。 第三阶段，处理RSet. 识别被老年代对象指向的Eden中的对象，这些被指向的Eden中的对象被认为是存活的对象。第四阶段，复制对象。 此阶段，对象树被遍历，Eden区 内存段中存活的对象会被复制到Survivor区中空的内存分段，Survivor区内存段中存活的对象如果年龄未达阈值，年龄会加1，达到阀值会被会被复制到 01d区中空的内存分段。如果Survivor空间不够，Eden空 间的部分数据会直接晋升到老年代空间。 第五阶段，处理引用。 处理Soft，Weak，Phantom, Final, JNI Weak等引用。最终Eden空间的数据为空，GC停止工作，而目标内存中的对象都是连续存储的，没有碎片，所以复制过程可以达到内存整理的效果，减少碎片。 ②并发标记过程初始标记阶段 根区域扫描 并发标记 再次标记 独占标记 独占清理 并发清理阶段 1.初始标记阶段:标记从根节点直接可达的对象。这个阶段是STW的，并且会触发- - 次年轻代GC。 2.根区域扫描(Root Region Scanning) : G1 GC扫描Survivor区直接可达的老年代， 区域对象，并标记被引用的对象。这一-过程必须在young GC之前完成。 3.并发标记(Concurrent Marking): 在整个堆中进行并发标记(和应用程序并发执行)， 此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾,那这个区域会被立即回收。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。 4.再次标记(Remark):由 于应用程序持续进行，需要修正上一- 次的标记结果。是STW 的。G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。 5.独占清理(cleanup,STW):计算各个区域的存活对象和GC回收比例，并进行排序，识别可以混合回收的区域。为下阶段做铺垫。是STW的。 ➢这个阶段并不会实际上去做垃圾的收集。 6.并发清理阶段:识别并清理完全空闲的区域。 ③混合回收当越来越多的对象晋升到老年代，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即Mixed GC，该算法并不是一个Old GC，除了回收整个Young Region，还会回收一部分的Old Region。这里需要注意：是一部分老年代，而不是全部老年代。可以选择哪些Old Region进行收集。从而可以对垃圾回收的耗时时间进行控制。也要注意的是Mixed GC并不是Full GC。 并发标记结束以后，老年代中百分百为垃圾的内存分段被回收了，部分为垃圾的内存分段被计算了出来。默认情况下，这些老年代的内存分段会分8次(可以通过-XX: G1MixedGCCountTarget设置)被回收。 混合回收的回收集(Collection Set)包括八分之- -的老年代内存分段，Eden区 内存 分段，Survivor区 内存分段。混合回收的算法和年轻代回收的算法完全一样， 只是回收集多了老年代的内存分段。具体过程请参考上面的年轻代回收过程。 由于老年代中的内存分段默认分8次回收，G1会优先回收垃圾多的内存分段。垃圾占内存分段比例越高的，越会被先回收。并且有一个阈值会决定内存分段是否被回收，-XX:G1MixedGCLiveThresholdPercent，默认为65%，意思是垃圾占内存分段比例要达到65%才会被回收。如果垃圾占比太低，意味着存活的对象占比高，在复制的时候会花费更多的时间。 混合回收并不一定要进行8次。有一个阈值-XX:G1HeapWastePercent，默认值为10%，意思是允许整个堆内存中有10%的空间被浪费，意味着如果发现可以回收的垃圾占堆内存的比例低于10%，则不再进行混合回收。因为GC会花费很多的时间但是回收到的内存却很少。 ④Full GCG1的初衷就是要避免Full GC的出现，但是如果上述方式不能正常工作，G1会停止应用程序的执行，使用单线程的内存回收算法进行垃圾回收，性能会非常差，应用程序停顿时间会很长。 要避免Full GC的发生，一旦发生需要进行调整。什么时候会发生Full GC呢？比如堆内存太小，当G1在复制存活对象的时候没有空的内存分段可用，则会回退到Full GC，这种情况下可以通过增大内存解决。 导致G1Full GC的原因可能有两个 Evacuation的时候没有足够的to-space来存放晋升的对象 并发处理过程完成之前空间耗尽 ⑤G1回收器优化建议年轻代大小 避免使用-Xmn或者-XX:NewRatio等相关选项显式设置年轻代的大小 固定年轻代的大小会覆盖暂停时间目标 暂停时间目标不要太过严苛 G1 GC的吞吐量目标是90%的应用程序时间和10%的垃圾回收时间 评估G1 GC的吞吐量时，暂停时间目标不要太严苛。目标太严苛表示你愿意承受更多的垃圾回收开销，而这些会直接影响到吞吐量。 80.垃圾回收器总结1）对比 垃圾收集器 分类 作用位置 使用算法 特点 适用场景 Serial 串行 新生代 复制算法 响应速度优先 单CPU的client模式 ParNew 并行 新生代 复制算法 响应速度优先 多CPU的server模式与CMS配合使用 Parallel 并行 新生代 复制算法 吞吐量优先 后台运算而不需要太多交互的场景 Serial Old 串行 老年代 标记压缩 响应速度优先 单CPU的client模式 Parallel Old 并行 老年代 标记压缩 吞吐量优先 后台运算而不需要太多交互的场景 CMS 并发 老年代 标记清除 响应速度优先 互联网/BS业务 G1 并发，并行 新生代，老年代 标记压缩，复制 响应速度优先 服务端应用 GC发展阶段 Serial =&gt; Parallel(并行) =&gt;CMS(并发)=&gt;G1=&gt;ZGC 2）垃圾回收器的组合 3）怎么选择垃圾回收器？1.优先调整堆的大小让JVM自适应完成 2.如果内存小于100M，使用串行收集器 3.如果是单核，单机程序，并且没有停顿时间的要求，串行收集器 4.如果是多CPU，需要高吞吐量，允许停顿时间超过1s，选择并行或者JVM自己选择 5.如果是多CPU，追求低停顿时间，需快速响应，使用并发收集器 官方推荐G1，性能高。现在互联网的项目，基本都是使用G1. 没有最好的收集器，调优是针对特定场景，特定需求。 81.GC日志分析1）内存分配与垃圾回收的参数列表1-XX: +PrintGC` 输出Gc日志。类似: `-verbose:gc -XX: +PrintGCDetails输出GC的详细日志 -XX: +PrintGCTimeStamps输出Gc的时间戳( 以基准时间的形式) -XX: +PrintGCDateStamps输出GC的时间戳(以日期的形式，如2013-05- 04T21 :53:59.234+0800) -XX: +PrintHeapAtGC在进行GC的前后打印出堆的信息 -Xloggc:../logs/gc.log日志文件的输出路径 2）GC日志中垃圾回收数据分析123456789101112131415/** * @author yhd * @createtime 2021/3/18 13:37 * vmoptions:-Xms10m -Xmx10m -XX:+PrintGCDetails */public class GetDump &#123; private int[] arr = new int[999999999]; public static void main(String[] args) &#123; IntStream.range(0, 1000000).forEach(i -&gt; new GetDump()); &#125;&#125; 12345678910111213141516171819202122[GC (Allocation Failure) [PSYoungGen: 2048K-&gt;496K(2560K)] 2048K-&gt;953K(9728K), 0.0008097 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [GC (Allocation Failure) [PSYoungGen: 1918K-&gt;488K(2560K)] 2375K-&gt;1141K(9728K), 0.0006189 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [GC (Allocation Failure) [PSYoungGen: 488K-&gt;488K(2560K)] 1141K-&gt;1181K(9728K), 0.0005018 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [Full GC (Allocation Failure) [PSYoungGen: 488K-&gt;0K(2560K)] [ParOldGen: 693K-&gt;1041K(7168K)] 1181K-&gt;1041K(9728K), [Metaspace: 4058K-&gt;4056K(1056768K)], 0.0079747 secs] [Times: user=0.14 sys=0.00, real=0.01 secs] [GC (Allocation Failure) [PSYoungGen: 0K-&gt;0K(2560K)] 1041K-&gt;1041K(9728K), 0.0004530 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [Full GC (Allocation Failure) [PSYoungGen: 0K-&gt;0K(2560K)] [ParOldGen: 1041K-&gt;987K(7168K)] 1041K-&gt;987K(9728K), [Metaspace: 4056K-&gt;4056K(1056768K)], 0.0072571 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] Heap PSYoungGen total 2560K, used 51K [0x00000000ffd00000, 0x0000000100000000, 0x0000000100000000) eden space 2048K, 2% used [0x00000000ffd00000,0x00000000ffd0cc80,0x00000000fff00000) from space 512K, 0% used [0x00000000fff80000,0x00000000fff80000,0x0000000100000000) to space 512K, 0% used [0x00000000fff00000,0x00000000fff00000,0x00000000fff80000) ParOldGen total 7168K, used 987K [0x00000000ff600000, 0x00000000ffd00000, 0x00000000ffd00000) object space 7168K, 13% used [0x00000000ff600000,0x00000000ff6f6d18,0x00000000ffd00000) Metaspace used 4093K, capacity 4642K, committed 4864K, reserved 1056768K class space used 463K, capacity 497K, committed 512K, reserved 1048576KException in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space 参数说明 1&quot; [GC&quot;和&quot; [Full GC&quot; 说明了这次垃圾收集的停顿类型，如果有&quot;Full &quot;则说明GC发生了&quot;Stop The World&quot; 1使用Serial收集器在新生代的名字是Default New Generation,因此显示的是&quot; [ De fNew&quot; 使用ParNew收集器在新生代的名字会变成&quot; [ParNew&quot;,意思是&quot;Parallel New Generation&quot; 1使用Parallel Scavenge收 集器在新生代的名字是&quot; [PSYoungGen&quot; 1老年代的收集和新生代道理一样， 名字也是收集器决定的 1使用G1收集器的话，会显示为&quot;garbage- first heap&quot; 1Allocation Failure 1表明本次引起GC的原因是因为在年轻代中没有足够的空间能够存储新的数据了。 1[PSYoungGen: 5986K-&gt;696K(8704K) ] 5986K-&gt; 704K (9216K) 1中括号内: GC回收前年轻代大小，回收后大小，(年轻代总 大小) 1括号外: GC回收前年轻代和老年代大小，回收后大小，( 年轻代和老年代总大小) 1user代表用户态回收耗时，sys 内核态回收耗时，rea实际耗时。由于多核的原因，时间总和可能会超过real时间 82.日志分析工具的使用常用日志分析工具 1GCViewer`,`GCEasy`,`GCHIsto`,`GCLogViewer`,`Hpjmeter`,`garbagecat 83.垃圾回收器的新发展1）Epsilon GC仅仅做内存分配，分配完之后程序直接结束 2）Shenandoah GC低停顿时间，由红帽开发。 暂停时间与堆大小无关，堆大小并不会影响垃圾收集的停顿时间。 高并发下吞吐量下降。 3）ZGC可伸缩，低延迟的垃圾回收器。 在尽可能对吞吐量影响不大的前提下，实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在10ms以内的低延迟。 ZGC收集器是一款基于区域内存布局的，不设置分代的，使用了读屏障，染色指针和内存多重映射等技术来实现可并发的标记-压缩算法的，以低延迟为首要目标的一款垃圾收集器。 工作过程分为四个阶段：并发标记-并发预备重分配-并发重分配-并发重映射。 除了初始标记是STW的，其余都可并发。 84.类的加载器概述类加载器是 JVM 执行类加载机制的前提 ClassLoader 的作用： ClassLoader 是 Java 的核心组件，所有的 Class 都是由 ClassLoader 进行加载的，ClassLoader 负责通过各种方式将 Class 信息的二进制数据流读入 JVM 内部，转换为一个与目标类对应的 java.lang.Class 对象实例。然后交给 Java 虚拟机尽心链接、初始化等操作。因此，ClassLoader 在整个装载阶段，只能影响到类的加载，而无法通过 ClassLoader 去改变类的链接和初始化行为。至于它是否可以运行，则由 Execution Engine 决定。 类加载器最早出现在 Java 1.0 版本中，那个时候只是单纯地为了满足 Java Applet 应用而被研发出来，但如今类加载器却在 OSGI、字节码加解密领域大放异彩。 这主要归功于 Java 虚拟机的设计者们当初在设计类加载器的时候，并没有考虑将它绑定在 JVM 内部，这样做的好处就是能够更加灵活和动态地执行类加载操作。 1）类加载的分类类的加载分类：显式加载 vs 隐式加载 Class 文件的显式加载与隐式加载的方式是指 JVM 加载 Class 文件到内存的方式 显式加载指的是在代码中通过调用 ClassLoader 加载 Class 对象，如直接使用 Class.forName(name) 或 this.getClass().getClassLoader().loadClass() 加载 Class对象 隐式加载则是不直接在代码中调用 ClassLoader 的方法加载 Class 对象，而是通过虚拟机自动加载到内存中，如在加载某个类的 Class 文件时，该类的 Class 文件中引用了另外一个类的对象，此时额外引用的类将通过 JVM 自动加载到内存中。 在日常开发中以上两种方式一般会混合使用。 2）类加载的必要性一般情况下，Java 开发人员并不需要在程序中显式地使用类加载器，但是了解类加载器的加载机制却显得至关重要。从以下几个方面说： 避免在开发中遇到 java.lang.ClassNotFoundException 异常或 java.lang.NoClassDeFoundError 异常时手足无措。只有了解类加载器的加载机制才能够在出现异常的时候快速地根据错误异常日志定位问题和解决问题 需要支持类的动态加载或需要对编译后的字节码文件进行加解密操作时，就需要与类加载器打交道了 开发人员可以在程序中编写自定义类加载器来重新定义类的加载规则，以便实现一些自定义的处理逻辑 3）命名空间①何为类的唯一性？对于任意一个类，都需要由加载它的类加载器和这个类本身一同确认其在 Java 虚拟机中的唯一性。每一个类加载器，都拥有一个独立的类名称空间：比较两个类是否相等，只有在这两个类是由同一个类加载器加载的前提下才有意义。否则，即使这两个类源自同一个 Class 文件，被同一个虚拟机加载，只要加载他们的类加载器不同，那这两个类就必定不相等 ②命名空间 每个类加载器都有自己的命名空间，命名空间由该加载器以及所有的父加载器所加载的类组成 在同一命名空间中，不会出现类的完整名字(包括类的包名)相同的两个类 在不同的命名空间中，有可能会出现类的完整名字(包括类的包名)相同的两个类 在大型应用中，我们往往借助这一特性，来运行同一个类的不同版本。 4）类加载机制的基本特征 双亲委派模型。但不是所有类加载都遵守这个模型，有的时候，启动类加载器所加载的类型，是可能要加载用户代码的，比如 JDK 内部的 ServiceProvider/ServiceLoader 机制，用户可以在标准 API 框架上，提供自己的实现，JDK 也需要提供些默认的参考实现。例如，Java 中 JNDI、JDBC、文件系统、Cipher 等很多方面，都是利用的这种机制，这种情况就不会用双亲委派模型去加载，而是利用所谓的上下文加载器 可见性，子类加载器可以访问父加载器加载的类型，但是反过来是不允许的。不然，因为缺少必要的隔离，我们就没有办法利用类加载器去实现容器的逻辑 单一性，由于父加载器的类型对于子加载器是可见的，所以父加载器中加载过的类型，就不会在子加载器中重复加载。但是注意，类加载器”邻居”间，同一类型仍然可以被加载多次，因为相互并不可见 85.类加载器分类JVM 支持两种类型的类加载器，分别为引导类加载器(Bootstrap ClassLoader)和自定义类加载器(User-Defined ClassLoader) 从概念上来讲，自定义类加载器一般指的是程序中由开发人员自定义的一类类加载器，但是 Java 虚拟机规范却没有这么定义，而是将所有派生于抽象类 ClassLoader 的类加载器都划分为自定义类加载器。无论类加载器的类型如何划分，在程序中我们最常见的类加载器结构主要是如下情况： 除了顶层的启动类加载器外，其余的类加载器都应当有自己的”父类”加载器 不同类加载器看似是继承(Inheritance)关系，实际上是包含关系。在下层加载器中，包含着上层加载器的引用 “父类加载器”并非下层加载器extends上层加载器，而且包含了上层加载器的引用。 具体代码 12345678910111213141516171819class ClassLoader &#123; ClassLoader parent; //父类加载器 public ClassLoader(ClassLoader parent) &#123; this.parent = parent; &#125;&#125;class ParentClassLoader extends ClassLoader &#123; public ParentClassLoader(ClassLoader parent) &#123; super(parent); &#125;&#125;class ChildClassLoader extends ClassLoader &#123; public ChildClassLoader(ClassLoader parent) &#123; //parent = new ParentClassLoader(); super(parent); &#125;&#125; 1）Bootstrap ClassLoader 这个类加载使用 C/C++ 语言实现的，嵌套在 JVM 内部 它用来加载 Java 的核心库(JAVA_HOME/jre/lib/rt.jar 或 sun.boot.class.path 路径下的内容)。用于提供 JVM 自身需要的类 并不继承自 java.lang.ClassLoader，没有父加载器 出于安全考虑，Bootstrap 启动类加载器之加载包名为 java、javax、sun 等开头的类 加载扩展类和应用程序类加载器，并指定为他们的父类加载器。 使用 -XX:+TraceClassLoading 参数 启动类加载器使用 C++ 编写的？Yes！ C/C++：指针函数 &amp; 函数指针、C++ 支持多继承、更加高效 Java ：由 C++ 演变而来，(C++)– 版，单继承 2）Extension ClassLoaderJava 语言编写，由 sun.misc.Launcher$ExtClassLoader 实现 继承于 ClassLoader 类 父类加载器为启动类加载器 从 java.ext.dirs 系统属性所指定的目录中加载类库，或从 JDK 的安装目录的 jre/lib/ext 子目录下加载类库。如果用户创建的 JAR 放在此目录下，也会自动由扩展类加载器加载 3）AppClassLoader Java 语言编写，由 sun.misc.Launcher$AppClassLoader 实现 继承于 ClassLoader 类 父类加载器为扩展类加载器 它负责加载环境变量 classpath 或系统属性 java.class.path 指定路径下的类库 应用程序中的类加载器默认是系统类加载器 它是用户自定义类加载器的默认父加载器 通过 ClassLoader 的 getSystemClassLoader() 方法可以获取到该类加载器 4）User DIY ClassLoader 在 Java 的日常应用程序开发中，类的加载几乎是由上述3种类加载器相互配合执行的。在必要时，我们还可以自定义类加载器，来定制类的加载方式 体现 Java 语言强大生命力和巨大魅力的关键因素之一便是，Java 开发者可以自定义类加载器来实现类库的动态加载，加载源可以是本地的 JAR 包，也可以是网络上的远程资源 通过类加载器可以实现非常绝妙的插件机制，这方面的实际应用案例不胜枚举。例如，著名的 OSGI 组件框架，再如 Eclipse 的插件机制。类加载器为应用程序提供了一种动态增加新功能的机制，这种机制无需重新打包发布应用程序就能实现 同时，自定义加载器能够实现应用隔离，例如 Tomcat、Spring 等中间件和组件框架都在内部实现了自定义的加载器，并通过自定义加载器隔离不同的组件模块。这种机制比 C/C++ 程序要好太多，想不修改 C/C++ 程序就能为其新增功能，几乎是不可能的，仅仅一个兼容性便能阻挡所有美好的设想 自定义类加载器通常需要继承于 ClassLoader 86.测试不同的类加载器每个 Class 对象都会包含一个定义它的 ClassLoader 的一个引用 获取 ClassLoader 的途径 说明： 站在程序的角度看，引导类加载器与另外两种类加载器(系统类加载器和扩展类加载器)并不是同一个层次意义上的加载器，引导类加载器是使用 C++ 语言编写而成的，而另外两种类加载器则是使用 Java 语言编写的。由于引导类加载器压根儿就不是一个 Java 类，因此在 Java 程序中只能打印出空值 数组类的 Class 对象，不是由类加载器去创建的，而是在 Java 运行期 JVM 根据需要自动创建的。对于数组类的类加载器来说，是通过 Class.geetClassLoader() 返回的，与数组当中元素类型的类加载器是一样的：如果数组当中的元素类型是基本数据类型，数组类是没有类加载器的 1234567891011String[] strArr = new String[6];System.out.println(strArr.getClass().getClassLoader());//运行结果：nullClassLoaderTest[] test = new ClassLoaderTest[1];System.out.println(test.getClass().getClassLoader());//运行结果：sun.misc.Launcher$AppClassLoader@18b4aac2int[] inst = new int[2];System.out.println(inst.getClass().getClassLoader());//运行结果：null 87.ClassLoader源码解析ClassLoader 与现有类加载的关系： 除了以上虚拟机自带的加载器外，用户还可以定制自己的类加载器。Java 提供了抽象类 java.lang.ClassLoader，所有用户自定义的类加载器都应该继承 ClassLoader 类。 1）ClassLoader的主要方法public final ClassLoader getParent()返回该类加载器的超类加载器 public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException加载名称为name的类，返回结果为java.lang.Class类的实例。如果找不到类，则返回ClassNotFoundException异常。 该方法中的逻辑就是双亲委派模式的实现。 protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException查找二进制名称为name的类，返回结果为java.lang.Class类的实例。这是一个受保护的方法，JVM鼓励我们重写此方法，需要自定义加载器遵循双亲委托机制，该方法会在检查完父类加载器之后被loadClass()方法调用。 在JDK1.2之前，在自定义类加载时，总会去继承ClassLoader类并重写loadClass方法，从而实现自定义的类加载类。但是在JDK1.2之后已不再建议用户去覆盖loadClass()方法，而是建议把自定义的类加载逻辑写在findClass()方法中，从前面的分析可知， findClass()方法是在loadClass()方法中被调用的，当loadClass()方法中父加载器加载失败后，则会调用自己的findClass()方法来完成类加载，这样就可以保证自定义的类加载器也符合双亲委托模式。需要注意的是ClassLoader类中并没有实现findClass()方法的具体代码逻辑，取而代之的是抛出ClassNotFoundException异常，同时应该知道的是findClass方法通常是和defineClass方法一起使用的。一般情况下，在自定义类加载器时，会直接覆盖ClassLoader的findClass()方法并编写加载规则，取得要加载类的字节码后转换成流，然后调用defineClass()方法生成类的Class对象。 protected final Class&lt;?&gt; defineClass(byte[] b, int off, int len)根据给定的字节数组b转换为Class的实例，off和len参数表示实际Class信息在byte数组中的位置和长度，其中byte数组b是ClassLoader从外部获取的。这是受保护的方法，只有在自定义ClassLoader子类中可以使用。 defineClass()方法是用来将byte字节流解析成JVM能够识别的Class对象(ClassLoader中已实现该方法逻辑)，通过这个方法不仅能够通过class文件实例化class对象，也可以通过其他方式实例化class对象，如通过网络接收一个类的字节码，然后转换为byte字节流创建对应的Class对象。 defineClass()方法通常与findClass()方法一起使用，一般情况下，在自定义类加载器时，会直接覆盖ClassLoader的findClass()方法并编写加载规则，取得要加载类的字节码后转换成流，然后调用defineClass()方法生成类的Class对象。 123456789101112131415/** * 编写findClass方法的逻辑 */ @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; // 获取类的class文件字节数组 byte[] classData = getClassData(name); if (classData == null) &#123; throw new ClassNotFoundException(); &#125; else &#123; //直接生成class对象 return defineClass(name, classData, 0, classData.length); &#125; &#125; protected final void resolveClass(Class&lt;?&gt; c)链接指定的一个Java类。使用该方法可以使用类的Class对象创建完成的同时也被解析(即加载的同时也进行解析)。前面我们说链接阶段主要是对字节码进行验证，为类变量分配内存并设置初始值同时将字节码文件中的符号引用转换为直接引用。 protected final Class&lt;?&gt; findLoadedClass(String name)查找名称为name的已经被加载过的类，返回结果为java.lang.Class类的实例。这个方法是final方法，无法被修改。 private final ClassLoader parent它也是一个ClassLoader的实例，这个字段所表示的ClassLoader也称为这个ClassLoader的双亲。在类加载的过程中,ClassLoader可能会将某些请求交予自己的双亲处理。 ①loadClass()的剖析测试代码 1ClassLoader.getSystemClassLoader().loadClass(&quot;com.atguig.java.User&quot;); 涉及到对如下方法的调用 123456789101112131415161718192021222324252627282930313233343536373839404142protected Class&lt;?&gt; loadClass(String name, boolean resolve)//resolve:true-加载class的同时进行解析操作。 throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; //同步操作，保证只能加载一次。 // First, check if the class has already been loaded //首先，在缓存中判断是否已经加载同名的类 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; //获取当前类加载器的父类加载器。 if (parent != null) &#123; //如果存在父类加载器，则调用父类加载器进行类的加载（递归） c = parent.loadClass(name, false); &#125; else &#123; //parent为null:父类加载器是引导类加教器 c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123;//当前类的加载器的父类加载器未加载此类or此类的加载器未加载此类 // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); //调用当前ClassLoader的findClass() c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; //是否进行解析操作 resolveClass(c); &#125; return c; &#125;&#125; 2）SecureClassLoader与URLClassLoader接着SecureClassLoader扩展了ClassLoader，新增了几个与使用相关的代码源(对代码源的位置及其证书的验证)和权限定义类验证(主要指对class源码的访问权限)的方法，一般我们不会直接跟这个类打交道，更多是与它的子类URLClassLoader有所关联。 前面说过，ClassLoader是一个抽象类，很多方法是空的没有实现，比如 findClass()、findResource()等。而URLClassLoader这个实现类为这些方法提供了具体的实现。并新增了URLClassPath类协助取得Class字节码流等功能。在编写自定义类加载器时，如果没有太过于复杂的需求，可以直接继承URLClassLoader类，这样就可以避免自己去编写findClass()方法及其获取字节码流的方式，使自定义类加载器编写更加简洁。 3）ExtClassLoader与AppClassLoader了解完URLClassLoader后接着看看剩余的两个类加载器，即拓展类加载器ExtClassLoader和系统类加载AppClassLoader，这两个类都继承自URLClassLoader，是sun.misc.Launcher的静态内部类。 sun.misc.Launcher主要被系统用于启动主应用程序，ExtClassLoader和AppClassLoader都是由sun.misc.Launcher创建的，其类主要类结构如下: 我们发现ExtClassLoader并没有重写loadClass()方法，这足矣说明其遵循双亲委派模式，而AppClassLoader重载了loadclass()方法，但最终调用的还是父类loadClass()方法，因此依然遵守双亲委派模式。 4）Class.forName与ClassLoader.loadClass()Class.forName():是一个静态方法,最常用的是Class.forName(String className);根据传入的类的全限定名返回一个Class对象。该方法在将Class文件加载到内存的同时,会执行类的初始化。 如:Class.forName( &quot;com.atguigu.java.Helloworld&quot;) ; 12ClassLoader.loadClass()`:这是一个实例方法,需要一个`ClassLoader`对象来调用该方法。该方法将class文件加载到内存时,并不会执行类的初始化,直到这个类第一次使用时才进行初始化。该方法因为需要得到个`ClassLoader`对象,所以可以根据需要指定使用哪个类加载器.如: `ClassLoader cl=......;cl.loadClass (&quot;com.atguigu.java.Helloworld&quot; ); 88.双亲委派模型1)定义与本质类加载器用来把类加载到Java虚拟机中。从JDK1.2版本开始，类的加载过程采用双亲委派机制，这种机制能更好地保证java平台的安全。 ①定义如果一个类加载器在接到加载类的请求时，它首先不会自己尝试去加载这个类，而是把这个请求任务委托给父类加载器去完成，依次递归，如果父类加载器可以完成类加载任务，就成功返回。只有父类加载器无法完成此加载任务时，才自己去加载。 ②本质规定了类加载的顺序是:引导类加载器先加载，若加载不到，由扩展类加载器加载，若还加载不到，才会由系统类加载器或自定义的类加载器进行加载。 2）优势与劣势①双亲委派机制优势避免类的重复加载，确保一个类的全局唯一性 Java类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层级关可以避免类的重复加载，当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次。 保护程序安全，防止核心API被随意篡改 ②代码支持双亲委派机制在java.lang.ClassLoader.loadClass(String,boolean)接口中体现。该接口的逻辑如下: (1)先在当前加载器的缓存中查找有无目标类，如果有，直接返回。 (2)判断当前加载器的父加载器是否为空，如果不为空，则调用parent.loadClass(name，false)接口进行加载。 (3)反之，如果当前加载器的父类加载器为空，则调用findBootstrapClassOrNull(name)接口，让引导类加载器进加载。 (4)如果通过以上3条路径都没能成功加载，则调用findClass(name)接口进行加载。该接口最终会调用java.lang.ClassLoader接口的defineClass系列的native接口加载目标Java类。 双亲委派的模型就隐藏在这第2和第3步中。 ③举例假设当前加载的是java.lang.Object这个类，很显然，该类属于JDK中核心得不能再核心的一个类，因此一定只能由引导类加载器进行加载。当JVM准备加载java.lang.Object时，JVM默认会使用系统类加载器去加载，按照上面4步加载的逻辑，在第1步从系统类的缓存中肯定查找不到该类，于是进入第2步。由于从系统类加载器的父加载器是扩展类加载器，于是扩展类加载器继续从第1步开始重复。由于扩展类加载器的缓存中也一定查找不到该类，因此进入第2步。扩展类的父加载器是null,因此系统调用findClass(String)，最终通过引导类加载器进行加载。 ④思考如果在自定义的类加载器中重写java.lang.ClassLoader.loadClass(String)或java.lang.ClassLoader.loadClass(String, boolean)方法,抹去其中的双亲委派机制,仅保留上面这4步中的第1步与第4步，那么是不是就能够加载核心类库了呢? 这也不行!因为JDK还为核心类库提供了一层保护机制。不管是自定义的类加载器，还是系统类加载器还是扩展类加载器，最终都必须调用java.lang.classLoader.defineClass(String,byte[], int, int, ProtectionDomain)方法，而该方法会执行preDefineClass()接口，该接口中提供了对JDK核心类库的保护。 ⑤双亲委托模式的弊端检查类是否加载的委托过程是单向的，这个方式虽然从结构上说比较清晰，使各个ClassLoader的职责非常明确，但是同时会带来一个问题，即顶层的ClassLoader无法访问底层的ClassLoader所加载的类。 通常情况下，启动类加载器中的类为系统核心类，包括一些重要的系统接口，而在应用类加载器中，为应用类。按照这种模式，应用类访问系统类自然是没有问题，但是系统类访问应用类就会出现问题。比如在系统类中提供了一个接口，该接口需要在应用类中得以实现，该接口还绑定一个工厂方法，用于创建该接口的实例，而接口和工厂方法都在启动类加载器中。这时，就会出现该工厂方法无法创建由应用类加载器加载的应用实例的问题。 ⑥结论:由于Java虚拟机规范并没有明确要求类加载器的加载机制一定要使用双亲委派模型，只是建议采用这种方式而己。 比如在Tomcat中，类加载器所采用的加载机制就和传统的双亲委派模型有一定区别，当缺省的类加载器接收到一个类的加载任务时，首先会由它自行加载，当它加载失败时，才会将类的加载任务委派给它的超类加载器去执行，这同时也是Servlet规范推荐的一种做法。 3）破坏双亲委派机制①破坏双亲委派机制1双亲委派模型并不是一个具有强制性约束的模型，而是Java设计者推荐给开发者们的类加载器实现方式。 在Java的世界中大部分的类加载器都遵循这个模型，但也有例外的情况，直到Java模块化出现为止，双亲委派模型主要出现过3次较大规模“被破坏”的情况。 第一次破坏双亲委派机制: 双亲委派模型的第一次“被破坏”其实发生在双亲委派模型出现之前–即JDK1.2面世以前的“远古”时代。 由于双亲委派模型在JDK 1.2之后才被引入，但是类加载器的概念和抽象类java.lang.ClassLoader则在Java的第一个版本中就已经存在，面对已经存在的用户自定义类加载器的代码，Java设计者们引入双亲委派模型时不得不做出一些妥协，为了兼容这些已有代码，无法再以技术手段避免loadClass()被子类覆盖的可能性，只能在JDK1.2之后的java.lang.ClassLoader中添加一个新的protected方法findClass()，并引导用户编写的类加载逻辑时尽可能去重写这个方法，而不是在loadClass()中编写代码。上节我们已经分析过loadClass()方法，双亲委派的具体逻辑就实现在这里面，按照loadClass()方法的逻辑，如果父类加载失败，会自动调用自己的findClass()方法来完成加载，这样既不影响用户按照自己的意愿去加载类，又可以保证新写出来的类加载器是符合双亲委派规则的。 ②破坏双亲委派机制2第二次破坏双亲委派机制:线程上下文类加载器 双亲委派模型的第二次“被破坏”是由这个模型自身的缺陷导致的，双亲委派很好地解决了各个类加载器协作时基础类型的一致性问题〈越基础的类由越上层的加载器进行加载），基础类型之所以被称为“基础”，是因为它们总是作为被用户代码继承、调用的API存在，但程序设计往往没有绝对不变的完美规则，如果有基础类型又要调用回用户的代码,那该怎么办呢? 这并非是不可能出现的事情，一个典型的例子便是JNDI服务，JNDI现在已经是Java的标准服务，它的代码由启动类加载器来完成加载（在JDK 1.3时加入到rt.jar的)，肯定属于Java中很基础的类型了。但JNDI存在的目的就是对资源进行查找和集中管理，它需要调用由其他厂商实现并部署在应用程序的ClassPath下的NDI服务提供者接口（Service Provider Interface，SPI）的代码，现在问题来了，启动类加载器是绝不可能认识、加载这些代码的，那该怎么办?(SPI:在Java平台中，通常把核心类rt.jar中提供外部服务、可由应用层自行实现的接口称为SPI) 为了解决这个困境，Java的设计团队只好引入了一个不太优雅的设计:线程上下文类加载器（Thread ContextClassLoader)。这个类加载器可以通过java.lang.Thread类的setContextClassLoader()方法进行设置，如果创建线程时还未设置，它将会从父线程中继承一个，如果在应用程序的全局范围内都没有设置过的话，那这个类加载器默认就是应用程序类加载器。 有了线程上下文类加载器，程序就可以做一些“舞弊”的事情了。JNDI服务使用这个线程上下文类加载器去加载所需的SPI服务代码，这是一种父类加载器去请求子类加载器完成类加载的行为，这种行为实际上是打通了双亲委派模型的层次结构来逆向使用类加载器，已经违背了双亲委派模型的一般性原则，但也是无可奈何的事情。Java中涉及SPI的加载基本上都采用这种方式来完成，例如NDI、JDBC、JCE、JAXB和BT等。不过，当SPI的服务提供者多于一个的时候，代码就只能根据具体提供者的类型来硬编码判断，为了消除这种极不优雅的实现方式，在JDK 6时，JDK提供了java.util.ServiceLoader类，以META-INF/services中的配置信息，辅以责任链模式，这才算是给SPI的加载提供了一种相对合理的解决方案。 默认上下文加载器就是应用类加载器，这样以上下文加载器为中介，使得启动类加载器中的代码也可以访问应用类加载器中的类。 ③破坏双亲委派机制3第三次破坏双亲委派机制: 双亲委派模型的第三次“被破坏”是由于用户对程序动态性的追求而导致的。如:代码热替换（Hot Swap)、模块热部署（Hot Deployment）等 IBM公司主导的JSR-291(即OSGi R4.2）实现模块化热部署的关键是它自定义的类加载器机制的实现，每一个程序模块（OSGi中称为Bundle)都有一个自己的类加载器，当需要更换一个Bundle时，就把Bundle连同类加载器一起换掉以实现代码的热替换。在OSGi环境下，类加载器不再双亲委派模型推荐的树状结构，而是进一步发展为更加复杂的网状结构。 当收到类加载请求时，OSGi将按照下面的顺序进行类搜索:（不细讲） 1）将以java.*开头的类，委派给父类加载器加载。 2）否则，将委派列表名单内的类，委派给父类加载器加载。 3）否则，将Import列表中的类，委派给Export这个类的Bundle的类加载器加载。 4）否则，查找当前Bundle的ClassPath，使用自己的类加载器加载。 5）否则，查找类是否在自己的Fragment Bundle中，如果在，则委派给Fragment Bundle的类加载器加载。 6）否则，查找Dynamic Import列表的Bundle，委派给对应Bundle的类加载器加载。 7）否则，类查找失败。 说明:只有开头两点仍然符合双亲委派模型的原则，其余的类查找都是在平级的类加载器中进行的 小结: 这里，我们使用了“被破坏”这个词来形容上述不符合双亲委派模型原则的行为，但这里“被破坏”并不一定是带有贬义的。只要有明确的目的和充分的理由，突破旧有原则无疑是一种创新。 正如:OSGi中的类加载器的设计不符合传统的双亲委派的类加载器架构，且业界对其为了实现热部署而带来的额外的高复杂度还存在不少争议，但对这方面有了解的技术人员基本还是能达成一个共识，认为OSGi中对类加载器的运用是值得学习的，完全弄懂了OSGi的实现，就算是掌握了类加载器的精粹。 4）热替换的实现热替换是指在程序的运行过程中，不停止服务，只通过替换程序文件来修改程序的行为。热替换的关键需求在于服务不能中断，修改必须立即表现正在运行的系统之中。基本上大部分脚本语言都是天生支持热替换的，比如: PHP，只要替换了PHP源文件，这种改动就会立即生效，而无需重启Web服务器。 但对Java来说，热替换并非天生就支持，如果一个类已经加载到系统中，通过修改类文件，并无法让系统再来加载并重新定义这个类。因此，在Java中实现这一功能的一个可行的方法就是灵活运用ClassLoader。 注意:由不同ClassLoader加载的同名类属于不同的类型，不能相互转换和兼容。即两个不同的ClassLoader加载同个类，在虚拟机内部，会认为这2个类是完全不同的。 根据这个特点，可以用来模拟热替换的实现，基本思路如下图所示: 89.沙箱安全机制 保护程序安全 保护 Java 原生的 JDK 代码 Java 安全模型的核心就是 Java 沙箱(Sandbox)。什么是沙箱？沙箱就是一个限制程序运行的环境 沙箱机制就是将 Java 代码限定在虚拟机(JVM)特定的运行范围中，并且严格限制代码对本地系统资源访问。通过这样的措施来保证对代码的有限隔离，防止对本地系统造成破坏 沙箱主要限制系统资源访问，那系统资源包括什么？CPU、内存、文件系统、网络。不同级别的沙箱对这些资源访问的限制也可以不一样 所有的 Java 程序运行都可以指定沙箱，可以定制安全策略 1）JDK1.0时期在Java中将执行程序分成本地代码和远程代码两种，本地代码默认视为可信任的，而远程代码则被看作是不受信的。对于授信的本地代码，可以访问一切本地资源。而对于非授信的远程代码在早期的Java实现中，安全依赖于沙箱（Sandbox）机制。如下图所示JDK1.0安全模型 2）JDK1.1时期JDK1.0中如此严格的安全机制也给程序的功能扩展带来障碍，比如当用户希望远程代码访问本地系统的文件时候，就无法实现。 因此在后续的Java1.1版本中，针对安全机制做了改进，增加了安全策略。允许用户指定代码对本地资源的访问权限。如下图所示JDK1.1安全模型 3）JDK1.2时期在Java1.2版本中，再次改进了安全机制，增加了代码签名。不论本地代码或是远程代码，都会按照用户的安全策略设定，由类加载器加载到虚拟机中权限不同的运行空间，来实现差异化的代码执行权限控制。如下图所示JDK1.2安全模型 4）JDK1.6时期虚拟机会把所有代码加载到不同的系统域和应用域。系统域部分专门负责与关键资源进行交互，而各个应用域部分则通过系统域的部分代理来对各种需要的资源进行访问。虚拟机中不同的受保护域(Protected Domain)，对应不一样的权限（Permission)。存在于不同域中的类文件就具有了当前域的全部权限，如下图所示，最新的安全模型（jdk1.6) 90.自定义类加载器1）为什么要自定义类加载器? 隔离加载类 在某些框架内进行中间件与应用的模块隔离，把类加载到不同的环境。比如:阿里内某容器框架通过自定义类加载器确保应用中依赖的jar包不会影响到中间件运行时使用的jar包。再比如: Tomcat这类web应用服务器，内部自定义了好几种类加载器，用于隔离同一个web应用服务器上的不同应用程序。（类的仲裁–&gt;类冲突） 修改类加载的方式 类的加载模型并非强制，除Bootstrap外，其他的加载并非一定要引入，或者根据实际情况在某个时间点进行按需进行动态加载 扩展加载源 比如从数据库、网络、甚至是电视机机顶盒进行加载 防止源码泄漏 Java代码容易被编译和篡改，可以进行编译加密。那么类加载也需要自定义，还原加密的字节码。 2）常见的场景 实现类似进程内隔离，类加载器实际上用作不同的命名空间，以提供类似容器、模块化的效果。例如，两个模块依赖于某个类库的不同版本，如果分别被不同的容器加载，就可以互不干扰。这个方面的集大成者是Java EE和OSGI、JPMS等框架。 应用需要从不同的数据源获取类定义信息，例如网络数据源，而不是本地文件系统。或者是需要自己操纵字节码，动态修改或者生成类型。 3）注意:在一般情况下，使用不同的类加载器去加载不同的功能模块，会提高应用程序的安全性。但是，如果涉及Java类型转换，则加载器反而容易产生不美好的事情。在做Java类型转换时，只有两个类型都是由同一个加载器所加载，才能进行类型转换，否则转换时会发生异常。 4）实现方式用户通过定制自己的类加载器，这样可以重新定义类的加载规则，以便实现一些自定义的处理逻辑。 ①实现方式Java提供了抽象类java.lang.ClassLoader，所有用户自定义的类加载器都应该继承ClassLoader类。 在自定义ClassLoader 的子类时候，我们常见的会有两种做法: 方式一:重写loadclass()方法 重写findClass()方法【推荐】 ②对比这两种方法本质上差不多，毕竟loadClass()也会调用findClass()，但是从逻辑上讲我们最好不要直接修改loadClass()的内部逻辑。建议的做法是只在findClass()里重写自定义类的加载方法，根据参数指定类的名字，返回对应的Class对象的引用。 loadClass()这个方法是实现双亲委派模型逻辑的地方，擅自修改这个方法会导致模型被破坏，容易造成问题。因此我们最好是在双亲委派模型框架内进行小范围的改动，不破坏原有的稳定结构。同时，也避免了自己重写loadClass()方法的过程中必须写双亲委托的重复代码，从代码的复用性来看，不直接修改这个方法始终是比较好的选择。 当编写好自定义类加载器后，便可以在程序中调用loadClass(）方法来实现类加载操作。 ③说明 其父类加载器是系统类加载器 JVM中的所有类加载都会使用java.lang.ClassLoader.loadClass(String)接口(自定义类加载器并重写java.lang.ClassLoader.loadClass(String)接口的除外)，连JDK的核心类库也不能例外。 91.类加载器Java9新特性为了保证兼容性，JDK 9没有从根本上改变三层类加载器架构和双亲委派模型，但为了模块化系统的顺利运行，仍然发生了一些值得被注意的变动。 1.扩展机制被移除，扩展类加载器由于向后兼容性的原因被保留，不过被重命名为平台类加载器（platform classloader)。可以通过ClassLoader的新方法getPlatformClassLoader()来获取。 JDK 9时基于模块化进行构建（原来的 rt.jar 和 tools.jar 被拆分成数十个 JMOD 文件)，其中的Java类库就已天然地满足了可扩展的需求，那自然无须再保留\\lib\\ext 目录，此前使用这个目录或者 java.ext.dirs系统变量来扩展JDK功能的机制已经没有继续存在的价值了。 2.平台类加载器和应用程序类加载器都不再继承自 java.net.URLClassLoader。 现在启动类加载器、平台类加载器、应用程序类加载器全都继承于jdk.internal.loader.BuiltinClassLoader。 如果有程序直接依赖了这种继承关系，或者依赖了URLClassLoader类的特定方法，那代码很可能会在 JDK9及更高版本的JDK中崩溃。 3.在Java 9中，类加载器有了名称。该名称在构造方法中指定，可以通过getName()方法来获取。平台类加载器的名称是platform，应用类加载器的名称是app。类加载器的名称在调试与类加载器相关的问题时会非常有用。 4.启动类加载器现在是在jvm内部和java类库共同协作实现的类加载器〈以前是C++实现)，但为了与之前代码兼容，在获取启动类加载器的场景中仍然会返回null，而不会得到BootClassLoader实例。 5.类加载的委派关系也发生了变动。 当平台及应用程序类加载器收到类加载请求，在委派给父加载器加载前，要先判断该类是否能够归属到某一个系统模块中，如果可以找到这样的归属关系，就要优先委派给负责那个模块的加载器完成加载。 在Java模块化系统明确规定了三个类加载器负责各自加载的模块。 92.jvm监控 JPS-查看正在运行的java进程1）概述jps：显式指定系统内所有的HotSpot虚拟机进程（查看虚拟机进程信息），可用于查询正在运行的虚拟机进程。 说明：对于本地虚拟机进程来说，进程的本地虚拟机ID与操作系统的进程ID时一致的，是唯一的。 2）基本语法1jps [options] [hostid] //可以通过追加参数，打印额外信息 ①options参数1-q :仅仅显示LVMID (local virtual machine id), 即本地虚拟机唯一-id。 不显示主类的名称等 1-l:输出应用程序主类的全类名或如果进程执行的是jar包，则输出jar完整路径 1-m:输出虚拟机进程启动时传递给主类main()的参数 1-v: 列出虚拟机进程启动时的JVM参数。比如: -Xms20m -Xmx50m是 启动程序指定的jvm参数。 说明:以上参数可以综合使用。 补充: 如果某Java进程关闭了默认开启的UsePerfData参数(即使用参数**-XX:-UsePerfData**) ，那么jps命令(以及下面介绍的jstat)将无法探知该Java进程。 如何将信息输入到同级文件中： 语法：命令 &gt; 文件名称 ②hostid参数RMI注册表中注册的主机名。 如果想要远程监控主机上的java 程序，需要安装jstatd。 对于具有更严格的安全实践的网络场所而言，可能使用-一个自定义的策略文件来显示对特定的可信主机或网络的访问，尽管这种技术容易受到IP地址欺诈攻击。 如果安全问题无法使用一个定制的策略文件来处理，那么最安全的操作是不运行 jstatd服务器， 而是在本地使用jstat和jps工具。 93.jvm监控 jstat-查看JVM统计信息1）概述jstat(JVM Statistics Monitoring Tool): 用于监视虚拟机各种运行状态信息的命令行工具。它可以显示本地或者远程虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。 在没有GUI图形界面，只提供了纯文本控制台环境的服务器上，它将是运行期定位虚拟机性能问题的首选工具。常用于检测垃圾回收问题以及内存泄漏问题。 2）基本语法1jstat -&lt;option&gt; [-t] [-h &lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;] ] 其中vmid是进程id号 ①option参数选项option可以由以下值构成。 ●类装载相关的: 1-class: 显示ClassLoader的相关信息: 类的装载、卸载数量、总空间、类装载所消耗的时间等 ●垃圾回收相关的: 1-gc: 显示与GC相关的堆信息。包括Eden区、两个Survivor区、 老年代、永久代等的容量、己用空间、GC时间合计等信息。 1-gccapacity:显示内容与-gc基本相同，但输出主要关注Java堆各个区域使用到的最大、最小空间。 1-gcutil:显示内容与-gc基本相同，但输出主要关注已使用空间占总空间的百分比。 1-gccause:与-gcuti1功能一样，但是会额外输出导致最后-一次或当前正在发生的GC产生的原因。 1-gcnew: 显示新生代GC状况 1-gcnewcapacity: 显示内容与-gcnew基 本相同，输出主要关注使用到的最大、最小空间 1-geold: 显示老年代GC状况 1-gcoldcapacity: 显示内容与-gcold基本相同，输出主要关注使用到的最大、最小空间 1-gcpermcapacity:显示永久代使用到的最大、最小空间。 ●JIT相关的: 1-compiler: 显示JIT编译器编译过的方法、耗时等信息 1-printcompilation: 输出已经被JIT编译的方法 以-GC为例： 新生代相关 1S0C是第一个幸存者区的大小（字节） 1S1C是第二个幸存者区的大小（字节） 1S0U是第一个幸存者区已使用的大小（字节） 1S1U是第二个幸存者区已使用的大小（字节） 1EC是Eden空间的大小（字节） 1EU是Eden空间已使用大小（字节） 老年代相关 1OC是老年代的大小（字节） 1OU是老年代已使用的大小（字节） 方法区相关 1MC是方法区的大小 1MU是方法区已使用的大小 1CCSC是压缩类空间的大小 1CCSU是压缩类空间已使用的大小 其他 1YGC是从应用程序启动到采样时young gc的次数 1YGCT是指从应用程序启动到采样时young gc消耗时间（秒） 1FGC是从应用程序启动到采样时full gc的次数 1FGCT是从应用程序启动到采样时的full gc的消耗时间（秒） 1GCT是从应用程序启动到采样时gc的总时间 ②interval参数用于指定输出统计数据的周期，单位为毫秒。即：查询间隔 ③count参数用于指定查询的总次数 ④-t参数可以在输出信息前加上一个Timestamp列，显示程序的运行时间。单位：秒 我们可以比较Java进程的启动时间以及总GC时间(GCT列)，或者两次测量的间隔时间以及总GC时间的增量，来得出GC时间占运行时间的比例。 如果该比例超过20%， 则说明目前堆的压力较大;如果该比例超过90%，则说明堆里几乎没有可用空间，随时都可能抛出00M异常。 我们执行jstat -gc -t 13152 1000 10，这代表1秒打印出1行，一共10行，-t代表打印出Timestamp总运行时间，结果如下所示： ⑤-h参数可以在周期性数据输出时，输出多少行数据后输出一个表头信息 3）补充jstat还可以用来判断是否出现内存泄漏。 第1步: 在长时间运行的Java程序中，我们可以运行jstat命令连续获取多行性能数据，并取这几行数据中ou列(即己占用的老年代内存)的最小值。 第2步: 然后，我们每隔- 段较长的时间重复- -次上述操作， 来获得多组ou最小值。 如果这些值呈 上 涨趋势，则说明该Java 程序的老年代内存已使用量在不断上涨，这意味着无法回收的对象在不断增加，因此很有可能存在内存泄漏。 94.jvm监控 jinfo-实时查看和修改JVM参数1）概述jinfo(Configuration Info for Java) 查看虛拟机配置参数信息，也可用于调整虚拟机的配置参数。 在很多情况不，Java应用程序不会指定所有的Java虚拟机参数。而此时，开发人员可能不知道某一个具体的Java虛拟机参数的默认值。在这种情况下，可能需要通过查找文档获取某个参数的默认值。这个查找过程可能是非常艰难的。但有了jinfo工具，开发人员可以很方便地找到Java虛拟机参数的当前值。 2）基本语法①查看jinfo -sysprops 进程id 可以查看由System.getProperties()取得的参数 jinfo -flags 进程id 查看曾经赋过值的一些参数 jinfo -flag 参数名称 进程id 查看某个java进程的具体参数信息 ②修改jinfo不仅可以查看运行时某-个Java虚拟机参数的实际取值，甚至可以在运行时修改部分参数，并使之立即生效。 但是，并非所有参数都支持动态修改。参数只有被标记为manageable的f1ag可以被实时修改其实，这个修改能力是极其有限的。 可以查看被标记为manageable的参数 1java -XX: +PrintFlagsFinal -version| grep manageable 针对boolean类型 1jinfo -flag [+|-]参数名称 进程id 1jinfo -flag +PrintGCDetails 18232 针对非boolean类型 1jinfo -flag 参数名称=参数值 进程id 1jinfo -flag MaxHeapFreeRatio=90 8600 3）拓展java -XX:+PrintFlagsInitial 查看所有JVM参数启动的初始值 java -XX:+PrintFlagsFinal查看所有JVM参数的最终值 java -参数名称:+PrintCommandLineFlags查看那些已经被用户或者JVM设置过的详细的XX参数的名称和值 95.jvm监控 jmap-导出内存映像文件&amp;内存使用情况1）概述jmap(JVM Memory Map): 作用一 方面是获取dump文件(堆转储快照文件，二进制文件)，它还可以获取目标Java进程的内存相关信息，包括Java堆各区域的使用情况、堆中对象的统计信息、类加载信息等。 开发人员可以在控制台中输入命令“jmap -help“查阅jmap工具的具体使用方式和一 些标准选项配置。 2）基本语法123jmap [option] &lt;pid&gt;jmap [option] &lt;executable &lt;core&gt;jmap [option] [server_id@]&lt;remote server IP or hostname&gt; 123使用语法可以通过在DOS窗口中使用jmap/jmap -h/jmap -help查看&lt;executable &lt;core&gt;代表可执行的代码，比如使用&gt; 文件名称来指定生成的dump文件的生成位置[server_id@]&lt;……&gt;是为远程连接准备的 1-dump:生成Java堆转储快照：dump文件,特别的：-dump:live只保存堆中的存活对象 1-heap:输出整个堆空间的详细信息，包括GC的使用、堆配置信息，以及内存的使用信息等 1-histo:输出堆中对象的同级信息，包括类、实例数量和合计容量,特别的：-histo:live只统计堆中的存活对象 1-permstat:以ClassLoader为统计口径输出永久代的内存状态信息(仅linux/solaris平台有效) 1-finalizerinfo:显示在F-Queue中等待Finalizer线程执行finalize方法的对象(仅linux/solaris平台有效) 1-F:当虚拟机进程对-dump选项没有任何响应时，可使用此选项强制执行生成dump文件(仅linux/solaris平台有效) 3)导出内存映像文件一般来说， 使用jmap指 令生成dump文件的操作算得上是最常用的jmap命令之一， 将堆中所有存活对象导出至一一个文件之中。 Heap Dump又叫做堆存储文件，指-个Java进 程在某个时间点的内存快照。 说明: 1.通常在写Heap Dump文件前会触发一 次Fu1l GC， 所以heap dump文件里保存的都是Fu11GC后留下的对象信息。 2.由于生成dump文件比较耗时，因此大家需要耐心等待，尤其是大内存镜像生成dump文件则需要耗费更长的时间来完成。 注意： 对于以上说明中的第1点是自动方式才会这样做，而手动不会在Full GC之后生成Dump 使用手动方式生成dump文件，一般指令执行之后就会生成，不用等到快出现OOM的时候 使用自动方式生成dump文件，当出现OOM之前先生成dump文件 如果使用手动方式，一般使用第2种，毕竟生成堆中存活对象的dump文件是比较小的，便于传输和分析 ①手动12jmap -dump:format=b,file=&lt;filename.hprof&gt; &lt;pid&gt;jmap -dump:live,format=b,file=&lt;filename.hprof&gt; &lt;pid&gt; 由于jmap将访问堆中的所有对象，为了保证在此过程中不被应用线程干扰，jmap 需要借助安 全点机制，让所有线程停留在不改变堆中数据的状态。也就是说，由jmap 导出的堆快照必定是安全点位置的。这可能导致基于该堆快照的分析结果存在偏差。 举个例子，假设在编译生成的机器码中，某些对象的生命周期在两个安全点之间，那么:live选项将无法探知到这些对象。 另外，如果某个线程长时间无法跑到安全点，jmap将一 直等 下去。与前面讲的jstat则不同，垃圾回收器会主动将jstat所需要的摘要数据保存至固定位置之中，而jstat只需直接读取即可。 ②自动12-XX:+HeapDumpOnOutOfMemoryError-XX:HeapDumpPath=&lt;filename.hprof&gt; 当程序发生00M退出系统时，一些瞬时信息都随着程序的终止而消失，而重现00M问题往往比较困难或者耗时。此时若能在0OM时，自动导出dump文件就显得非常迫切。 这里介绍一种比较常用的取得堆快照文件的方法，即使用:-XX: +HeapDump0nOutOfMemoryError:在程序发生0OM时，导出应用程序的当前堆快照。-XX:HeapDumpPath:可以指定堆快照的保存位置。 比如:-Xmx100m -XX: +HeapDumpOnOutOfMemoryError -XX: HeapDumpPath=D: \\m . hprof 4)显示堆内存相关信息①jmap -heap 进程idjmap -heap 进程id只是时间点上的堆信息，而jstat后面可以添加参数，可以指定时间动态观察数据改变情况，而图形化界面工具，例如jvisualvm等，它们可以用图表的方式动态展示出相关信息，更加直观明了 1jmap -heap 3540 &gt;a.txt ②jmap -histo 进程id输出堆中对象的同级信息，包括类、实例数量和合计容量，也是这一时刻的内存中的对象信息 1jmap -histo 3540 &gt;b.txt 5）其他作用这两个指令仅linux/solaris平台有效，所以无法在windows操作平台上演示。 jmap -permstat 进程id 查看系统的ClassLoader信息 jmap -finalizerinfo查看堆积在finalizer队列中的对象 96.jvm监控 jhat-JDK自带堆分析工具jhat命令在jdk9及其之后就被移除了，官方建议使用jvisualvm代替jhat，所以该指令只需简单了解一下即可. 1)概述Sun JDK提供的jhat命令与jmap命令搭配使用，用于分析jmap生成的heap dump文件(堆转 储快照)。jhat内置了-个微 型的HTTP/HTML服务器， 生成dump 文件的分析结果后，用户可以在浏览器中查看分析结果(分析虚拟机转储快照信息)。 使用了jhat命令， 就启动了一个http服务，端口是7000， 即http://localhost:7000/,就可以在浏览器里分析。 说明: jhat 命令在JDK9、JDK10中已经被删除，官方建议用VisualVM代替。 2）基本语法1jhat [option] [dumpfile] 其中dumpfile代表dump文件的地址以及名称,jhat d:\\3.hprof。 ①options参数 -stack falseltrue 关闭|打开对象分配调用栈跟踪 -refs falseltrue 关闭|打开对象引用跟踪 -port port-number 设置jhat HTTP，Server的端口号，默认7000 -exclude exclude-file 执行对象查询时需要排除的数据成员 -baseline exclude-file 指定一个基准堆转储 -debug int 设置debug级别 -version 启动后显示版本信息就退出 -J 传入启动参数，比如-J -Xmx512m 97.jvm监控 jstack-打印JVM中线程快照1）概述jstack(JVM Stack Trace): 用于生成虚拟机指定进程当前时刻的线程快照(虚拟机堆栈跟踪)。线程快照就是当前虛拟机内指定进程的每–条线程正在执行的方法堆栈的集合。 生成线程快照的作用:可用于定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等问题。这些都是导致线程长时间停顿的常见原因。当线程出现停顿时，就可以用jstack显示各个线程调用的堆栈情况。 在thread dump中， 要留意下面几种状态： 1死锁，Deadlock (重点关注) 1等待资源，Waiting on condition (重点关注) 1等待获取监视器，Waiting on monitor entry (重点关注) 1阻塞，Blocked (重点关注) 1执行中，Runnable 1暂停，Suspended 1对象等待中，object .wait()或TIMED_ WAITING 1停止，Parked 2）基本语法1jstack option pid 如果程序出现等待问题，可以使用该指令去查看问题所在。 ①option参数1-F：当正常输出的请求不被响应时，强制输出线程堆栈 1-l：除堆栈外，显示关于锁的附加信息 1-m：如果调用本地方法的话，可以显示C/C++的堆栈 1-h：帮助操作 98.jvm监控 jcmd-多功能命令行1）概述在JDK 1.7以后，新增了一个命令行工具jcmd。 它是一个多功能的工具，可以用来实现前面除了jstat之外所有命令的功能。比如:用它来导出堆、内存使用、查看Java进程、导出线程信息、执行GC、JVM运行时间等。 jcmd拥有jmap的大部分功能， 并且在0racle的官方网站上也推荐使用jcmd命令代jmap命令。 2）语法1jcmd -l 列出所有的JVM进程 1jcmd 进程号 help 针对指定的进程，列出支持的所有具体命令 1jcmd 进程号 具体命令 显示指定进程的指令命令的数据 99.jvm监控 jstatd-远程主机信息收集之前的指令只涉及到监控本机的Java应用程序，而在这些工具中，一些监控工具也支持对远 程计算机的监控(如jps、 jstat)。为了启用远程监控，则需要配合使用jstatd 工具。 命令jstatd是-一个RMI服务端程序，它的作用相当于代理服务器，建立本地计算机与远程监 控工具的通信。 jstatd服务器将本机的Java应用程序信息传递到远程计算机。 100.jvm监控 Eclipse MAT1）概述功能强大的java堆内存分析器，可以用于查找堆内存泄漏和内存消耗情况。MAT是基于eclipse开发的，可以单独使用或者以插件形式嵌入到eclipse。 2）获取堆dump文件①dump文件内存MAT可以分析heap dump文件。在进行内存分析时，只要获得了反映当前设备内存映像的hprof文件，通过MAT打开就可以直观地看到当前的内存信息。 一般说来，这些内存信息包含: ●所有的对象信息，包括对象实例、成员变量、存储于栈中的基本类型值和存储于堆中的其他对象的引用值。 ● 所有的类信息，包括classloader、 类名称、父类、静态变量等 GCRoot到所有的这些对象的引用路径 线程信息，包括线程的调用栈及此线程的线程局部变量(TLS) 2）说明说明1:缺点: MAT 不是一个万能工具，它并不能处理所有类型的堆存储文件。但是比较主流的厂家和格式，例如 Sun, HP, SAP所采用的HPROF 二进制堆存储文件，以及IBM的PHD堆存储文件等都能被很好的解析。 说明2： 最吸引人的还是能够快速为开发人员生成内存泄漏报表，方便定位问题和分析问题。虽然MAT有如此强大的功能，但是内存分析也没有简单到-键完成的程度，很多内存问题还是需要我们从MAT展现给我们的信息当中通过经验和直觉来判断才能发现。 3）获取dump文件方法一:通过前一章介绍的jmap工具生成，可以生成任意-一个java进程的dump文件; 方法二:通过配置JVM参数生成。 ● 选项”-XX: +HeapDumpOnOutOfMemoryError”或”-XX: +HeapDumpBeforeFullGC” ● 选项”-XX:HeapDumpPath”所代表的含义就是当程序出现0utofMemory时，将会在相应的目录下 生成一份dump文件。如果不指定选项“-XX:HeapDumpPath” 则在当前目录下生成dump文件。 对比:考虑到生产环境中几乎不可能在线对其进行分析，大都是采用离线分析，因此使用jmap+MAT工具是最常见的组合。 方法三:使用VisualVM可以导出堆dump文件 方法四: 使用MAT既可以打开一个已有的堆快照，也可以通过MAT直接从活动Java程序中导出堆快照。 该功能将借助jps列出当前正在运行的Java 进程，以供选择并获取快照。 4）分析堆dump文件①histogram展示了各个类的实例数目以及这些实例的Shallow heap或者Retained heap的总和。 具体内容： ②thread overview查看系统中的Java线程 查看局部变量的信息 ③获得对象互相引用的关系1with outgoing references 1with incoming references ④浅堆与深堆1shallow heap 对象头代表根据类创建的对象的对象头，还有对象的大小不是可能向8字节对齐，而是就向8字节对齐。 浅堆(Shallow Heap)是指一个对象所消耗的内存。在32位系统中，一个对象引用会占据4个字节，一个int类型会占据4个字节，long型变量会占据8个字节，每个对象头需要占用8个字节。根据堆快照格式不同，对象的大小可能会成8字节进行对齐。 以String为例: 2 个int值共占8字节，对象引用占用4字节，对象头8字节，合计20字节，向8字节对齐，故占24字节。(jdk7中) int hash32 0 int hash 0 ref value C:\\Users\\ASUS 这24字节为String对象的浅堆大小。它与String的value实际取值无关，无论字符串长度如何，浅堆大小始终是24字节。 1retained heap 保留集(Retained Set): 对象A的保留集指当对象A被垃圾回收后，可以被释放的所有的对象集合(包括对象A本身)，即对象A的保留集可以被认为是只能通过对象A被直接或间接访问到的所有对象的集合。通俗地说，就是指仅被对象A所持有的对象的集合。 深堆(Retained Heap): 深堆是指对象的保留集中所有的对象的浅堆大小之和。 注意:浅堆指对象本身占用的内存，不包括其内部引用对象的大小。一个对象的深堆指只能通过该对象访问到的(直接或间接)所有对象的浅堆之和，即对象被回收后，可以释放的真实空间。 1对象实际大小 另外一个常用的概念是对象的实际大小。这里，对象的实际大小定义为一个对象所能触及的所有对象的浅堆大小之和，也就是通常意义上我们说的对象大小。与深堆相比，似乎这个在日常开发中更为直观和 被人接受， 但实际上，这个概念和垃圾回收无关。 下图显示了一个简单的对象引用关系图，对象A引用了C和D，对象B引用了C和E。那么对象A的浅堆大 小只是A本身， 不含C和D,而A的实际大小为A、 C、D三者之 和。而A的深堆大小为A与D之和， 由于对象C还可以通过对象B访问到，因此不在对象A的深堆范围内。 1练习 A对象的Retained Size=A对象的Shallow Size B对象的Retained Size=B对象的Shallow Size+C对象的Shallow Size 这里不包括D对象，因为D对象被GC Roots直接引用 如果GC Roots不引用D对象呢？ 1案例分析：StudentTrace 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 有一个学生浏览网页的记录程序，它将记录 每个学生访问过的网站地址。 * 它由三个部分组成：Student、WebPage和StudentTrace三个类 * * -XX:+HeapDumpBeforeFullGC -XX:HeapDumpPath=c:\\code\\student.hprof * @author shkstart * @create 16:11 */public class StudentTrace &#123; static List&lt;WebPage&gt; webpages = new ArrayList&lt;WebPage&gt;(); public static void createWebPages() &#123; for (int i = 0; i &lt; 100; i++) &#123; WebPage wp = new WebPage(); wp.setUrl(&quot;http://www.&quot; + Integer.toString(i) + &quot;.com&quot;); wp.setContent(Integer.toString(i)); webpages.add(wp); &#125; &#125; public static void main(String[] args) &#123; createWebPages();//创建了100个网页 //创建3个学生对象 Student st3 = new Student(3, &quot;Tom&quot;); Student st5 = new Student(5, &quot;Jerry&quot;); Student st7 = new Student(7, &quot;Lily&quot;); for (int i = 0; i &lt; webpages.size(); i++) &#123; if (i % st3.getId() == 0) st3.visit(webpages.get(i)); if (i % st5.getId() == 0) st5.visit(webpages.get(i)); if (i % st7.getId() == 0) st7.visit(webpages.get(i)); &#125; webpages.clear(); System.gc(); &#125;&#125;class Student &#123; private int id; private String name; private List&lt;WebPage&gt; history = new ArrayList&lt;&gt;(); public Student(int id, String name) &#123; super(); this.id = id; this.name = name; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public List&lt;WebPage&gt; getHistory() &#123; return history; &#125; public void setHistory(List&lt;WebPage&gt; history) &#123; this.history = history; &#125; public void visit(WebPage wp) &#123; if (wp != null) &#123; history.add(wp); &#125; &#125;&#125;class WebPage &#123; private String url; private String content; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125;&#125; 结论： elementData数组的浅堆是80个字节，而elementData数组中的所有WebPage对象的深堆之和是1208个字节，所以加在一起就是elementData数组的深堆之和，也就是1288个字节。 解释： 我说“elementData数组的浅堆是80个字节”，其中15个对象一共是60个字节，对象头8个字节，数组对象本身4个字节，这些的和是72个字节，然后总和要是8的倍数，所以“elementData数组的浅堆是80个字节” 我说“WebPage对象的深堆之和是1208个字节”，一共有15个对象，其中0、21、42、63、84、35、70不仅仅是7的倍数，还是3或者5的倍数，所以这几个数值对应的i不能计算在深堆之内，这15个对象中大多数的深堆是152个字节，但是i是0和7的那两个深堆是144个字节，所以(13152+1442)-(6*152+144)=1208，所以这也印证了我上面的话，即“WebPage对象的深堆之和是1208个字节”。 因此“elementData数组的浅堆80个字节”加上“WebPage对象的深堆之和1208个字节”，正好是1288个字节，说明“elementData数组的浅堆1288个字节”。 ⑤支配树支配树( Dominator Tree)支配树的概念源自图论。 MAT提供了一个称为支配树(Dominator Tree)的对象图。支配树体现了对象实例间的支配关系。在对象引用图中，所有指向对象B的路径都经过对象A，则认为对象A支配对象B。如果对象A是离对象B最近的一个支配对象，则认为对象A为对象B的直接支配者。支配树是基于对象间的引用图所建立的，它有以下基本性质: ●对象A的子树(所有被对象A支配的对象集合)表示对象A的保留集(retained set) ，即深堆。 ●如果对象A支配对象B，那么对象A的直接支配者也支配对象B。 ●支配树的边与对象引用图的边不直接对应。 如下图所示:左图表示对象引用图，右图表示左图所对应的支配树。对象A和B由根对象直接支配，由于在到对象C的路径中，可以经过A，也可以经过B，因此对象C的直接支配者也是根对象。对象F与对象D相互引用，因为到对象F的所有路径必然经过对象D，因此，对象D是对象F的直接支配者。而到对象D的所有路径中，必然经过对象C，即使是从对象F到对象D的引用，从根节点出发，也是经过对象C的，所以，对象D的直接支配者为对象C。 同理，对象E支配对象G。到达对象H的可以通过对象D，也可以通过对象E，因此对象D和E都不能支配对象H，而经过对象C既可以到达对象D也可以到达对象E，因此对象C为对象H的直接分配者。 注意： 跟随我一起来理解如何从“对象引用图—》支配树”，首先需要理解支配者（如果要到达对象B，毕竟经过对象A，那么对象A就是对象B的支配者，可以想到支配者大于等于1），然后需要理解直接支配者（在支配者中距离对象B最近的对象A就是对象B的直接支配者，你要明白直接支配者不一定就是对象B的上一级，然后直接支配者只有一个），然后还需要理解支配树是怎么画的，其实支配树中的对象与对象之间的关系就是直接支配关系，也就是上一级是下一级的直接支配者，只要按照这样的方式来作图，肯定能从“对象引用图—》支配树”。 在Eclipse MAT工具中如何查看支配树： 5）案例：Tomcat堆溢出分析①说明Tomcat是最常用的Java Servlet容器之一，同时也可以当做单独的Web服务器使用。Tomcat本身使 用Java实现，并运行于Java虚拟机之上。在大规模请求时，Tomcat有 可能会因为无法承受压力而发生内存溢出错误。这里根据一个被压垮的Tomcat的堆快照文件，来分析Tomcat在崩溃时的内部情况。 ②分析过程 6）支持使用OQL语言查询对象信息SELECT子句 FROM子句 WHERE子句 内置对象与方法 101.jvm监控 Arthas 1）基本概述①背景不需要远程连接，也不需要配制监控参数，同时也提供了丰富的性能监控数据 ②概述Arthas (阿尔萨斯) 是Alibaba开源的Java诊断工具，深受开发者喜爱。在线排查问题，无需重启 ;动态跟踪Java代码; 实时监控JVM状态。 Arthas 支持JDK 6+， 支持Linux/Mac/Windows, 采用命令行交互模式，同时提供丰富的Tab 自动补全功能，进–步方便进行问题的定位和诊断。 当你遇到以下类似问题而束手无策时，Arthas 可以帮助你解决: ●这个类从哪个jar包加载的?为什么会报各种类相关的Exception? ●我改的代码为什么没有执行到?难道是我没commit?分支搞错了? ●遇到问题无法在线上debug, 难道只能通过加日志再重新发布吗? ●线上遇到某个用户的数据处理有问题，但线上同样无法debug,线下无法重现! ●是否有一个全局视角来查看系统的运行状况? ●有什么办法可以监控到JVM的实时运行状态? ●怎么快速定位应用的执占，生成火焰图2 ③基于哪些工具开发而来greys- anatomy: Arthas代码基 于Greys =二次开发而来，非常感谢Greys之前所有的工作，以及Greys原作者对Arthas提出的意见和建议! termd: Arthas 的命令行实现基于termd开发，是一款优秀的命令行程序开发框架，感谢termd提供了优秀的框架。 crash: Arthas的文本渲染功能基于crash中的文本渲染功能开发，可以从这里看到源码，感谢crash在这方面所做的优秀工作。 cli: Arthas的命令行界面基于vert. x提供的cli库进行开发，感谢vert . x在这方面做的优秀工作。 compiler Arthas 里的内存编绎器代码来源 Apache Commons Net Arthas 里的Telnet Client 代码来源 JavaAgent:运行在main方法之前的拦截器，它内定的方法名叫premain ，也就是说先执行premain方法然后再执行main方法 ASM:一个通用的Java字节码操作和分析框架。它可以用于修改现有的类或直接以二进制形式动 态生成类。ASM提供了一些常见的字节码转换和分析算法，可以从它们构建定制的复杂转换和代码分析工具。ASM提供了与其他Java字节码框架类似的功能，但是主要关注性能。因为它被设计和实现得尽可能小和快，所以非常适合在动态系统中使用(当然也可以以静态方式使用，例如在编译器中) ④官方文档1https://arthas.aliyun.com/doc/quick-start.html 2）安装与使用①安装1wget https://alibaba.github.io/arthas/arthas-boot.jar ②工程目录arthas-agent:基于JavaAgent技术的代理 bin:–些启动脚本 arthas- boot: Java版 本的一键安装启动脚本 arthas-client: telnet client代码 arthas - common:一些共用的工具类和枚举类 arthas-core:核心库，各种arthas命令的交互和实现 arthas-demo:示例代码 arthas -memorycompiler:内存编绎器代码，Fork from https://github.com/skalogs/SkaETL/tree/master/compiler arthas-packaging: maven打 包相关的 arthas-site: arthas站点 arthas-spy:编织到目标类中的各个切面 static:静态资源 arthas-testcase:测试 ③启动1java -jar arthas-boot.jar ④查看进程1jps ⑤查看日志1cat ~/logs/arthas/arthas.log ⑥查看帮助1java -jar arthas-boot.jar -h ⑦web console除了在命令行查看外，Arthas 目前还支持Web Console。 在成功启动连接进程之后就已经自动启 动，可以直接访问http://127.0.0.1:8563/ 访问，页面上的操作模式和控制台完全一样。 ⑧退出最后一行[arthas@7457]$， 说明打开进入了监控客户端，在这里就可以执行相关命令进行查看了。 使用quit\\exit:退出当前客户端 使用stop\\shutdown:关闭arthas服 务端,并退出所有客户端。 3）相关诊断指令①基础指令●help- -查看命令帮助信息 ●cat- - -打印文件内容,和linux里的cat命令类似 ●echo-打印参数，和linux里的echo命令类似 ●grep– -匹配查找，和linux里的grep命令类似 ●tee– -复制标准输入到标准输出和指定的文件，和linux里的tee命令类似 pwd- - -返回当前的工作目录，和linux命令类似 ●cls– -清空当前屏幕区域 ●session- - 查看当前会话的信息 ●reset– 重置增强类，将被Arthas增强过的类全部还原，Arthas服务端关闭时会重置所有增强过的类 ●version– 输出当前目标Java进程所加载的Arthas版本号 ●history- – 打印命令历史 ●quit- -退出当前Arthas客户端，其他Arthas客户端不受影响 ●stop–关闭Arthas服务端，所有Arthas客户端全部退出 ●keymap- - - Arthas快捷键列表及自定义快捷键 ②jvm相关●dashboard- -当前系统的实时数据面板 ●thread- - _查看当前JVM的线程堆栈信息 ●jvm–查看当前JVM的信息 ●sysprop–查看和修改JVM的系统属性 ●sysenv- –查看JVM的环境变量 ●vmoption- – -查看和修改JVM里诊断相关的option ●perfcounter- – 查看当前JVM的Perf Counter信息 ●logger–查看和修改logger ●getstatic– 查看类的静态属性 ●ognl- – 执行ogn装达式 ●mbean– _查看Mbean的信息 ●heapdump- – -dump java heap,类似jmap命令的heap dump功能 ③class/classloader相关●SC–查看JM已加载的类信息 ●sm–查看已加载类的方法信息 ●jad–反编译指定已加载类的源码 ●mc- -内存编译器,内存编译. java文件为.class文件 ●retransform- - - 加载外部的.class文件, retransform到JVM里， ● redefine– 加载外部的.class文件, redefine到JVM里 ●dump– -dump已加载类的byte code到特定目录 ●classloader– 查看classloader的继承树， urls, 类加载信息,使用classloader去getResource ④monitor/watch/trace相关monitor–方法抉行監控 watch- – -方法丸行数跼規測 trace– -方法内部凋用路径，并輸出方法路径上的毎个芍点上耗肘 stack- – -輸出当前方法被凋用的凋用路径 tt–方法丸行数据的吋空隧道，記彖下指定方法毎次調用的入参和返回信息，并能対文些不同的吋同下凋用迸行規測 ⑤其他使用&gt;将结果重写到日志文件，使用&amp;指令命令是后台运行，session断开不影响任务执行(生命周期默认为1天) jobs:列出所有job kill:强制终止任务 fg:将暂停的任务拉到前台执行 bg:将暂停的任务放到后台执行 grep:搜索满足条件的结果 plaintext :将命令的结果去除ANSI颜色 WC :;按行统计输出结果 options:查看或设置Arthas全局开关 profiler :使用async-profiler对应用采样，生成火焰图 102.再谈内存泄漏1）内存泄露的理解与分析可达性分析算法来判断对象是否是不再使用的对象，本质都是判断一个对象是否还被引用。那么对于这种情况下，由于代码的实现不同就会出现很多种内存泄漏问题(让JVM误以为此对象还在引用中，无法回收，造成内存泄漏)。 是否还被使用? 是 是否还被需要? 否 内存泄漏(memory leak) 的理解 严格来说，只有对象不会再被程序用到了，但是GC又不能回收他们的情况，才叫内存泄漏。 但实际情况很多时候一些不太好的实践(或疏忽)会导致对象的生命周期变得很长甚至导致0OM，也可以叫做宽泛意义上的“内存泄漏”。 对象X引用对象Y,X的生命周期比Y的生命周期长; 那么当Y生命周期结束的时候，X依然引用着Y，这时候，垃圾回收期是不会回收对象Y的; 如果对象X还引用着生命周期比较短的A、B、C,对象A又引用着对象a、b、c，这样就可能造成大量无用的对象不能被回收，进而占据了内存资源，造成内存泄漏，直到内存溢出。 内存泄漏与内存溢出的关系: 11.内存泄漏(memory leak ) 申请了内存用完了不释放，比如一共有1024M的内存，分配了512M 的内存一-直不回收，那么可以 用的内存只有512M 了，仿佛泄露掉了一部分; 通俗一点讲的话，内存泄漏就是[占着茅坑不拉shi]。 12.内存溢出(out of memory ) 申请内存时，没有足够的内存可以使用; 通俗一点儿讲，-一个厕所就三个坑，有两个站着茅坑不走的(内存泄漏)，剩下最后一个坑，厕所表示接待压力很大，这时候一下子来了两个人，坑位(内存)就不够了，内存泄漏变成内存溢出了。 可见，内存泄漏和内存溢出的关系:内存泄漏的增多，最终会导致内存溢出。 1泄漏的分类 经常发生:发生内存泄露的代码会被多次执行，每次执行，泄露一块内存; 偶然发生:在某些特定情况下才会发生 一次性:发生内存泄露的方法只会执行一次; 隐式泄漏:一 直占着内存不释放，直到执行结束;严格的说这个不算内存泄漏，因为最终释放掉了，但是如果执行时间特别长，也可能会导致内存耗尽。 2）Java中内存泄露的8种情况11-静态集合类 静态集合类，如HashMap、 LinkedList等等。 如果这些容器为静态的，那么它们的生命周期与JVM程序一致，则容器中的对象在程序结束之前将不能被释放，从而造成内存泄漏。简单而言，长生命周期的对象持有短生命周期对象的引用，尽管短生命周期的对象不再使用，但是因为长生命周期对象持有它的引用而导致不能被回收。 12-单例模式 单例模式，和静态集合导致内存泄露的原因类似，因为单例的静态特性，它的生命周期和JVM的生命周期一样长，所以如果单例对象如果持有外部对象的引用，那么这个外部对象也不会被回收，那么就会造成内存泄漏。 13-内部类持有外部类 内部类持有外部类，如果一个外部类的实例对象的方法返回了一个内部类的实例对象。 这个内部类对象被长期引用了，即使那个外部类实例对象不再被使用，但由于内部类持有外部类的实例对象，这个外部类对象将不会被垃圾回收，这也会造成内存泄漏。 14-各种连接，如数据库连接、网络连接和IO连接等 各种连接，如数据库连接、网络连接和IO连接等。 在对数据库进行操作的过程中，首先需要建立与数据库的连接，当不再使用时，需要调用close方法来释放与数据库的连接。只有连接被关闭后，垃圾回收器才会回收对应的对象。 否则，如果在访问数据库的过程中，对Connection、 Statement或ResultSet不显性地关闭，将会造成大量的对象无法被回收，从而引起内存泄漏。 15-变量不合理的作用域 变量不合理的作用域。一般而言，一个变量的定义的作用范围大于其使用范围，很有可能会造成内存泄漏。另一方面，如果没有及时地把对象设置为null,很有可能导致内存泄漏的发生。 16-改变哈希值 改变哈希值，当一个对象被存储进HashSet集合中以后，就不能修改这个对象中的那些参与计算哈希值的字段了。 否则，对象修改后的哈希值与最初存储进HashSet集合中时的哈希值就不同了，在这种情况下，即使在contains方法使用该对象的当前引用作为的参数去HashSet集合中检索对象，也将返回找不到对象的结果，这也会导致无法从HashSet集合中单独删除当前对象，造成内存泄漏。 这也是String为什么被设置成了不可变类型，我们可以放心地把String 存入HashSet,或者把 String当做HashMap 的key值; 当我们想把自己定义的类保存到散列表的时候，需要保证对象的hashCode不可变。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118public class ChangeHashCode &#123; public static void main(String[] args) &#123; HashSet set = new HashSet(); Person p1 = new Person(1001, &quot;AA&quot;); Person p2 = new Person(1002, &quot;BB&quot;); set.add(p1); set.add(p2); p1.name = &quot;CC&quot;;//导致了内存的泄漏 set.remove(p1); //删除失败 System.out.println(set); set.add(new Person(1001, &quot;CC&quot;)); System.out.println(set); set.add(new Person(1001, &quot;AA&quot;)); System.out.println(set); &#125;&#125;class Person &#123; int id; String name; public Person(int id, String name) &#123; this.id = id; this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (!(o instanceof Person)) return false; Person person = (Person) o; if (id != person.id) return false; return name != null ? name.equals(person.name) : person.name == null; &#125; @Override public int hashCode() &#123; int result = id; result = 31 * result + (name != null ? name.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125;例2：/** * 演示内存泄漏 * @author shkstart * @create 14:47 */public class ChangeHashCode1 &#123; public static void main(String[] args) &#123; HashSet&lt;Point&gt; hs = new HashSet&lt;Point&gt;(); Point cc = new Point(); cc.setX(10);//hashCode = 41 hs.add(cc); cc.setX(20);//hashCode = 51 此行为导致了内存的泄漏 System.out.println(&quot;hs.remove = &quot; + hs.remove(cc));//false hs.add(cc); System.out.println(&quot;hs.size = &quot; + hs.size());//size = 2 System.out.println(hs); &#125;&#125;class Point &#123; int x; public int getX() &#123; return x; &#125; public void setX(int x) &#123; this.x = x; &#125; @Override public int hashCode() &#123; final int prime = 31; int result = 1; result = prime * result + x; return result; &#125; @Override public boolean equals(Object obj) &#123; if (this == obj) return true; if (obj == null) return false; if (getClass() != obj.getClass()) return false; Point other = (Point) obj; if (x != other.x) return false; return true; &#125; @Override public String toString() &#123; return &quot;Point&#123;&quot; + &quot;x=&quot; + x + &#x27;&#125;&#x27;; &#125;&#125; 17-缓存泄露 内存泄漏的另一个常见来源是缓存，- -旦你把对象引用放入到缓存中，他就很容易遗忘。比如:之前项目在一次上线的时候，应用启动奇慢直到夯死，就是因为代码中会加载-个表中的数据到缓存(内存)中，测试环境只有几百条数据，但是生产环境有几百万的数据。 对于这个问题，可以使用WeakHashMap代表缓存，此种Map的特点是，当除了自身有对key的引用外，此key没有其他引用那么此map会自动丢弃此值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class MapTest &#123; static Map wMap = new WeakHashMap(); static Map map = new HashMap(); public static void main(String[] args) &#123; init(); testWeakHashMap(); testHashMap(); &#125; public static void init() &#123; String ref1 = new String(&quot;obejct1&quot;); String ref2 = new String(&quot;obejct2&quot;); String ref3 = new String(&quot;obejct3&quot;); String ref4 = new String(&quot;obejct4&quot;); wMap.put(ref1, &quot;cacheObject1&quot;); wMap.put(ref2, &quot;cacheObject2&quot;); map.put(ref3, &quot;cacheObject3&quot;); map.put(ref4, &quot;cacheObject4&quot;); System.out.println(&quot;String引用ref1，ref2，ref3，ref4 消失&quot;); &#125; public static void testWeakHashMap() &#123; System.out.println(&quot;WeakHashMap GC之前&quot;); for (Object o : wMap.entrySet()) &#123; System.out.println(o); &#125; try &#123; System.gc(); TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;WeakHashMap GC之后&quot;); for (Object o : wMap.entrySet()) &#123; System.out.println(o); &#125; &#125; public static void testHashMap() &#123; System.out.println(&quot;HashMap GC之前&quot;); for (Object o : map.entrySet()) &#123; System.out.println(o); &#125; try &#123; System.gc(); TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;HashMap GC之后&quot;); for (Object o : map.entrySet()) &#123; System.out.println(o); &#125; &#125;&#125; 上面代码和图示主演演示WeakHashMap如何自动释放缓存对象，当init函 数执行完成后，局部变量字 符串引用weakd1 ,weakd2,d1,d2都会消失，此时只有静态map中保存中对字符串对象的引用，可以 看到，调用gc之后，HashMap的没有被回收，而WeakHashMap 里面的缓存被回收了。 18-监听器和回调 内存泄漏第三个常见来源是监听器和其他回调，如果客户端在你实现的API中注册回调，却没有显示的取消，那么就会积聚。 需要确保回调立即被当作垃圾回收的最佳方法是只保存它的弱引用，例如将他们保存成为WeakHashMap中的键。 3）内存泄露案例分析①代码123456789101112131415161718192021222324252627public class Stack &#123; private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() &#123; elements = new Object[DEFAULT_INITIAL_CAPACITY]; &#125; public void push(Object e) &#123; //入栈 ensureCapacity(); elements[size++] = e; &#125; public Object pop() &#123; if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; return result; &#125; private void ensureCapacity() &#123; if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); &#125;&#125; ②分析上述程序并没有明显的错误，但是这段程序有一个内存泄漏，随着GC活动的增加，或者内存占用的不断增加，程序性能的降低就会表现出来，严重时可导致内存泄漏，但是这种失败情况相对较少。 代码的主要问题在pop函数，下面通过这张图示展现 假设这个栈- -直增长，增长后如下图所示 当进行大量的POP操作时，由于引用未进行置空，gc是不会释放的，如下图所示： 从上图可以看出，如果栈先增长，在收缩，那么从栈中弹出的对象将不会被当作垃圾回收，即使程序不再使用栈中的这些对象，他们也不会回收，因为栈中仍然保存这对象的引用，这个内存泄漏很隐蔽。 ③解决办法将代码中的pop()方法变成如下方法： 12345public Object pop() &#123; //出栈 if (size == 0) throw new EmptyStackException(); return elements[--size];&#125; 一旦引用过期，清空这些引用，将引用置空。 103.GC日志格式1）GC分类针对HotSpot VM的实现，它里面的GC按照回收区域又分为两大种类型:一种是部分收集（Partial GC)，一种是整堆收集（Full GC) 部分收集:不晕完整收集整个Java堆的垃圾收集。其中又分为: ·新生代收集(Minor GC / Young GC）:只是新生代（Eden\\Se,S1）的垃圾收集·老年代收集（Major Gc / old GC):只是老年代的垃圾收集。 ·目前，只有cMS GC会有单独收集老年代的行为。 ·注意，很多时候Major GC会和FullGC混淆使用，需要具体分辨是老年代回收还是整堆回收。 ·混合收集（Mixed GC):收集整个新生代以及部分老年代的垃圾收集。 ·目前,只有G1 GC会有这种行为。 整堆收集（Ful1 GC):收集整个java堆和方法区的垃圾收集。 新生代收集：当Eden区满的时候就会进行新生代收集，所以新生代收集和S0区域和S1区域无关。 老年代收集和新生代收集的关系：进行老年代收集之前会先进行一次年轻代的垃圾收集，原因如下：一个比较大的对象无法放入新生代，那它自然会往老年代去放，如果老年代也放不下，那会先进行一次新生代的垃圾收集，之后尝试往新生代放，如果还是放不下，才会进行老年代的垃圾收集，之后在往老年代去放，这是一个过程，我来说明一下为什么需要往老年代放，但是放不下，而进行新生代垃圾收集的原因，这是因为新生代垃圾收集比老年代垃圾收集更加简单，这样做可以节省性能。 进行垃圾收集的时候，堆包含新生代、老年代、元空间/永久代：可以看出Heap后面包含着新生代、老年代、元空间，但是我们设置堆空间大小的时候设置的只是新生代、老年代而已，元空间是分开设置的。 哪些情况会触发Full GC：老年代空间不足、方法区空间不足、显示调用System.gc()、Minior GC进入老年代的数据的平均大小 大于 老年代的可用内存、大对象直接进入老年代，而老年代的可用空间不足。 2）不同GC分类的GC细节123456789101112131415161718192021222324252627282930/** * -XX:+PrintCommandLineFlags * * -XX:+UseSerialGC:表明新生代使用Serial GC ，同时老年代使用Serial Old GC * * -XX:+UseParNewGC：标明新生代使用ParNew GC * * -XX:+UseParallelGC:表明新生代使用Parallel GC * -XX:+UseParallelOldGC : 表明老年代使用 Parallel Old GC * 说明：二者可以相互激活 * * -XX:+UseConcMarkSweepGC：表明老年代使用CMS GC。同时，年轻代会触发对ParNew 的使用 * @author shkstart * @create 17:19 */public class GCUseTest &#123; public static void main(String[] args) &#123; ArrayList&lt;byte[]&gt; list = new ArrayList&lt;&gt;(); while(true)&#123; byte[] arr = new byte[1024 * 10];//10kb list.add(arr);// try &#123;// Thread.sleep(5);// &#125; catch (InterruptedException e) &#123;// e.printStackTrace();// &#125; &#125; &#125;&#125; ①老年代使用CMS GCGC设置方法：参数中使用-XX:+UseConcMarkSweepGC，说明老年代使用CMS GC，同时年轻代也会触发对ParNew的使用，因此添加该参数之后，新生代使用ParNew GC，而老年代使用CMS GC，整体是并发垃圾收集，主打低延迟。 ②新生代使用Serial GCGC设置方法：参数中使用-XX:+UseSerialGC，说明新生代使用Serial GC，同时老年代也会触发对Serial Old GC的使用，因此添加该参数之后，新生代使用Serial GC，而老年代使用Serial Old GC，整体是串行垃圾收集。 3）GC日志分类①MinorGC ②FullGC 4）GC日志结构剖析①垃圾收集器使用Serial收集器在新生代的名字是Default New Generation，因此显示的是&quot;[DefNew&quot;· 使用ParNew收集器在新生代的名字会变成&quot;[ParNew&quot;,意思是&quot;Parallel New Generation&quot;· 使用parallel Scavenge收集器在新生代的名字是&quot;[PSYoungGen&quot; ,这里的3DK1.7使用的就是PSYoungGen 使用Parallel old Generation收集器在老年代的名字是&quot;[ParoldGen&quot;· 使用G1收集器的话，会显示为&quot;garbage-first heap&quot; 1Allocation Failure 表明本次引起GC的原因是因为在年轻代中没有足够的空间能够存储新的数据了。 ②GC前后情况通过图示，我们可以发现Gc日志格式的规律一般都是:GC前内存占用一&gt;GC后内存占用（该区域内存总大小) 1[PSYoungGen: 5986K-&gt;696K(8704K)]5986K-&gt;704K(9216K) 中括号内:Gc回收前年轻代堆大小，回收后大小，（年轻代堆总大小) 括号外:GC回收前年轻代和老年代大小，回收后大小，（年轻代和老年代总大小) ③GC时间Gc日志中有三个时间:user，sys和real user - 进程执行用户态代码（核心之外）所使用的时间。这是执行此进程所使用的实际CPU时间，其他进程和此进程阻塞的时间并不包括在内。在垃圾收集的情况下，表示GC线程执行所使用的CPU总时间。 sys -进程在内核态消耗的 CPU时间，即在内核执行系统调用或等待系统事件所使用的CPU时间 real -程序从开始到结束所用的时钟时间。这个时间包括其他进程使用的时间片和进程阻塞的时间（比如等待I/0 完成）。对于并行gc，这个数字应该接近（用户时间+系统时间）除以垃圾收集器使用的线程数。 由于多核的原因，一般的Gc事件中，real time是小于sys + user time的，因为一般是多个线程并发的去做GCc，所以real time是要小于sys+user time的。如果real&gt;sys+user的话，则你的应用可能存在下列问题:IO负载非常重或者是CPU不够用。 5)Minor GC 日志解析123456789101112131415161718192021222324252627282020-11-20T17:19:43.265-0800: 0.822:[GC(ALLOCATION FAILURE)[PSYOUNGGEN:76800K-&gt;8433K(89600K)] 76800K-&gt;8449K(294400K)，0.0088371 SECS][TIMES:USER=0.02svs=e.01,REAL=0.01 SECS]##注释2020-11-20T17:19:43.265-0800 添加-XX:+PrintGCDateStamps参数 日志打印时间 日期格式 如2013-05-04T21:53:59.234+0800 0.822: 添加-XX:+PrintGCTimeStamps该参数 gc发生时，Java虚拟机启动以来经过的秒数[GC(Allocation Failure) 发生了一次垃圾回收，这是一次Minior GC。它不区分新生代还是老年代GC，括号里的内容是gc发生的原因，这里的Allocation Failure的原因是新生代中没有足够区域能够存放需要分配的数据而失败。[PSYoungGen:76800K-&gt;8433K(89600K) PSYoungGen：表示GC发生的区域，区域名称与使用的GC收集器是密切相关的 Serial收集器：Default New Generation 显示Defnew ParNew收集器：ParNew Parallel Scanvenge收集器：PSYoung 老年代和新生代同理，也是和收集器名称相关 76800K-&gt;8433K(89600K)：GC前该内存区域已使用容量-&gt;GC后盖区域容量(该区域总容量) 如果是新生代，总容量则会显示整个新生代内存的9/10，即eden+from/to区 如果是老年代，总容量则是全身内存大小，无变化76800K-&gt;8449K(294400K) 虽然本次是Minor GC，只会进行新生代的垃圾收集，但是也肯定会打印堆中总容量相关信息在显示完区域容量GC的情况之后，会接着显示整个堆内存区域的GC情况：GC前堆内存已使用容量-&gt;GC后堆内存容量（堆内存总容量），并且堆内存总容量 = 9/10 新生代 + 老年代，然后堆内存总容量肯定小于初始化的内存大小。,0.0088371 整个GC所花费的时间，单位是秒[Times：user=0.02 sys=0.01,real=0.01 secs] user：指CPU工作在用户态所花费的时间 sys：指CPU工作在内核态所花费的时间 real：指在此次事件中所花费的总时间 6)Full GC 日志解析123456789101112131415161718192021222324252627282930312020-11-20T17:19:43.794-0800: 1.351:[FULL GC (METADATA GC THRESHOLD)[PsYOUNGGEN: 10082K-&gt;eK(89600K)][ PAROLDGEN: 32K-&gt;9638K(204800K)]10114K-&gt;9638K ( 29440OK),[METASPACE: 20158K-&gt;20156K(1067008K)]，0.0285388 SECS] [TIMES: USER=0.11sYS=0.00，REAL=6.03 SECS]##注释2020-11-20T17:19:43.794-0800 日志打印时间 日期格式 如2013-05-04T21:53:59.234+0800 添加-XX:+PrintGCDateStamps参数1.351 gc发生时，Java虚拟机启动以来经过的秒数 添加-XX:+PrintGCTimeStamps该参数Full GC(Metadata GCThreshold) 括号中是gc发生的原因，原因：Metaspace区不够用了。 除此之外，还有另外两种情况会引起Full GC，如下： 1、Full GC(FErgonomics) 原因：JVM自适应调整导致的GC 2、Full GC（System） 原因：调用了System.gc()方法[PSYoungGen: 100082K-&gt;0K(89600K)] PSYoungGen：表示GC发生的区域，区域名称与使用的GC收集器是密切相关的 Serial收集器：Default New Generation 显示DefNew ParNew收集器：ParNew Parallel Scanvenge收集器：PSYoungGen 老年代和新生代同理，也是和收集器名称相关 10082K-&gt;0K(89600K)：GC前该内存区域已使用容量-&gt;GC该区域容量(该区域总容量) 如果是新生代，总容量会显示整个新生代内存的9/10，即eden+from/to区 如果是老年代，总容量则是全部内存大小，无变化 ParOldGen：32K-&gt;9638K(204800K) 老年代区域没有发生GC，因此本次GC是metaspace引起的10114K-&gt;9638K(294400K),在显示完区域容量GC的情况之后，会接着显示整个堆内存区域的GC情况：GC前堆内存已使用容量-&gt;GC后堆内存容量（堆内存总容量），并且堆内存总容量 = 9/10 新生代 + 老年代，然后堆内存总容量肯定小于初始化的内存大小[Meatspace:20158K-&gt;20156K(1067008K)], metaspace GC 回收2K空间 104.垃圾回收算法 引用计数法 可达性分析法 对于可达性分析法，我们知道需要存在一个GC Root的对象作为起点，从这个节点开始向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连（就是从GC Roots到对象不可达）时，则证明此对象是不可用的。 105.GC Root有哪些？ 虚拟机栈中的引用对象 方法区中类静态属性引用的对象 方法区中常量引用对象 本地方法栈中JNI引用对象 106.垃圾回收器有哪些？ Serial收集器(-XX:+UseSerialGC -XX:+UseSerialOldGC)：Serial（串行）收集器是一个单线程收集器，新生代采用复制算法，老年代采用标记-整理算法。 Parallel Scavenge收集器(-XX:+UseParallelGC(年轻代),-XX:+UseParallelOldGC(老年代)) ：Parallel收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和Serial收集器类似。默认的收集线程数跟cpu核数相同 ParNew收集器(-XX:+UseParNewGC)：只有它能与CMS收集器（真正意义上的并发收集器，后面会介绍到）配合工作 CMS收集器(-XX:+UseConcMarkSweepGC(old))：收集器是一种以获取最短回收停顿时间为目标的收集器CMS收集器是一种 “标记-清除”算法实现的。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。 G1收集器(-XX:+UseG1GC)：G1 (Garbage-First)是一款面向服务器的垃圾收集器，主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征。G1将Java堆划分为多个大小相等的独立区域（Region），JVM最多可以有2048个Region。一般Region大小等于堆大小除以2048，比如堆大小为4096M，则Region大小为2M Shenandoah：可以看成是G1升级版 ZGC收集器：ZGC是一款JDK 11中新加入的具有实验性质的低延迟垃圾收集器。1、支持TB量级的堆；2、最大GC停顿时间不超10ms；3、奠定未来GC特性的基础；4、最糟糕的情况下吞吐量会降低15% 107.CMS运行过程，缺点？整个过程分为四个步骤 初始标记(STW)： 暂停所有的其他线程(STW)，并记录下gc roots直接能引用的对象，速度很快 并发标记： 并发标记阶段就是从GC Roots的直接关联对象开始遍历整个对象图的过程， 这个过程耗时较长但是不需要停顿用户线程，因为用户程序继续运行，可能会有导致已经标记过的对象状态发生改变。 重新标记(STW)： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短。 并发清理： 开启用户线程，同时GC线程开始对未标记的区域做清扫。 并发重置：重置本次GC过程中的标记数据。 缺点： 对CPU资源敏感（会和服务抢资源） 无法处理浮动垃圾(在并发标记和并发清理阶段又产生垃圾，这种浮动垃圾只能等到下一次gc再清理了)； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生，当然通过参数-XX:+UseCMSCompactAtFullCollection可以让jvm在执行完标记清除后再做整理 并发模式失败(最大问题)，执行过程中的不确定性，会存在上一次垃圾回收还没执行完，然后垃圾回收又被触发的情况，特别是在并发标记和并发清理阶段会出现，一边回收，系统一边运行，也许没回收完就再次触发full gc，也就是”concurrent mode failure”，此时会进入stop the world，用serial old垃圾收集器来回收 108.G1运行过程G1收集器一次GC的运作过程大致分为以下几个步骤： 初始标记（initial mark，STW）：暂停所有的其他线程，并记录下gc roots直接能引用的对象，速度很快 ； 并发标记（Concurrent Marking）：同CMS的并发标记 最终标记（Remark，STW）：同CMS的重新标记 筛选回收（Cleanup，STW）：G1采用复制算法回收几乎不会有太多内存碎片 109.G1适合什么场景 50%以上的堆被存活对象占用 对象分配和晋升的速度变化非常大 垃圾回收时间特别长，超过1秒 8GB以上的堆内存(建议值) 停顿时间是500ms以内 110.判断元空间是无用的类法区（元空间）主要回收的是无用的类，那么如何判断一个类是无用的类呢？类需要同时满足下面3个条件才能算是 “无用的类” ： 该类所有的对象实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。（条件苛刻，自定义类加载器会被回收掉） 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 111.安全点和安全区域安全点：就是指代码中一些特定的位置,当线程运行到这些位置时它的状态是确定的,这样JVM就可以安全的进行一些操作,比如GC等，所以GC不是想什么时候做就立即触发的，是需要等待所有线程运行到安全点后才能触发。这些特定的安全点位置主要有以下几种: 方法返回之前 调用某个方法之后 抛出异常的位置 循环的末尾 安全区域：如果一个线程处于 Sleep 或中断状态，它就不能响应 JVM 的中断请求，引用关系不会发生变化。在这个区域内的任意地方开始 GC 都是安全的。 112.YGC和FGC发生的场景YGC：对新生代堆进行gc。频率比较高，因为大部分对象的存活寿命较短，在新生代里被回收。性能耗费较小。edn空间不足,执行 FGC：全堆范围的gc。默认堆空间使用到达80%(可调整)的时候会触发fgc。生产环境，一般比较少会触发fgc，有时10天或一周左右会有一次。 老年代空间不足，永久区空间不足，调用方法System.gc() ，ygc时的悲观策略, dump live的内存信息时(jmap –dump:live)，都会执行full gc 113.jstack，jmap，Jstat作用jmap：可以用来查看内存信息，实例个数以及占用内存大小 jmap -heap 进程号：查看堆内存信息 jmap ‐dump:format=b,file=eureka.hprof 进程号： 堆内存的快照信息，添加jvm参数也可以设置内存溢出自动导出dump文件 jstack: 可以获得java线程的运行情况，可以查看死锁，阻塞，等待 Jstack -l PID &gt;&gt; 123.txt 打印某个java进程的堆栈信息 Jstat：jstat命令可以查看堆内存各部分的使用量，以及加载类的数量 jstat -gc pid 最常用，可以评估程序内存使用及GC压力整体情况","categories":[{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"数据结构和算法","slug":"2.算法/数据结构和算法","date":"2022-03-30T16:00:00.000Z","updated":"2024-08-22T03:23:14.706Z","comments":true,"path":"2022/03/31/2.算法/数据结构和算法/","link":"","permalink":"https://zhangxin66666.github.io/2022/03/31/2.%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/","excerpt":"","text":"排序算法高效交换算法(异或^)概念 0^N = N ; N^N = 0 相同为0，不同为1，也可以叫做无进位相加，这么做的前提：需要交换的两个数指向的内存是两位位置 异或运算满足交换律和结合律 不用额外变量交换两个数 123456// 交换arr的i和j位置上的值public static void swap(int[] arr, int i, int j) &#123; arr[i] = arr[i] ^ arr[j]; arr[j] = arr[i] ^ arr[j]; arr[i] = arr[i] ^ arr[j];&#125; 一种数出现奇数次一个数组中有一个数出现奇数次，其他数都出现偶数次，怎么找到这一个数？ 1234567public static void printOddTimesNum1(int[] arr) &#123; int eor = 0; for (int i = 0; i &lt; arr.length; i++) &#123; eor ^= arr[i]; &#125; System.out.println(eor);&#125; 两种数出现奇数次一个数组中有两个数出现奇数次，其他数都出现了偶数次，怎么找到这两个数？ 123456789101112131415161718192021public static void printOddTimesNum2(int[] arr) &#123; int eor = 0; for (int i = 0; i &lt; arr.length; i++) &#123; eor ^= arr[i]; &#125; // a 和 b是两种数 // eor != 0 // eor最右侧的1，提取出来 // eor : 00110010110111000 // rightOne :00000000000001000 int rightOne = eor &amp; (-eor); // 提取出最右的1 int onlyOne = 0; // eor&#x27; for (int i = 0 ; i &lt; arr.length;i++) &#123; // arr[1] = 111100011110000 // rightOne= 000000000010000 if ((arr[i] &amp; rightOne) != 0) &#123; onlyOne ^= arr[i]; &#125; &#125; System.out.println(onlyOne + &quot; &quot; + (eor ^ onlyOne));&#125; 冒泡排序 基本冒泡排序算法 123456789101112131415public static void bubbleSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; // 0 ~ N-1 // 0 ~ N-2 // 0 ~ N-3 for (int e = arr.length - 1; e &gt; 0; e--) &#123; // 0 ~ e for (int i = 0; i &lt; e; i++) &#123; if (arr[i] &gt; arr[i + 1]) &#123; swap(arr, i, i + 1); &#125; &#125; &#125; &#125; 冒泡排序算法优化 在一趟排序过程中如果一次都没有交换过，那说明后续的数都是有序的，不需要在进行后续的排序了，如果元素本来就是有序的，就只比较一次就结束了。 12345678910111213141516171819202122232425public static void bubbleSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; // 标识变量，表示是否进行过交换 boolean flag = false; for (int i = 0; i &lt; arr.length - 1; i++) &#123; for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123; // 如果前面的数比后面的数大，则交换 if (arr[j] &gt; arr[j + 1]) &#123; flag = true; int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; // 在一趟排序中，一次交换都没有发生过 if (!flag) &#123; break; &#125; else &#123; // 重置flag!!!, 进行下次判断 flag = false; &#125; &#125;&#125; 选择排序 0 ~ N-1 找到最小值，在哪，放到0位置上 1 ~ n-1 找到最小值，在哪，放到1 位置上 2 ~ n-1 找到最小值，在哪，放到2 位置上 123456789101112public static void selectionSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; for (int i = 0; i &lt; arr.length - 1; i++) &#123; int minIndex = i; for (int j = i + 1; j &lt; arr.length; j++) &#123; // i ~ N-1 上找最小值的下标 minIndex = arr[j] &lt; arr[minIndex] ? j : minIndex; &#125; swap(arr, i, minIndex); &#125;&#125; 插入排序 0~1单范围有序 0~2范围有序 0~3范围有序 0~N范围有序 1234567891011public static void insertionSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; // 不只1个数 for (int i = 1; i &lt; arr.length; i++) &#123; // 0 ~ i 做到有序 for (int j = i - 1; j &gt;= 0 &amp;&amp; arr[j] &gt; arr[j + 1]; j--) &#123; swap(arr, j, j + 1); &#125; &#125;&#125; 查找算法二分查找二分法的详解与扩展 在一个有序数组中，查找某个数是否存在 在一个有序数组中，找&gt;=某个数最左侧的位置 局部最小值问题 1234567891011121314151617181920public static boolean exist(int[] sortedArr, int num) &#123; if (sortedArr == null || sortedArr.length == 0) &#123; return false; &#125; int L = 0; int R = sortedArr.length - 1; int mid = 0; // L..R while (L &lt; R) &#123; // L..R 至少两个数的时候 mid = L + ((R - L) &gt;&gt; 1); if (sortedArr[mid] == num) &#123; return true; &#125; else if (sortedArr[mid] &gt; num) &#123; R = mid - 1; &#125; else &#123; L = mid + 1; &#125; &#125; return sortedArr[L] == num;&#125; 数组范围上求最大值递归解法 123456789101112public static int process(int[] arr, int L, int R) &#123; // arr[L..R]范围上只有一个数，直接返回，base case if (L == R) &#123; return arr[L]; &#125; // L...R 不只一个数 // mid = (L + R) / 2 int mid = L + ((R - L) &gt;&gt; 1); // 中点 int leftMax = process(arr, L, mid); int rightMax = process(arr, mid + 1, R); return Math.max(leftMax, rightMax);&#125; 对数器的概念 有一个你想要测试的方法a 实现复杂度不好但是容易实现的方法b 实现一个随机样本产生器 把方法a和方法b跑相同的随机样本，看看得到的结果是否一样 如果有一个随机样本时的比对结果不一致，打印样本进行人工干预，改对方法a或者方法b 当样本数量很多时比对测试依然正确，可以确定方法a已经正确 数组ab数组合并到a 题目：给出两个有序的整数数组A和B，请将数组B合并到数组A中，变成一个有序的数组。注意：可以假设A数组有足够的空间存放B数组的元素，A和B中初始的元素数目分别为m和n。 题解：最优解：从后往前处理,不需要开辟额外空间。从后往前，这样不需要进行冗余处理。 12345678910111213141516171819202122/** * @param a 数组a，有足够的空间合并数组b * @param m 数组a里面的元素个数 * @param b 数组b * @param n 数组b里面的元素个数 */public static void merge(int[] a, int m, int[] b, int n) &#123; int i = m - 1; int j = n - 1; int index = m + n - 1; while (i &gt;= 0 &amp;&amp; j &gt;= 0) &#123; if(a[i] &gt; b[j] )&#123; a[index--] = a[i--]; &#125; else &#123; a[index--] = b[j--]; &#125; &#125; //如果A的数字比B多，则不会进入后续处理；如果B的数字比A多，则进入后续处理，将B剩余数字添加到数组A中。 while (j &gt;= 0) &#123; a[index--] = b[j--]; &#125;&#125; ab数组合并到c 题目： 合并两个有序整型数据（入参两个需要合并的数组，返回值合并好的新数组） 123456789101112131415161718192021222324252627private static int[] margeArr(int[] a, int[] b) &#123; int[] c = new int[a.length + b.length]; int i = 0, j = 0; int index = 0; //比较指针i,j指向的值，小的值存入指针index指向的结果数组中，当有一个指针（i或j）先到达数组末尾时，比较结束； while (i &lt; a.length &amp;&amp; j &lt; b.length) &#123; if (a[i] &lt; b[j]) &#123; c[index++] = a[i++]; &#125; else &#123; c[index++] = b[j++]; &#125; &#125; int l; //将指针（i或j）没有到达数组末尾的数组复制到指针index指向的结果数组中 if (i &lt; a.length) &#123; for (l = i; l &lt; a.length; l++) &#123; c[index++] = a[l]; &#125; &#125; //将指针（i或j）没有到达数组末尾的数组复制到指针index指向的结果数组中 if (j &lt; b.length) &#123; for (l = j; l &lt; b.length; l++) &#123; c[index++] = b[l]; &#125; &#125; return c;&#125; 查找两数之和等于目标数 题目：给定一个数组和一个目标数，从数组中找到两个数，是这两个数之和等于目标数。返回其在数组中的编号 12345678910public static int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; if (map.containsKey(target - nums[i])) &#123; return new int[]&#123;map.get(target - nums[i]), i&#125;; &#125; map.put(nums[i], i); &#125; return null;&#125; 螺旋打印二维数组1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public static void printEdge(int[][] m, int tR, int tC, int dR, int dC) &#123; if (tR == dR) &#123; for (int i = tC; i &lt;= dC; i++) &#123; System.out.print(m[tR][i] + &quot; &quot;); &#125; &#125; else if (tC == dC) &#123; for (int i = tR; i &lt;= dR; i++) &#123; System.out.print(m[i][tC] + &quot; &quot;); &#125; &#125; else &#123; int curC = tC; int curR = tR; while (curC != dC) &#123; System.out.print(m[tR][curC] + &quot; &quot;); curC++; &#125; while (curR != dR) &#123; System.out.print(m[curR][dC] + &quot; &quot;); curR++; &#125; while (curC != tC) &#123; System.out.print(m[dR][curC] + &quot; &quot;); curC--; &#125; while (curR != tR) &#123; System.out.print(m[curR][tC] + &quot; &quot;); curR--; &#125; &#125;&#125;public static void spiralOrderPrint(int[][] matrix) &#123; int tR = 0; int tC = 0; int dR = matrix.length - 1; int dC = matrix[0].length - 1; while (tR &lt;= dR &amp;&amp; tC &lt;= dC) &#123; printEdge(matrix, tR++, tC++, dR--, dC--); &#125;&#125;public static void main(String[] args) &#123; int[][] matrix = &#123; &#123; 1, 2, 3, 4 &#125;, &#123; 5, 6, 7, 8 &#125;, &#123; 9, 10, 11, 12 &#125;, &#123; 13, 14, 15, 16 &#125; &#125;; spiralOrderPrint(matrix);&#125; 链表 对于笔试，不用太在乎空间复杂度，一切为了时间复杂度 对于面试，时间复杂度依然放在第一位，但是一定要找到空间最省的方法 重要技巧 额外数据结构记录（哈希表等） 快慢指针 链表反转单向链表反转123456789// 单向链表节点public static class Node &#123; public int value; public Node next; public Node(int data) &#123; value = data; &#125;&#125; 1234567891011121314// head 单向链表反转算法// a -&gt; b -&gt; c -&gt; null// c -&gt; b -&gt; a -&gt; nullpublic static Node reverseLinkedList(Node head) &#123; Node pre = null; Node next = null; while (head != null) &#123; next = head.next; head.next = pre; pre = head; head = next; &#125; return pre;&#125; 123456789101112/** * 递归反转方式 */private Node reverseLinkedList(Node head) &#123; if (head == null || head.next == null) &#123; return head; &#125; Node newHead = reverseLinkedList(head.next); head.next.next = head; head.next = null; return newHead;&#125; 双向链表反转12345678910// 双向链表节点public static class DoubleNode &#123; public int value; public DoubleNode last; public DoubleNode next; public DoubleNode(int data) &#123; value = data; &#125;&#125; 12345678910111213// 双向链表反转算法public static DoubleNode reverseDoubleList(DoubleNode head) &#123; DoubleNode pre = null; DoubleNode next = null; while (head != null) &#123; next = head.next; head.next = pre; head.last = next; pre = head; head = next; &#125; return pre;&#125; 查找链表中点快慢指针应用 查找链表中点或中点上一个1234567891011121314// head 头 解：查找链表中点或者中点前一个节点public static Node midOrUpMidNode(Node head) &#123; if (head == null || head.next == null || head.next.next == null) &#123; return head; &#125; // 链表有3个点或以上 Node slow = head.next; Node fast = head.next.next; while (fast.next != null &amp;&amp; fast.next.next != null) &#123; slow = slow.next; fast = fast.next.next; &#125; return slow;&#125; 查找链表中点或中点下一个12345678910111213// 查找链表中点或者中点下一个节点public static Node midOrDownMidNode(Node head) &#123; if (head == null || head.next == null) &#123; return head; &#125; Node slow = head.next; Node fast = head.next; while (fast.next != null &amp;&amp; fast.next.next != null) &#123; slow = slow.next; fast = fast.next.next; &#125; return slow;&#125; 判断一个链表是否是回文结构【题目】给定一个单向链表的头节点head，请判断该链表是否为回文结构。【例子】1-&gt;2-&gt;1,返回true;1-&gt;2-&gt;2-&gt;1,返回true；15-&gt;6-&gt;15，返回true；1-&gt;2-&gt;3，返回false。如果链表长度为N，时间复杂度达到O(N)，额外空间复杂度达到O(1)。 解题思路1： 1：遍历链表，把每个元素放到栈里面；2：从栈里面依次弹出元素和原来链表挨个元素比对。 解题思路2： 1：通过快慢指针找到中点和结束点，只把右侧部分放到栈里面去；2：从栈里面依次弹出元素和原来链表挨个元素比对。 12345678910111213141516// 1：遍历链表，把每个元素放到栈里面；2：从栈里面依次弹出元素和原来链表挨个元素比对public static boolean isPalindrome1(Node head) &#123; Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;(); Node cur = head; while (cur != null) &#123; stack.push(cur); cur = cur.next; &#125; while (head != null) &#123; if (head.value != stack.pop().value) &#123; return false; &#125; head = head.next; &#125; return true;&#125; 123456789101112131415161718192021222324// 1：通过快慢指针找到中点和结束点，只把右侧部分放到栈里面去；2：从栈里面依次弹出元素和原来链表挨个元素比对public static boolean isPalindrome2(Node head) &#123; if (head == null || head.next == null) &#123; return true; &#125; Node right = head.next; Node cur = head; while (cur.next != null &amp;&amp; cur.next.next != null) &#123; right = right.next; cur = cur.next.next; &#125; Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;(); while (right != null) &#123; stack.push(right); right = right.next; &#125; while (!stack.isEmpty()) &#123; if (head.value != stack.pop().value) &#123; return false; &#125; head = head.next; &#125; return true;&#125; 链表排序单向链表分左中右【题目】 将单向链表按照某值划分成左边小，中间相等，右边大的形式：给定一个单链表头节点head，节点的值类型是整型，再给定一个整数pivot。实现一个调整链表的函数，将链表调整为做部分都是值小于pivot的节点，中间部分都是值等于piovt的节点，有部分都是值大于piovt的节点。【进阶】在实现原问题功能的基础上增加要求：小于，等于，大于pivot节点之间顺序和之前一样，时间复杂度O(N)，额外空间复杂度O(1)。 解题思路1： 将链表放入数组，排序，再转成链表 解题思路2： 左中右分别准备两个指针(共6个变量)，然后遍历原链表，排序之后分别插入相应位置(考虑边界问题) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 将链表放入数组，排序，再转成链表public static Node listPartition1(Node head, int pivot) &#123; if (head == null) &#123; return head; &#125; Node cur = head; int i = 0; while (cur != null) &#123; i++; cur = cur.next; &#125; Node[] nodeArr = new Node[i]; i = 0; cur = head; for (i = 0; i != nodeArr.length; i++) &#123; nodeArr[i] = cur; cur = cur.next; &#125; arrPartition(nodeArr, pivot); for (i = 1; i != nodeArr.length; i++) &#123; nodeArr[i - 1].next = nodeArr[i]; &#125; nodeArr[i - 1].next = null; return nodeArr[0];&#125;public static void arrPartition(Node[] nodeArr, int pivot) &#123; int small = -1; int big = nodeArr.length; int index = 0; while (index != big) &#123; if (nodeArr[index].value &lt; pivot) &#123; swap(nodeArr, ++small, index++); &#125; else if (nodeArr[index].value == pivot) &#123; index++; &#125; else &#123; swap(nodeArr, --big, index); &#125; &#125;&#125;public static void swap(Node[] nodeArr, int a, int b) &#123; Node tmp = nodeArr[a]; nodeArr[a] = nodeArr[b]; nodeArr[b] = tmp;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// 左中右分别准备两个指针(共6个变量)，然后遍历原链表，排序之后分别插入相应位置(考虑边界问题)public static Node listPartition2(Node head, int pivot) &#123; Node sH = null; // small head Node sT = null; // small tail Node eH = null; // equal head Node eT = null; // equal tail Node mH = null; // big head Node mT = null; // big tail Node next = null; // save next node // every node distributed to three lists while (head != null) &#123; next = head.next; head.next = null; if (head.value &lt; pivot) &#123; if (sH == null) &#123; sH = head; sT = head; &#125; else &#123; sT.next = head; sT = head; &#125; &#125; else if (head.value == pivot) &#123; if (eH == null) &#123; eH = head; eT = head; &#125; else &#123; eT.next = head; eT = head; &#125; &#125; else &#123; if (mH == null) &#123; mH = head; mT = head; &#125; else &#123; mT.next = head; mT = head; &#125; &#125; head = next; &#125; // 小于区域的尾巴，连等于区域的头，等于区域的尾巴连大于区域的头 if (sT != null) &#123; // 如果有小于区域 sT.next = eH; eT = eT == null ? sT : eT; // 下一步，谁去连大于区域的头，谁就变成eT &#125; // 下一步，一定是需要用eT 去接 大于区域的头 // 有等于区域，eT -&gt; 等于区域的尾结点 // 无等于区域，eT -&gt; 小于区域的尾结点 // eT 尽量不为空的尾巴节点 if (eT != null) &#123; // 如果小于区域和等于区域，不是都没有 eT.next = mH; &#125; return sH != null ? sH : (eH != null ? eH : mH);&#125; 链表闭环及相交问题单链表查找闭环位置注：单链表闭环只能有一个环，如果产生环，必然会出现闭环 解法1：额外空间解决 申请一个set集合，从头节点遍历链表，每遍历一个元素就查询该节点是否在集合中，如果没有就把该节点放进去，如果有，该节点就是环位置 1.... 解法2：有限几个变量解决 快慢指针从单链表头节点开始走，直至两个节点相遇，说明有环，最后指向null，说明无环 相遇之后快指针回到头节点，之后一次走一步，慢指针停在原地，再次相遇的位置即是环节点位置(记住结论，不要问为什么) 123456789101112131415161718192021222324252627282930public static class Node &#123; public int value; public Node next; public Node(int data) &#123; this.value = data; &#125;&#125;// 找到链表第一个入环节点，如果无环，返回nullpublic static Node getLoopNode(Node head) &#123; if (head == null || head.next == null || head.next.next == null) &#123; return null; &#125; // n1 慢 n2 快 Node slow = head.next; // n1 -&gt; slow Node fast = head.next.next; // n2 -&gt; fast while (slow != fast) &#123; if (fast.next == null || fast.next.next == null) &#123; return null; &#125; fast = fast.next.next; slow = slow.next; &#125; // slow fast 相遇 fast = head; // n2 -&gt; walk again from head while (slow != fast) &#123; slow = slow.next; fast = fast.next; &#125; return slow;&#125; 两个无环链表交点位置注： 如果两个单向链表相交，相交后面的部分必然是共有的，那么两个链表最后的那个节点必然是同一个节点 长链表先走两个链表差值的步数，然后短链表在开始走，他俩一定会在第一个相交的位置相遇 123456789101112131415161718192021222324252627282930313233// 如果两个链表都无环，返回第一个相交节点，如果不想交，返回nullpublic static Node noLoop(Node head1, Node head2) &#123; if (head1 == null || head2 == null) &#123; return null; &#125; Node cur1 = head1; Node cur2 = head2; int n = 0; while (cur1.next != null) &#123; n++; cur1 = cur1.next; &#125; while (cur2.next != null) &#123; n--; cur2 = cur2.next; &#125; if (cur1 != cur2) &#123; return null; &#125; // n : 链表1长度减去链表2长度的值 cur1 = n &gt; 0 ? head1 : head2; // 谁长，谁的头变成cur1 cur2 = cur1 == head1 ? head2 : head1; // 谁短，谁的头变成cur2 n = Math.abs(n); while (n != 0) &#123; n--; cur1 = cur1.next; &#125; while (cur1 != cur2) &#123; cur1 = cur1.next; cur2 = cur2.next; &#125; return cur1;&#125; 两条有环链表查找交点分为两种情况：入环节点可能是同一个节点，也可能不是同一个节点，如图 情况1：入环节点可能是同一个节点，就是求单链表的第一个环节点问题，只不过从相交位置开始走 情况2：如果链表1在转回到自己的过程中没有遇到链表2，就说明是各自成环的，相交节点返回空就醒来，如果遇到，就是情况2，返回两个节点，都对，都属于第一个相交的节点 123456789101112131415161718192021222324252627282930313233343536373839// 两个有环链表，返回第一个相交节点，如果不想交返回nullpublic static Node bothLoop(Node head1, Node loop1, Node head2, Node loop2) &#123; Node cur1 = null; Node cur2 = null; if (loop1 == loop2) &#123; cur1 = head1; cur2 = head2; int n = 0; while (cur1 != loop1) &#123; n++; cur1 = cur1.next; &#125; while (cur2 != loop2) &#123; n--; cur2 = cur2.next; &#125; cur1 = n &gt; 0 ? head1 : head2; cur2 = cur1 == head1 ? head2 : head1; n = Math.abs(n); while (n != 0) &#123; n--; cur1 = cur1.next; &#125; while (cur1 != cur2) &#123; cur1 = cur1.next; cur2 = cur2.next; &#125; return cur1; &#125; else &#123; cur1 = loop1.next; while (cur1 != loop1) &#123; if (cur1 == loop2) &#123; return loop1; &#125; cur1 = cur1.next; &#125; return null; &#125;&#125; 查找两个链表相交节点注：难点为考虑是否有环，有环链表相交以及无环链表相交问题，没有用到额外数据结构，只用到有限几个变量 1234567891011121314151617public static Node getIntersectNode(Node head1, Node head2) &#123; if (head1 == null || head2 == null) &#123; return null; &#125; // 分别找到两个链表的环位置 Node loop1 = getLoopNode(head1); Node loop2 = getLoopNode(head2); if (loop1 == null &amp;&amp; loop2 == null) &#123; // 无环链表相交问题 return noLoop(head1, head2); &#125; if (loop1 != null &amp;&amp; loop2 != null) &#123; // 两条有环链表相交问题 return bothLoop(head1, loop1, head2, loop2); &#125; return null;&#125; 二叉树数据结构 12345678public static class Node &#123; public int value; public Node left; public Node right; public Node(int v) &#123; value = v; &#125;&#125; 二叉树遍历 先序遍历：头-&gt;左-&gt;右 中序遍历：左-&gt;头-&gt;右 后序遍历：左-&gt;右-&gt;头 递归遍历二叉树遍历说明 递归通过打印时机不同，实现先，中，后序遍历 12345678910public static void f(Node head) &#123; if (head == null) &#123; return; &#125; // 1 前序 f(head.left); // 2 中序 f(head.right); // 3 后序&#125; 非递归遍历二叉树 先序遍历(深度性遍历) 准备一个栈，根节点入栈弹出，打印，然后先压右，再压左 弹出打印，先压右再压左，周而复始 123456789101112131415161718public static void pre(Node head) &#123; System.out.print(&quot;pre-order: &quot;); if (head != null) &#123; Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;(); stack.add(head); while (!stack.isEmpty()) &#123; head = stack.pop(); System.out.print(head.value + &quot; &quot;); if (head.right != null) &#123; stack.push(head.right); &#125; if (head.left != null) &#123; stack.push(head.left); &#125; &#125; &#125; System.out.println();&#125; 后序遍历 在先序遍历的基础之上，增加一个收集栈，弹出来就放到收集栈中(不打印)，然后先压左，再压右 把收集栈中的元素依次出栈，打印 1234567891011121314151617181920212223public static void pos1(Node head) &#123; System.out.print(&quot;pos-order: &quot;); if (head != null) &#123; Stack&lt;Node&gt; s1 = new Stack&lt;Node&gt;(); Stack&lt;Node&gt; s2 = new Stack&lt;Node&gt;(); s1.push(head); while (!s1.isEmpty()) &#123; head = s1.pop(); // 头 右 左 s2.push(head); if (head.left != null) &#123; s1.push(head.left); &#125; if (head.right != null) &#123; s1.push(head.right); &#125; &#125; // 左 右 头 while (!s2.isEmpty()) &#123; System.out.print(s2.pop().value + &quot; &quot;); &#125; &#125; System.out.println();&#125; 中序遍历 整棵树左边界进栈，依次弹出的过程中，打印，对弹出节点的右树周而复始 为什么？ 因为整个树都会被他的左边界分解掉，我们把头和左边界压栈，然后再右，出栈的时候就是左，头，右 1234567891011121314151617public static void in(Node cur) &#123; System.out.print(&quot;in-order: &quot;); if (cur != null) &#123; Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;(); while (!stack.isEmpty() || cur != null) &#123; if (cur != null) &#123; stack.push(cur); cur = cur.left; &#125; else &#123; cur = stack.pop(); System.out.print(cur.value + &quot; &quot;); cur = cur.right; &#125; &#125; &#125; System.out.println();&#125; 宽度遍历宽度遍历就是横着遍历，也是层次遍历 用队列，头节点放队列，每一次弹出就打印，然后先放左再放右，每一个元素出队列都是先放左再放右 123456789101112131415161718// 从node出发，进行宽度优先遍历public static void width(Node head) &#123; if (head == null) &#123; return; &#125; Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;(); queue.add(head); while (!queue.isEmpty()) &#123; Node cur = queue.poll(); System.out.println(cur.value); if (cur.left != null) &#123; queue.add(cur.left); &#125; if (cur.right != null) &#123; queue.add(cur.right); &#125; &#125;&#125; 求二叉树的最大宽度(难)分析：宽度性遍历的时候要知道每一层的节点个数 解：遍历每个节点的时候，知道他在第几层 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// 用hash表的解法public static int maxWidthUseMap(Node head) &#123; if (head == null) &#123; return 0; &#125; Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;(); queue.add(head); // key 在 哪一层，value HashMap&lt;Node, Integer&gt; levelMap = new HashMap&lt;&gt;(); levelMap.put(head, 1); int curLevel = 1; // 当前你正在统计哪一层的宽度 int curLevelNodes = 0; // 当前层curLevel层，宽度目前是多少 int max = 0; while (!queue.isEmpty()) &#123; Node cur = queue.poll(); int curNodeLevel = levelMap.get(cur); if (cur.left != null) &#123; levelMap.put(cur.left, curNodeLevel + 1); queue.add(cur.left); &#125; if (cur.right != null) &#123; levelMap.put(cur.right, curNodeLevel + 1); queue.add(cur.right); &#125; if (curNodeLevel == curLevel) &#123; curLevelNodes++; &#125; else &#123; max = Math.max(max, curLevelNodes); curLevel++; curLevelNodes = 1; &#125; &#125; max = Math.max(max, curLevelNodes); return max;&#125;// 不用hash表的方法 (难度高)public static int maxWidthNoMap(Node head) &#123; if (head == null) &#123; return 0; &#125; Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;(); queue.add(head); Node curEnd = head; // 当前层，最右节点是谁 Node nextEnd = null; // 下一层，最右节点是谁 int max = 0; int curLevelNodes = 0; // 当前层的节点数 while (!queue.isEmpty()) &#123; Node cur = queue.poll(); if (cur.left != null) &#123; queue.add(cur.left); nextEnd = cur.left; &#125; if (cur.right != null) &#123; queue.add(cur.right); nextEnd = cur.right; &#125; curLevelNodes++; if (cur == curEnd) &#123; max = Math.max(max, curLevelNodes); curLevelNodes = 0; curEnd = nextEnd; &#125; &#125; return max;&#125; 折纸问题 结果可看做头节点是凹，所有的左子树头节点都是凹，所有右子树头肩点都是凸的二叉树 中序遍历即可打印出从上到下的所有结构 12345678910111213141516public static void main(String[] args) &#123; int N = 4; process(1, N, true);&#125;// 这个节点在第i层，一共有N层，N固定不变的// 这个节点如果是凹的话，down = T// 这个节点如果是凸的话，down = F// 函数的功能：中序打印以你想象的节点为头的整棵树！public static void process(int i, int N, boolean down) &#123; if (i &gt; N) &#123; return; &#125; process(i + 1, N, true); System.out.print(down ? &quot;凹 &quot; : &quot;凸 &quot;); process(i + 1, N, false);&#125; 搜索二叉树(递归套路) 题目：如何判断一颗二叉树是搜索二叉树？ 搜索二叉树的特点：任何一个节点，左子树的节点一定比它小，右子树的节点一定比它大 解题： 中序遍历一定是升序，如果某个位置有降序，一定不是搜索二叉树 递归套路题解 :向我左树要信息，右树要信息，左树必须是搜索二叉树且左树最大值小于我，右树是搜索二叉树并且最小值大于我；左树信息：1.是否是搜索二叉树，2.最大值，3.最小值；右树也是 注意：递归套路，可以解决一切树形DP 问题，无非是可能性的罗列有难度 12345678910111213141516171819202122232425262728293031323334// 解法1： 额外引入数组// 节点public static class Node &#123; public int value; public Node left; public Node right; public Node(int data) &#123; this.value = data; &#125;&#125;// 中序遍历到一个数组中，然后判断数组是不是升序public static boolean isBST1(Node head) &#123; if (head == null) &#123; return true; &#125; ArrayList&lt;Node&gt; arr = new ArrayList&lt;&gt;(); in(head, arr); for (int i = 1; i &lt; arr.size(); i++) &#123; if (arr.get(i).value &lt;= arr.get(i - 1).value) &#123; return false; &#125; &#125; return true;&#125;// 中序遍历public static void in(Node head, ArrayList&lt;Node&gt; arr) &#123; if (head == null) &#123; return; &#125; in(head.left, arr); arr.add(head); in(head.right, arr);&#125; 12345678910111213141516171819// 解法2：递归方式public static int preValue = Integer.MIN_VALUE;public static boolean checkBST(Node head) &#123; if (head == null) &#123; return true; &#125; boolean checkLeft = checkBST(head.left); if (!checkLeft) &#123; return false; &#125; // 中序打印的时机，换成了中序比较的时机 if (head.value &lt;= preValue) &#123; return false; &#125; else &#123; preValue = head.value; &#125; return checkBST(head.right);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 解法3：递归套路/** * 递归返回值 */public static class Info &#123; public boolean isBST; // 是否是搜索二叉树 public int max; // 最大值 public int min; // 最小值 public Info(boolean i, int ma, int mi) &#123; isBST = i; max = ma; min = mi; &#125;&#125;public static Info process(Node x) &#123; if (x == null) &#123; return null; &#125; Info leftInfo = process(x.left); Info rightInfo = process(x.right); int min = x.value; int max = x.value; // 最小值和最大值就是我当前节点的值和它比较得出的最小值和最大值 if (leftInfo != null) &#123; min = Math.min(min, leftInfo.min); max = Math.max(max, leftInfo.max); &#125; if (rightInfo != null) &#123; min = Math.min(min, rightInfo.min); max = Math.max(max, rightInfo.max); &#125; // 是否是搜索二叉树 boolean isBST = true; // 左边有信息并且左边不是搜索二叉树 if (leftInfo != null &amp;&amp; !leftInfo.isBST) &#123; isBST = false; &#125; if (rightInfo != null &amp;&amp; !rightInfo.isBST) &#123; isBST = false; &#125; // 左边有信息，但是左边的最大值大于等于我的值 if (leftInfo != null &amp;&amp; leftInfo.max &gt;= x.value) &#123; isBST = false; &#125; // 右边有信息，右边的最小值小于等于我当前值 if (rightInfo != null &amp;&amp; rightInfo.min &lt;= x.value) &#123; isBST = false; &#125; return new Info(isBST, max, min);&#125; 完全二叉树 如何判断一个二叉树是完全二叉树 完全二叉树：一颗二叉树从左到右是依次变满的，即使不满，也是变满的样子 解题：二叉树宽度遍历，1.任何一个节点如果有有节点，没左节点，false；2.在第一个条件不违规情况，如果遇到第一个左右两个节点不双全情况，接下来遇到的所有节点，必须是叶子节点 1234567891011121314151617181920212223242526272829303132public static boolean isCBT(Node head) &#123; if (head == null) &#123; return true; &#125; LinkedList&lt;Node&gt; queue = new LinkedList&lt;&gt;(); // 是否遇到过左右两个孩子不双全的节点 boolean leaf = false; Node l = null; Node r = null; queue.add(head); while (!queue.isEmpty()) &#123; head = queue.poll(); l = head.left; r = head.right; // 如果遇到了不双全的节点之后，又发现当前节点不是叶节点 if ((leaf &amp;&amp; (l != null || r != null)) || (l == null &amp;&amp; r != null)) &#123; return false; &#125; if (l != null) &#123; queue.add(l); &#125; if (r != null) &#123; queue.add(r); &#125; // 遇到左右两个节点不双全的情况，修改标记 if (l == null || r == null) &#123; leaf = true; &#125; &#125; return true;&#125; 平衡二叉树 如何判断一棵树是平衡二叉树 平衡二叉树特性：对于任何一个子树来说，左树的高度和右树的高度差，都不超过1 解决思路：假设我可以向我的左树要信息，可以向右树要信息，如果我整棵树是平衡二叉树，我左树得是平的，右树得是平的，对于X节点来说，左树-右树高度差&lt;=1；我向左树要信息：1.是否是平的；2.高度是多少，右树要信息：1.是否是平的；2.高度是多少 12345678910111213141516171819202122232425262728293031323334/** * 返回值信息 */public static class Info &#123; public boolean isBalanced; public int height; public Info(boolean i, int h) &#123; isBalanced = i; height = h; &#125;&#125;public static Info process(Node x) &#123; if (x == null) &#123; return new Info(true, 0); &#125; Info leftInfo = process(x.left); Info rightInfo = process(x.right); // x为头的节点的高度：左树和右树较大的那个高度再加上我自己(+1) int height = Math.max(leftInfo.height, rightInfo.height) + 1; // 是否是平衡树：我左树得是平衡，右树得是平衡树，并且我左树和右树的高度差的绝对值得小于2 boolean isBalanced = true; if (!leftInfo.isBalanced) &#123; isBalanced = false; &#125; if (!rightInfo.isBalanced) &#123; isBalanced = false; &#125; if (Math.abs(leftInfo.height - rightInfo.height) &gt; 1) &#123; isBalanced = false; &#125; return new Info(isBalanced, height);&#125; 满二叉树 如何判断一棵树是满二叉树 树最大深度L，节点个数N，满足N=2(L次方)-1 解法(递归套路)：先求二叉树最大深度L，再求节点个数N，满足N=2(L次方)-1 12345678910111213141516171819202122232425262728293031323334// 满二叉树解法：递归套路/** * 返回值信息 */public static class Info &#123; public int height; public int nodes; public Info(int h, int n) &#123; height = h; nodes = n; &#125;&#125;public static boolean isFull(Node head) &#123; if (head == null) &#123; return true; &#125; Info all = process(head); // N=2(L次方)-1 return (1 &lt;&lt; all.height) - 1 == all.nodes;&#125;public static Info process(Node head) &#123; if (head == null) &#123; return new Info(0, 0); &#125; Info leftInfo = process(head.left); Info rightInfo = process(head.right); // 高度等于左树和右树最高的高度+1 int height = Math.max(leftInfo.height, rightInfo.height) + 1; // 总个数等于左边的个数加上右边的个数加1 int nodes = leftInfo.nodes + rightInfo.nodes + 1; return new Info(height, nodes);&#125; 二叉树最低公共祖先 给定两个二叉树节点node1和node2，找到他们的最低公共祖先节点 解法1：遍历整棵树，把所有节点的父节点都维护到一个Map中，然后找到node1的所有父节点维护到set中，再遍历node2的所有的父，第一个在set中遇到的节点就是最低公共祖先 1234567891011121314151617181920212223242526272829303132333435// 解法1public static Node lca(Node head, Node o1, Node o2) &#123; if (head == null) &#123; return null; &#125; HashMap&lt;Node, Node&gt; parentMap = new HashMap&lt;&gt;(); parentMap.put(head, null); fillParentMap(head, parentMap); HashSet&lt;Node&gt; set = new HashSet&lt;&gt;(); Node cur = o1; set.add(cur); // 只有头节点才等于自己的父，如果当前节点不等于自己的父，就可以往上走 while (null != parentMap.get(cur)) &#123; cur = parentMap.get(cur); set.add(cur); &#125; // o1往上所有节点都在这个set里面，只有最初的head不在里面 cur = o2; while (!set.contains(cur)) &#123; cur = parentMap.get(cur); &#125; return cur;&#125;// 维护整棵树的所有父节点到Map中private static void fillParentMap(Node head, HashMap&lt;Node, Node&gt; fatherMap) &#123; if (head.left != null) &#123; fatherMap.put(head.left, head); fillParentMap(head.left, fatherMap); &#125; if (head.right != null) &#123; fatherMap.put(head.right, head); fillParentMap(head.right, fatherMap); &#125;&#125; 解法2(非常抽象)：可能的情况有1：node1和node2互为公共祖先；2：node1和node2不互为公共祖先； 1234567891011121314/** * 解法2：可能的情况有1：node1和node2互为公共祖先；2：node1和node2不互为公共祖先； */public static Node lowestCommonAncestor(Node head, Node o1, Node o2) &#123; if (head == null || head == o1 || head == o2) &#123; return head; &#125; Node left = lowestCommonAncestor(head.left, o1, o2); Node right = lowestCommonAncestor(head.right, o1, o2); if (left != null &amp;&amp; right != null) &#123; return head; &#125; return left != null ? left : right;&#125; 图 常见的表示图的方法：临接表法，和临接矩阵法，数组等 做模板，然后把所有的图的问题转化为自己熟悉的数据结构，带着模板上考场 图的宽度优先遍历约瑟夫圆环问题逆波兰计算器 实现一个计算器，只有加减乘除法，没有括号，输入是一个字符串如10+2+3*5 解题：表达式转化为后缀表达式（逆波兰表达式） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125// 解题public static void main(String[] args) &#123; String str = &quot;10+2+3*5&quot;; // 表达式转换为操作数和操作符的中缀表达式数组 String[] strArr = changeStrArr(str); // 中缀表达式转化为后缀表达式 List&lt;String&gt; polandList = polandNotation(strArr); System.out.println(polandList); // 后缀表达式计算 System.out.println(calculate(polandList));&#125;/** * 字符串转成中缀的数组 * 只有加减乘除，没有扩号 */private static String[] changeStrArr(String str) &#123; StringBuilder stringBuilder = new StringBuilder(); for (char c : str.toCharArray()) &#123; if (&quot;+-*/&quot;.contains(String.valueOf(c))) &#123; stringBuilder.append(&quot;;&quot;); stringBuilder.append(c); stringBuilder.append(&quot;;&quot;); &#125; else &#123; stringBuilder.append(c); &#125; &#125; return stringBuilder.toString().split(&quot;;&quot;);&#125;/** * 中缀表达式转化为后缀表达式 * 1.初始化两个栈，运算符栈s1和储存中间结果的栈s2 * 2.从左到右扫描中缀表达式 * 3.遇到操作数，入栈s2 * 4.遇到运算符，比较其与s1栈顶运算符的优先级： * (1).如果s1为空，或者栈顶运算符为左括号&quot;(&quot;,直接将运算符入栈； * (2).否则，若优先级比栈顶运算符高，也将运算符入栈s1; * (3).否则，将s1栈顶运算符弹出入栈s2中，再次转到(4-1) 与s1中新的栈顶运算符比较； * 本方法只考虑到加减乘除操作，不考虑扩号 */private static List&lt;String&gt; polandNotation(String[] str) &#123; List&lt;String&gt; s2 = new ArrayList&lt;&gt;(); Stack&lt;String&gt; s1 = new Stack&lt;&gt;(); for (String itm : str) &#123; // 遇到操作数，直接入栈s2 if (itm.matches(&quot;\\\\d+&quot;)) &#123; s2.add(itm); &#125; else &#123; // 如果栈不为空，并且栈顶优先级比我目前运算符优先级高，就把栈顶运算符如s2，然后吧当前运算符如s1 while (!s1.isEmpty() &amp;&amp; Operation.getValue(s1.peek()) &gt;= Operation.getValue(itm)) &#123; s2.add(s1.pop()); &#125; s1.push(itm); &#125; &#125; while (!s1.isEmpty()) &#123; s2.add(s1.pop()); &#125; return s2;&#125;/** * 运算符比较 */private static class Operation &#123; private static int ADD = 1; private static int SUB = 1; private static int MUL = 2; private static int DIV = 2; public static int getValue(String operation) &#123; int result = 0; switch (operation) &#123; case &quot;+&quot;: result = ADD; break; case &quot;-&quot;: result = SUB; break; case &quot;*&quot;: result = MUL; break; case &quot;/&quot;: result = DIV; break; default: break; &#125; return result; &#125;&#125;/** * 逆波兰表达式计算 * 1.定义一个栈，匹配到非运算符就入栈 * 2.遇到运算符就把栈顶两个数字出栈，用后出栈的数和先出栈的数做运算，把运算结果再入栈 * 3.直到最后，栈顶结果即为计算结果 */private static int calculate(List&lt;String&gt; polandList) &#123; Stack&lt;String&gt; stack = new Stack&lt;&gt;(); for (String itm : polandList) &#123; // 匹配的是多位数 if (itm.matches(&quot;\\\\d+&quot;)) &#123; stack.push(itm); &#125; else &#123; // 弹出两个数，并运算，再入栈 int num2 = Integer.parseInt(stack.pop()); int num1 = Integer.parseInt(stack.pop()); int res = 0; if (itm.equals(&quot;+&quot;)) &#123; res = num1 + num2; &#125; else if (itm.equals(&quot;-&quot;)) &#123; res = num1 - num2; &#125; else if (itm.equals(&quot;*&quot;)) &#123; res = num1 * num2; &#125; else if (itm.equals(&quot;/&quot;)) &#123; res = num1 / num2; &#125; else &#123; throw new RuntimeException(&quot;运算符有误&quot;); &#125; stack.push(String.valueOf(res)); &#125; &#125; return Integer.parseInt(stack.pop());&#125; 前缀树1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// 前缀树数据结构public static class Node1 &#123; public int pass; public int end; public Node1[] nexts; // char tmp = &#x27;b&#x27; (tmp - &#x27;a&#x27;) public Node1() &#123; pass = 0; end = 0; // 0 a // 1 b // 2 c // .. .. // 25 z // nexts[i] == null i方向的路不存在 // nexts[i] != null i方向的路存在 nexts = new Node1[26]; &#125;&#125;// 添加元素public void insert(String word) &#123; if (word == null) &#123; return; &#125; char[] str = word.toCharArray(); Node1 node = root; node.pass++; int path = 0; for (int i = 0; i &lt; str.length; i++) &#123; // 从左往右遍历字符 path = str[i] - &#x27;a&#x27;; // 由字符，对应成走向哪条路 if (node.nexts[path] == null) &#123; node.nexts[path] = new Node1(); &#125; node = node.nexts[path]; node.pass++; &#125; node.end++;&#125;// word这个单词之前加入过几次public int search(String word) &#123; if (word == null) &#123; return 0; &#125; char[] chs = word.toCharArray(); Node1 node = root; int index = 0; for (int i = 0; i &lt; chs.length; i++) &#123; index = chs[i] - &#x27;a&#x27;; if (node.nexts[index] == null) &#123; return 0; &#125; node = node.nexts[index]; &#125; return node.end;&#125;// 所有加入的字符串中，有几个是以pre这个字符串作为前缀的public int prefixNumber(String pre) &#123; if (pre == null) &#123; return 0; &#125; char[] chs = pre.toCharArray(); Node1 node = root; int index = 0; for (int i = 0; i &lt; chs.length; i++) &#123; index = chs[i] - &#x27;a&#x27;; if (node.nexts[index] == null) &#123; return 0; &#125; node = node.nexts[index]; &#125; return node.pass;&#125;// 删除public void delete(String word) &#123; if (search(word) != 0) &#123; char[] chs = word.toCharArray(); Node1 node = root; node.pass--; int path = 0; for (int i = 0; i &lt; chs.length; i++) &#123; path = chs[i] - &#x27;a&#x27;; if (--node.nexts[path].pass == 0) &#123; node.nexts[path] = null; return; &#125; node = node.nexts[path]; &#125; node.end--; &#125;&#125; 贪心算法在某一个标准下，优先考虑最满足标准的样本，最后考虑最不满足标准的样本，最终得到一个答案的算法，叫做贪心算法。也就是说，不从整体最优上加以考虑，所做出的是某种意义上的局部最优解。 会议室占用问题问题：只有一个会议室，多个会议占用会议室的时间有冲突，如何让会议室进行的会议最多，返回最多的会议场次 解：哪个会议结束时间早，就优先安排，然后接下来继续找下一个会议结束时间早的会议 1234567891011121314151617181920212223242526272829303132// 会议开始时间和结束时间public static class Program &#123; public int start; public int end; public Program(int start, int end) &#123; this.start = start; this.end = end; &#125;&#125;// 比较器public static class ProgramComparator implements Comparator&lt;Program&gt; &#123; @Override public int compare(Program o1, Program o2) &#123; return o1.end - o2.end; &#125;&#125;// 会议的开始时间和结束时间，都是数值，不会 &lt; 0public static int bestArrange2(Program[] programs) &#123; Arrays.sort(programs, new ProgramComparator()); int timeLine = 0; int result = 0; // 依次遍历每一个会议，结束时间早的会议先遍历 for (int i = 0; i &lt; programs.length; i++) &#123; if (timeLine &lt;= programs[i].start) &#123; result++; timeLine = programs[i].end; &#125; &#125; return result;&#125; 八皇后问题多线程相关三个线程交替打印 三个线程交替打印，1线程打印1；2线程打印2；3线程打印3；1线程打印4……一直打印到100 解题方式由很多，无非就是涉及到线程通信问题以及修改共享变量问题 题解1：不加锁，利用线程可见性 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class ThreadPrint &#123; private static volatile int i = 0; private static volatile int flag = 0; public static void main(String[] args) &#123; Thread thread1 = new Thread(new Thread1()); Thread thread2 = new Thread(new Thread2()); Thread thread3 = new Thread(new Thread3()); thread1.start(); thread2.start(); thread3.start(); &#125; public static class Thread1 implements Runnable &#123; public void run() &#123; while (i &lt;= 100) &#123; if (flag == 0) &#123; System.out.println(&quot;t1=&quot; + i); i++; flag = 1; &#125; &#125; &#125; &#125; public static class Thread2 implements Runnable &#123; public void run() &#123; while (i &lt;= 100) &#123; if (flag == 1) &#123; System.out.println(&quot;t2=&quot; + i); i++; flag = 2; &#125; &#125; &#125; &#125; public static class Thread3 implements Runnable &#123; public void run() &#123; while (i &lt;= 100) &#123; if (flag == 2) &#123; System.out.println(&quot;t3=&quot; + i); i++; flag = 0; &#125; &#125; &#125; &#125;&#125; 题解2：LockSupport 实现 123456789101112131415161718192021222324252627282930313233343536373839404142public class ThreadPrint2 &#123; private static int number = 1; private static Thread thread1, thread2,thread3; public static void main(String[] args) &#123; thread1 = new Thread(ThreadPrint2::thread1, &quot;thread1&quot;); thread2 = new Thread(ThreadPrint2::thread2, &quot;thread2&quot;); thread3 = new Thread(ThreadPrint2::thread3, &quot;thread3&quot;); thread1.start(); thread2.start(); thread3.start(); &#125; public static void thread1() &#123; while (ThreadPrint2.number &lt;= 100) &#123; System.out.println(&quot;thread1:&quot; + ThreadPrint2.number); ThreadPrint2.number++; LockSupport.unpark(thread2); LockSupport.park(); &#125; &#125; public static void thread2() &#123; while (ThreadPrint2.number &lt;= 100) &#123; LockSupport.park(); System.out.println(&quot;thread2:&quot; + ThreadPrint2.number); ThreadPrint2.number++; LockSupport.unpark(thread3); &#125; &#125; public static void thread3() &#123; while (ThreadPrint2.number &lt;= 100) &#123; LockSupport.park(); System.out.println(&quot;thread3:&quot; + ThreadPrint2.number); ThreadPrint2.number++; LockSupport.unpark(thread1); &#125; &#125;&#125; 题解3：经典实现 实现wait-&gt;notifyAll 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class ThreadPrint3 &#123; private static int count = 1; public static void main(String[] args) &#123; Object lock = new Object(); new Thread(() -&gt; &#123; try &#123; synchronized (lock) &#123; while (count &lt;= 100) &#123; if (count % 3 == 1) &#123; System.out.println(Thread.currentThread().getName() + &quot;---&gt;&quot; + count); count++; &#125; lock.notifyAll(); lock.wait(); &#125; lock.notifyAll(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, &quot;T1&quot;).start(); new Thread(() -&gt; &#123; try &#123; synchronized (lock) &#123; while (count &lt;= 100) &#123; if (count % 3 == 2) &#123; System.out.println(Thread.currentThread().getName() + &quot;---&gt;&quot; + count); count++; &#125; lock.notifyAll(); lock.wait(); &#125; lock.notifyAll(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, &quot;T2&quot;).start(); new Thread(() -&gt; &#123; try &#123; synchronized (lock) &#123; while (count &lt;= 100) &#123; if (count % 3 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;---&gt;&quot; + count); count++; &#125; lock.notifyAll(); lock.wait(); &#125; lock.notifyAll(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, &quot;T3&quot;).start(); &#125;&#125; 题解4：Lock实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class ThreadPrint4 &#123; private static int cnt = 0; public static void main(String[] args) &#123; Lock lock = new ReentrantLock(); Condition c1 = lock.newCondition(); Condition c2 = lock.newCondition(); Condition c3 = lock.newCondition(); new Thread(() -&gt; &#123; lock.lock(); try &#123; while (cnt &lt;= 100 &amp;&amp; cnt % 3 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;----&gt;&quot; + cnt); cnt++; c2.signal(); c1.await(); &#125; c3.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;, &quot;T1&quot;).start(); new Thread(() -&gt; &#123; lock.lock(); try &#123; while (cnt &lt;= 100 &amp;&amp; cnt % 3 == 1) &#123; System.out.println(Thread.currentThread().getName() + &quot;----&gt;&quot; + cnt); cnt++; c3.signal(); c2.await(); &#125; c1.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;, &quot;T2&quot;).start(); new Thread(() -&gt; &#123; lock.lock(); try &#123; while (cnt &lt;= 100 &amp;&amp; cnt % 3 == 2) &#123; System.out.println(Thread.currentThread().getName() + &quot;----&gt;&quot; + cnt); cnt++; c1.signal(); c3.await(); &#125; c2.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;, &quot;T3&quot;).start(); &#125;&#125; 其他算法题","categories":[{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://zhangxin66666.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"业务架构-稳定性建设方法论","slug":"11.方法论/业务架构-稳定性建设方法论","date":"2022-01-16T16:00:00.000Z","updated":"2024-08-22T03:23:14.703Z","comments":true,"path":"2022/01/17/11.方法论/业务架构-稳定性建设方法论/","link":"","permalink":"https://zhangxin66666.github.io/2022/01/17/11.%E6%96%B9%E6%B3%95%E8%AE%BA/%E4%B8%9A%E5%8A%A1%E6%9E%B6%E6%9E%84-%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%BB%BA%E8%AE%BE%E6%96%B9%E6%B3%95%E8%AE%BA/","excerpt":"","text":"先介绍几个概念，同步一下认知： 容灾：是指系统冗余部署，当一处由于意外停止工作，整个系统应用还可以正常工作。 容错：是指在运行中出现错误（如上下游故障或概率性失败）仍可正常提供服务。 可用性：描述的是系统可提供服务的时间长短。用公式来说就是正常工作时间/(正常工作时间+故障时间)。 可靠性：描述的是系统指定时间单位内无故障的次数。比如：一年365天，以天为单位来衡量。有天发生了故障，哪怕只有1秒，这天算不可靠。其他没有故障的是可靠的。 稳定性：这个业界没有明确的定义，我的理解是：在受到各种干扰时仍然能够提供符合预期的服务的能力。 一、容灾建设1.代码容灾 现代互联网公司最值钱的就是程序员写出来的代码啦，所以我们需要对代码进行容灾备份，防止放代码的服务器哪天一不高兴宕机异或爆炸啦数据都丢了就完蛋了，现在代码同步的方案比较多，是在不行运维人员写个脚本，每天或者几个小时自动同步一下全量代码到另一个机房还是可以做到滴。 2.服务容灾服务容灾的解决方案就是冗余。多几个备份来切换。 2.1同城容灾同城 容灾 是在同城或相近区域内 （ ≤ 200K M ）建立两个数据中心 : 一个为数据中心，负责日常生产运行 ; 另一个为灾难备份中心，负责在灾难发生后的应用系统运行。同城灾难备份的数据中心与灾难备份中心的距离比较近，通信线路质量较好，比较容易实现数据的同步 复制 ，保证高度的数据完整性和数据零丢失。同城灾难备份一般用于防范火灾、建筑物破坏、供电故障、计算机系统及人为破坏引起的灾难 2.2异地/跨洋容灾异地 容灾 主备中心之间的距离较远 （＞ 200KM ) ， 因此一般采用异步镜像，会有少量的数据丢失。异地灾难备份不仅可以防范火灾、建筑物破坏等可能遇到的风险隐患，还能够防范战争、地震、水灾等风险。由于同城灾难备份和异地灾难备份各有所长，为达到最理想的防灾效果，数据中心应考虑采用同城和异地各建立一个灾难备份中心的方式解决。 2.3两地三中心两地三中心 ： 是指 同城双中心 加 异地灾备 一种商用容灾备份解决方案； 两地 是指同城、异地； 三中心 是指生产中心、同城容灾中心、异地容灾中心。（ 生产中心、同城灾备中心、异地 灾备 中心 ） 结合近年国内出现的大范围自然灾害，以同城双中心加异地灾备中心的 “两地三中心”的灾备模式也随之出现，这一方案兼具高可用性和灾难备份的能力。 同城双中心 是指在同城或邻近城市建立两个可独立承担关键系统运行的数据中心，双中心具备基本等同的业务处理能力并通过高速链路实时同步数据，日常情况下可同时分担业务及管理系统的运行，并可切换运行；灾难情况下可在基本不丢失数据的情况下进行灾备应急切换，保持业务连续运行。与异地灾备模式相比较，同城双中心具有投资成本低、建设速度快、运维管理相对简单、可靠性更高等优点。 异地灾备中心 是指在异地的城市建立一个备份的灾备中心，用于双中心的数据备份，当双中心出现自然灾害等原因而发生故障时，异地灾备中心可以用备份数据进行业务的恢复。 二、服务治理1.链路梳理需要梳理核心接口服务的血缘依赖关系、相关干系人及联系方式，做到心中有数，关键接口调用链路流程尽量短，下游依赖接口tp999尽量越快越好。 2.强弱依赖分析强依赖：假定服务A依赖于服务B，服务B出现故障不可用时，服务A也不可用，通常服务A会返回错误信息，我们称这种依赖为强依赖。 弱依赖：假定服务A依赖于服务B，服务B出现故障不可用时，服务A仍然可用，通常服务A会返回正确信息，只是与服务B相关的信息会不返回或者做默认处理，我们称这种依赖为弱依赖。 通过下面的流程图，我们来分析一下强弱依赖。从图中可以看到，在服务A中调用了服务B和服务C，但是他们的处理逻辑是不一样的。 1、调用服务B：如果调用成功，则接着调用服务C；如果调用失败，则服务A直接结束。这种场景我们称之为服务A强依赖于服务B。 2、调用服务C：不管调用成功还是失败，服务A会接续执行后续逻辑处理。这种场景我们称之为服务A弱依赖于服务C。 12345678910111213141516171819public void serviceA()&#123; try &#123; System.out.println(&quot;it`s serviceA&quot;); /// serviceA 强依赖于 serviceB serviceB(); try &#123; /// serviceA 弱依赖于 serviceC serviceC(); &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; System.out.println(&quot;xxxxxxx&quot;); &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; 在代码的处理上，可以分为一下三种模式：1、弱依赖：服务A的主流程不依赖服务B的返回结果。该场景可以有两种解决方式：1）可以启动单独的线程进行服务B调用。2）在当前线程中发消息，在消息消费线程中访问服务B。 2、弱依赖：服务A的主流程依赖服务B的返回结果。与强依赖处理有些类似，一般建议对服务B做降级处理，根据请求超时时间、并发请求数、请求失败数、请求返回的错误码做降级(或者熔断)处理。同时使用默认值来代替服务B的返回结果，默认值的设置需要根据具体的业务场景进行分析。 3、强依赖：服务A的主流程依赖服务B的返回结果。一般建议对服务A整体做降级处理，请求返回的错误码。 在系统设计时一定要考虑系统的强弱依赖关系，根据需要采用不同的处理方案。 3.异步化建设普通流量下的接口流程设计上可以一个接口，做了好多好多事情，之后接口同步实时返回处理成功或者处理失败。在高并发场景下是非常糟糕的设计，接口一方面业务逻辑非常的重，另一方面，同步接口依赖的很多的三方接口，也可能会多次的操作数据库，一个点出问题都会造成整个业务的瘫痪。此时可以把整个流程异步化，同步接口里面只做一些校验工作以及数据的保存，之后发送mq触发后续流程的执行，此时实时接口返回处理中，异步流程中处理完毕后通过回调的方式通知调用方结果，同时提供查询接口供调用方主动获取结果。 需要注意的是因为是通过发mq的方式触发后续流程，那么就要保证mq一定发送成功，此处牵扯到分布式事务相关的方案，不同的业务逻辑可能会使用不同的解决办法。（本地事务表或者依靠状态来驱动都可以） 4.切量/降级/限流/熔断4.1切量切量是新业务上线初期，或者大版本更新的时候，会对用户进行切量开放xx功能，用少量的生产流量验证流程，如果没有问题慢慢切量剩余用户，如果有问题，也会将问题影响面降到最低。 4.2降级降级是通过开关配置将某些不重要的业务功能屏蔽掉，以提高服务处理能力。 在大促场景中经常会对某些服务进行降级处理，大促结束之后再进行复原。 本系统或者下游系统出现问题时，当前系统可以手动对服务进行降级操作，问题修复后关闭降级。 大促场景时会对日志打印进行降级，防止磁盘io对系统造成影响，另外也会避免正好大促时磁盘空间满了，系统没来得及回收日志造成服务不可用。 为什么要降级在不影响业务核心链路的情况下，屏蔽某些不重要的业务功能，可以节省系统的处理时间，提供系统的响应能力，在服务器资源固定的前提下处理更多的请求。 4.3限流限流是针对服务请求数量的一种自我保护机制，当请求数量超出服务的处理能力时，会自动丢弃新来的请求。 为什么要限流任何一个系统的处理能力都是有极限的，假定服务A的处理能力为TPS=100，当TPS&lt;100时服务A可以提供正常的服务。当TPS&gt;100时，由于请求量增大，会出现争抢服务资源的情况（数据库连接、CPU、内存等），导致服务A处理缓慢；当TPS继续增大时，可能会造成服务A响应更加缓慢甚至奔溃。如果不进行限流控制，服务A始终会面临着被大流量冲击的风险。所以我们需要做好系统请求流量的评估，制定合理的限流策略。 4.4熔断在服务的依赖调用中，被调用方出现故障时，出于自我保护的目的，调用方会主动停止调用，并根据业务需要进行相应处理。调用方这种主动停止调用的行为我们称之为熔断。 为什么要熔断假定服务A依赖服务B，当服务B处于正常状态，整个调用是健康的，服务A可以得到服务B的正常响应。当服务B出现故障时，比如响应缓慢或者响应超时，如果服务A继续请求服务B，那么服务A的响应时间也会增加，进而导致服务A响应缓慢。如果服务A不进行熔断处理，服务B的故障会传导至服务A，最终导致服务A也不可用。 5.traceid日志中串一次请求的id，系统并发的时候，多次请求的日志会交叉打印，排查问题会比较难受，可以在代码中加入traceid，可以串起来一次完整请求，有条件的可以把traceid进行传递，这样就能用traceid把一起完整的请求在各应用系统中一起串起来，也就实现了链路追踪功能。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172//Constant中的TRACE_ID定义//日志中的traceidpublic static final String TRACE_ID = &quot;traceId&quot;;//traceidInterceptorimport org.slf4j.MDC;import org.springframework.util.StringUtils;import org.springframework.web.servlet.HandlerInterceptor;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.util.UUID;@WebServletpublic class TraceIDInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; String traceID = request.getHeader(Constant.TRACE_ID); if (StringUtils.isEmpty(traceID)) &#123; traceID = UUID.randomUUID().toString(); &#125; MDC.put(Constant.TRACE_ID, traceID); return true; &#125;&#125;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.InterceptorRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;@Configurationpublic class InterceptorConfig implements WebMvcConfigurer &#123; @Override public void addInterceptors(InterceptorRegistry interceptorRegistry) &#123; // 多个拦截器组成一个拦截器链 // addPathPatterns 用于添加拦截规则 // excludePathPatterns 用户排除拦截 interceptorRegistry.addInterceptor(new TraceIDInterceptor()).addPathPatterns(&quot;/**&quot;); &#125;&#125;//如果feign方式调用往下游传递的可以加上下面的Interceptorimport feign.RequestInterceptor;import feign.RequestTemplate;import org.slf4j.MDC;import org.springframework.context.annotation.Configuration;import org.springframework.util.StringUtils;import java.util.UUID;@Configurationpublic class FeignInterceptor implements RequestInterceptor &#123; @Override public void apply(RequestTemplate requestTemplate) &#123; String traceID = MDC.get(Constant.TRACE_ID); if (StringUtils.isEmpty(traceID)) &#123; traceID = UUID.randomUUID().toString(); &#125; requestTemplate.header(Constant.HEADER_TRACE_ID, traceID); &#125;&#125;//日志配置文件中加入[%X&#123;traceId&#125;]//此时接口调用的traceid加完了，但是例如mq、调度之类的可能不行，需要在此类代码入口手动设置traceidMDC.put(Constant.TRACE_ID, UUID.randomUUID().toString()); 6.方法执行时间注解一个接口调用了一次数据库查询，一次数据库保存，两次rpc。某一天上游反馈接口慢，该如何进行排查呢？ 优化接口慢，那就得找到影响接口耗时的关键点，我们如果知道了数据库查询耗时，数据库保存耗时，两次rpc的分别耗时，是否问题就迎刃而解了呢 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//自定义Timer注解import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;/** * 标注需要记录时间消耗的方法 */@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface Timer &#123;&#125;//aop环绕获取方法执行耗时import lombok.extern.slf4j.Slf4j;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.springframework.stereotype.Component;/** * 时间记录切面，收集接口的运行时间 */@Slf4j@Aspect@Componentpublic class TimeAspect &#123; // 修正Timer注解的全局唯一限定符 @Around(&quot;@annotation(com.renrenche.pricing.process.annotation.Timer)&quot;) public Object around(ProceedingJoinPoint joinPoint) throws Throwable &#123; // 获取目标类名称 String clazzName = joinPoint.getTarget().getClass().getName(); // 获取目标类方法名称 String methodName = joinPoint.getSignature().getName(); long start = System.currentTimeMillis();// log.info(&quot;================================&#123;&#125;: &#123;&#125;: start...&quot;, clazzName, methodName); // 调用目标方法 Object result = joinPoint.proceed(); long time = System.currentTimeMillis() - start; log.info(&quot;耗时统计:clazzName:&#123;&#125;,methodName:&#123;&#125;,time:&#123;&#125; ms&quot;, clazzName, methodName, time); return result; &#125;&#125;//用法，在需要监控耗时的方法上添加@Timer注解即可耗时统计:clazzName:com.xx.xx.XX,methodName:queryXX,time:10 ms耗时统计:clazzName:com.xx.xx.XX,methodName:saveXX,time:20 ms耗时统计:clazzName:com.xx.xx.YY,methodName:queryYY,time:50 ms耗时统计:clazzName:com.xx.xx.ZZ,methodName:queryZZ,time:500 ms我们只需要去找下游zz系统负责人沟通接口耗时问题即可 7.业务自定义异常优化关于系统内部自定义业务异常，单系统内部定义一个即可，另外需要重写Throwable中fillInStackTrace方法，原因有二，一方面原fillInStackTrace方法被synchronized修饰，另一方面fillInStackTrace方法中设置了很多堆栈信息，对于自定义业务异常来说是不太需要的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 自定义业务异常 */public class BizException extends RuntimeException &#123; private static final long serialVersionUID = 1L; private String errorCode; private String errorMessage; public BizException(String errorCode, String errorMessage) &#123; this.errorMessage = errorMessage; this.errorCode = errorCode; &#125; public String getErrorCode() &#123; return this.errorCode; &#125; public void setErrorCode(String errorCode) &#123; this.errorCode = errorCode; &#125; public String getErrorMessage() &#123; return this.errorMessage; &#125; public void setErrorMessage(String errorMessage) &#123; this.errorMessage = errorMessage; &#125; //重写以提高性能 @Override public Throwable fillInStackTrace() &#123; return this; &#125; @Override public String toString() &#123; return &quot;AppRuntimeException&#123;&quot; + &quot;errorCode=&#x27;&quot; + errorCode + &#x27;\\&#x27;&#x27; + &quot;, errorMessage=&#x27;&quot; + errorMessage + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 8.错误文案转换器提供给app前端或者web管理后台相关的写接口，接口返回错误码的时候，前端同学一般会tost提示用户错误信息，一般情况下前端同学要么全部提示系统太火爆了，要么会跟后端同学要接口的错误码列表，然后跟产品一起确认每个错误码该tost什么信息，之后if else 错误码和tost文案一一映射，如果后端新加了错误码，需要前端一起更改，没有及时通知，可能前端页面会弹出技术性文案。 如果所有的友好性提示统一是后端下发呢？是不是就可以规避类似的问题，前端同学只需关心如果正常返回，提示成功，接口没有成功，直接tost后端返回的错误msg提示即可，你好我好大家好～ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475###############yml文件配置################错误信息转换，给前端以友好提示errormsgconfig: errorMsgConvertMap: #key以controller_method命名 PricingController_setPricing: default: 系统太火爆了，请您稍后再试~ 0: 定价成功 1: 系统太火爆了，请您稍后再试~ 5: 请刷新后重试 PricingController_getPricingOrderDetail: default: 系统太火爆了，请您稍后再试~ -42: 没有权限！###############config文件###############@Slf4j@ConfigurationProperties(prefix = &quot;errormsgconfig&quot;)@Component@Setter@Getterpublic class ErrorMsgConvertConf &#123; private Map&lt;String, Map&lt;String,String&gt;&gt; errorMsgConvertMap;&#125;###############错误文案转换Adapter###############/** * 错误信息转换，给前端以友好提示 */@Slf4j@Servicepublic class ErrorMsgAdapter &#123; @Autowired private ErrorMsgConvertConf errorMsgConvertConf; /** * 错误信息转换，给前端以友好提示 */ @Timer public void handlerErrorMsg(String convertKey, Response res) &#123; Map&lt;String, Map&lt;String, String&gt;&gt; errorMsgConvertMap = errorMsgConvertConf.getErrorMsgConvertMap(); Map&lt;String, String&gt; convertMap = errorMsgConvertMap.get(convertKey); if (StringUtils.isBlank(convertKey) || Objects.isNull(res)) &#123; log.info(&quot;ErrorMsgAdapter.handlerErrorMsg,参数为空，直接返回&quot;); return; &#125; log.info(&quot;出参返回信息转换开始，convertKey:&#123;&#125;,oldRes:&#123;&#125;&quot;, convertKey, JSONObject.toJSONString(res)); if (ErrorCode.OK.getCode() == res.getStatus() &amp;&amp; !CollectionUtils.isEmpty(convertMap)) &#123; String codeMsg = convertMap.get(Integer.toString(ErrorCode.OK.getCode())); if (StringUtils.isNotBlank(codeMsg)) &#123; res.setErrMsg(codeMsg); &#125; &#125; else if (ErrorCode.OK.getCode() != res.getStatus() &amp;&amp; !CollectionUtils.isEmpty(convertMap)) &#123; String defaultMsg = convertMap.get(&quot;default&quot;); if (StringUtils.isBlank(defaultMsg)) &#123; defaultMsg = &quot;系统太火爆了，请您稍后再试~&quot;; &#125; String codeMsg = convertMap.get(Integer.toString(res.getStatus())); if (StringUtils.isNotBlank(codeMsg)) &#123; res.setErrMsg(codeMsg); &#125; else &#123; res.setErrMsg(defaultMsg); &#125; &#125; log.info(&quot;出参返回信息转换结束，convertKey:&#123;&#125;,oldRes:&#123;&#125;&quot;, convertKey, JSONObject.toJSONString(res)); &#125;&#125;###############使用方法#################controller出参前调用即可，convertKey建议使用controllerName_methodName//错误码转换errorMsgAdapter.handlerErrorMsg(&quot;PricingController_setPricing&quot;, ret); 9.validationutil很多人喜欢在controller层的req前面增加 @Validated，直接对入参进行格式校验，但是这样做如果字段格式不符合要求，不会进行入参打印，不利于问题排查，另一方面抛出的异常也不是我们指定的。 12345678910111213141516171819202122232425262728293031import com.renrenche.pricing.process.exception.BizException;import com.renrenche.rest.protocol.ErrorCode;import lombok.extern.slf4j.Slf4j;import org.springframework.util.CollectionUtils;import javax.validation.ConstraintViolation;import javax.validation.Validation;import javax.validation.Validator;import javax.validation.ValidatorFactory;import java.util.Set;@Slf4jpublic class ValidationUtil &#123; private static ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); public static &lt;T&gt; void validate(T t) &#123; Validator validator = factory.getValidator(); Set&lt;ConstraintViolation&lt;T&gt;&gt; constraintViolations = validator.validate(t); if(!CollectionUtils.isEmpty(constraintViolations))&#123; StringBuilder errMsg=new StringBuilder(&quot;参数校验失败&quot;); for (ConstraintViolation&lt;T&gt; constraintViolation : constraintViolations) &#123; errMsg.append(&quot;,&quot;).append(constraintViolation.getMessage()); &#125; log.info(&quot;ValidationUtil校验参数失败&quot;); throw new BizException(ErrorCode.PARAM_INVALID, errMsg.toString()); //此处可以定义一个专门的业务异常码值来代表参数异常，msg放的是全量信息，可以for循环返回全量未通过原因，也可以只返回单条，不过多次请求返回的信息会有不同。 &#125; &#125;&#125;//用法 代码显示调用ValidationUtil.validate(xxx); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import com.renrenche.pricing.process.validation.ValidationUtil;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;import org.hibernate.validator.constraints.Length;import javax.validation.constraints.*;import java.io.Serializable;import java.math.BigDecimal;import java.util.List;@Setter@Getter@NoArgsConstructorpublic class DemoReq implements Serializable &#123; @NotBlank(message = &quot;name不能为空&quot;) @Length(min = 6, max = 30, message = &quot;name应该在6-30字符之间&quot;) private String name; @NotNull(message = &quot;年龄不能为空&quot;) @Min(value = 18, message = &quot;年龄必须大于等于18&quot;) @Max(value = 20, message = &quot;年龄必须小于等于20&quot;) private Integer age; @NotEmpty(message = &quot;nameList不能为空&quot;) private List&lt;String&gt; nameList; @Pattern(regexp = &quot;^(.+)@(.+)$&quot;, message = &quot;邮箱的格式不合法&quot;) private String email; private Byte type; private Integer type2; @Pattern(regexp = &quot;aaaaa|bbbbb|ccccc&quot;, message = &quot;type3不在枚举范围内&quot;) private String type3; @DecimalMin(value = &quot;150&quot;, message = &quot;必须大于等于150&quot;) @DecimalMax(value = &quot;300&quot;, message = &quot;必须小于等于300&quot;) @Digits(integer=3,fraction = 2,message = &quot;整数位上限为3位，小数位上限为2位&quot;) private BigDecimal money; public static void main(String[] args) &#123; DemoReq demoReq=new DemoReq(); demoReq.setName(&quot;1111111&quot;);// demoReq.setType3(&quot;222&quot;); ValidationUtil.validate(demoReq); &#125; //如果有对象实体，需要深入校验对象内部字段格式的，需要在实体上增加@valid注解&#125; 10.统一日志打印各公司内部应该针对于日志打印格式形成一个统一的规范，方便日志采集，不管是最终收集到公司内部自建日志平台，还是elk，格式还是得统一的。 抛砖引玉： log4j2 配置： 1Pattern: &quot;[%X&#123;trace_id&#125;] %-d&#123;yyyy-MM-dd HH:mm:ss&#125; - [%p] [%C&#123;1&#125; %M] %m%n&quot; logback配置 1&lt;pattern&gt;[%X&#123;trace_id&#125;] %d&#123;yyyy-MM-dd HH:mm:ss&#125; - [%level] [%class&#123;0&#125; %method] %msg%n&lt;/pattern&gt; 日志示例： [ed02f4ae5f18415b8a0143100b585f0e] 2019-03-14 18:32:08 - [INFO] [LogController test] this is my log!!! 另外很关键的日志打印必不可少，包括自己接口的入参&amp;返回，以及调用三方rpc接口的入参&amp;返回，用于查询线上问题，跟三方扯皮 ，屡试不爽。 11.接入统一日志平台之前服务是单体架构，一个服务就对应一台服务器，排插生产问题时，登陆服务器即可。 现在都是微服务架构，一个服务一般对应多台服务器，jd、阿里之类的核心系统服务器一个项目都是上千上万台，此时排查问题如果一台一台登陆去查找就根本没法处理了。 所以一般公司内部都会有自建日志平台或者elk来支持生产日志的存储及查询。 一般的一个查询流程：根据关键词查询到单条日志，获取到traceid，然后继续根据traceid查询出来本次请求的全量日志进行排查。 12.日志级别动态变更一般有三个很实际的场景会使用，一个是大促（双十一、618），一个是秒杀，另一个是线上出问题但是还未定位到问题时。 前两种是因为用户合法请求非常多，打印的日志非常多，最后一种是用户会频繁点击，流量激增且错误日志打印非常多。此时都会造成机器io压力非常大。再介绍一下一般情况下运维清理服务器日志的流程，一般是定时每隔xx时间去扫描服务器磁盘空间剩余百分比，到达阈值会进行日志备份，之后删除服务器上日志。但是上面三种情况下，会有两个问题，首先是磁盘io会非常高，会影响接口性能，另外在运维同学脚本扫描的间隙，磁盘满了，此时服务就废了，这种关键时间点出事情，P0少不了，轻则个人拜拜，重则部门解散，后果很严重。 一般会对日志进行级别的动态调整，平时线上为info，大促时降到error，线上大流量服务出问题时，可以关闭error日志。更优化的一个解决方案时不按照项目级别统一对日志级别进行调整，还是针对单个的logger类来进行调整，这样会更有针对性一些，因为日志降级是个双刃剑，会提升系统性能，但是同样会丢失日志。 13.丝滑上线三个方面。 技术层面。 业务层面。发版经验 。 技术层面： 以Nacos注册中心为例，服务a有两台服务器，一般上线的时候大家就直接先部署服务器1，启动成功后再部署服务器2，此时会有什么问题呢，直接重启服务器1的时候，Nacos是不知道服务器1下线了，还会把请求转发到服务器1上面，过一段时间Nacos心跳检测发现服务器1宕机了，此时才会停止转发请求到服务器1，但是上游来看已经有超时失败的请求了。 一个合理的上线流程是先在Nacos上对服务器1的机器进行下线，观察服务器1日志，确认无请求进入后，重启服务器1，此时Nacos会自动重新上线服务器1，之后观察服务器1有正常请求进入后，在Nacos上对服务器2的机器进行下线，观察服务器2的日志，确认无请求进入后，重启服务器2，确保服务器2请求正常进入，本次发版结束。 为啥需要Nacos上对服务器下线以后还要继续观察日志呢，因为在Nacos下线一台服务器后，实际生效会有一个短暂的间隔。 上面这样做对于一个服务就三五台服务器的机器来说，还算有可操作性，大厂里面服务器都是几十台起步，有的上千台，这样搞效率很低，所以一般每个公司都会有自建的发版平台（eg：devops），在对机器发版的时候，会自动进行服务流量下线、发版、上线这样的操作，对用户来讲就是选择几台机器，然后发版就ok了。 业务层面： 经常会有的一个需求是对某个接口的流程进行升级，而且还有可能是颠覆性的，此时如果直接上线，出点问题，整个流程都废了，P0怕是跑不了。 一般这种颠覆性或者重构性质的升级会保留老流程的逻辑，这就保证了有问题的快速流程切换，另外一方面会对新老流程进行切量，上线后都是100%老流程，之后1%新流量，观察，看请求是否符合预期，有问题，立马100%切老流程，没问题，接口逻辑正常，符合预期，继续5%，10%，直到100%新流程。 发版经验： 发版的时候一定要先只发一台，完了以后观察本次代码修改部分是否正常，然后也要观察error日志，看是否有影响其他流程，观察一段时间无问题，在批量对其他服务器进行发版。 有些公司服务器会分组，有的按机房分组，有的按调用方分组，此时不同分组的配置文件可能是会有些不同的，所以此时发版需要每个分组的机器各自发版一台，按上述流程进行观察，无问题后，对其他机器进行批量发版。配置文件的少配，配错经常发生，对线上操作一定要保持敬畏之心。 14.服务隔离核心服务需要分组部署提供服务，两种场景可以使用，服务是公共服务，其中一个业务方特别关键，公司黄金链路种的一环，另一个是多个业务方，其中一个量级特别大，其他的加起来也就一丢丢，此时可以进行服务隔离。 类似于dubbo类型的服务可以分不同别名来供不同业务方使用来做到服务隔离 http类型的服务可以根据业务情况部署多套，举个例子：nginx A转发到服务器1和2，nginx B转发到服务器3和4，不用业务方调用不同的请求链接来进行服务隔离 15.远程rpc调用超时时间及重试控制为什么要进行超时设置呢？ 由于下游服务响应过慢、线程死锁、线上BUG等一系列原因导致我们作为调用方调用之后一直没有响应。从而最终导致用户请求长时间得不到响应，同时长时间占有线程资源甚至会拖垮整个服务影响其他正常请求。所以我们应该进行超时设置，这是微服务中快速失败的一个基本手段。 第二个问题，我们该如何设置呢？ 通常有两个层面的超时：服务级超时、接口级别的超时。服务级别的超时时间一般以最慢接口的耗时时间为基准。而且大部分框架都只会有服务级的超时设置，很少有接口级的超时设置。接口级的超时设置，没有必要所有接口都设置，仅针对流量极大、性能要求较高的接口进行单独设置就行。 第三个问题，服务超时时我们该如何处理呢？ 基于这个这个问题，我们需要从上下游、流量、业务场景这三个维度进行考量。 超时的处理手段一般有两种：重试、异常快速返回。写场景通常不要重试（会有幂等相关问题，不同接口幂等实现方式不同）。流量较大的接口一般不适合重试，直接返回异常就好，最极端的场景下容易导致流量翻n倍，导致后面的链路奔溃。 具体超时时间和重试次数建议根据下游接口的压测TP999来进行设置。 eg：a服务依赖b（读）、c（读）、d（写）服务，b服务tp999为15ms，c服务tp999为100ms，d服务tp999为20ms，a服务要求tp999必须在200ms以内，那么我们可以这样设置： b服务（超时时间20ms，重试一次） C服务（超时时间120ms，不重试） d服务（超时时间30ms，不重试） b极端情况20*2+c极端120+d极端30=190ms&lt;200ms 16.幂等处理接口的幂等一般采用两层防护措施进行防重设计。 首先一般接口会加上调用请求流水号字段 1.Redis的分布式锁来解决同类型请求的并发（相同用户并发请求，看情况，有的业务也可以不需要，因为毕竟有数据看兜底） 2.1分布式锁中按照请求流水号查询数据，有记录则直接返回现有数据状态及数据，无记录继续往下执行业务逻辑 2.2分布式锁中执行业务逻辑，然后保存业务数据，通过抓取保存时数据库报唯一键值冲突错来达到幂等处理，返回数据现有情况 3.DB层，请求唯一流水号设置唯一索引，保证DB底层只有一条数据入库。 通常情况幂等操作会按照上述逻辑进行，相同流水号怎么请求，返回的都是当前时间点该条数据的真实情况，但是也有做的简单的情况，按照流水号查询，有数据，抛异常，直接返回上游错误码，但是需要注意如果用这种方式，一方面需要保证该接口幂等错误码唯一，不要其他校验失败也返回这个错误码，另一方面要暴露查询接口，让上游收到该错误码后可以继续进行查询来得到最新的状态等信息，一般建议用上面的方案，你好我好大家好，大家写代码都挺舒服对吧。 三、数据治理1.慢查询治理公司内部有慢sql相关自动报警最好，如果没有，尽量每隔一段时间主动找dba看看自己负责系统相关库表中是否有慢sql，然后在根据不同场景来决定优化业务流程或者修改增加索引。 2.数据备份与恢复核心服务数据库主从同步还是需要有的，主库出问题的时候可以很快的进行主从切换，另一方面变相的做了数据备份，如果一些非实时查询逻辑也可以走从库。 3.冷热数据分离冷热数据分离可以有两个层面的分离，一个是把非常热点的数据放到缓存中，查询的时候热点数据从缓存中查询，非热点数据查询数据库；另一个层面冷热可以指比如近一年的数据相对经常查询，放到一张业务表中，一年前的数据在另一张表中，这样在查询正常业务的时候表中数据尽可能的少，性能相对就提高了，需要查询历史数据时查询另外的表或者库。 4.历史数据归档跟冷热数据分离有些异曲同工之处，比如银行流水查询，可以在App内查询近一两年的数据，查询历史数据需要去营业厅查询，相当于业务库中只保留近一两年的数据，再往前之前的数据会做归档，放到比如ES或者hbase等大数据存储介质中 四、变更管控1.发布、资源变更每次上线前，准备本次上线checklist，包含数据库变更，数据库初始化数据，配置文件，发版先后顺序，相关数据库、redis、mq等资源申请等 2.数据变更数据库变更表结构，人工插入、修改、变更线上数据，需要经过评估。 eg：修改数据是否会对线上流程造成影响，增加索引时库中多少数据，大体需要多长时间等等 3.流量灰度线上有10台服务正在提供服务，发版的时候直接一股脑全都发版么，如果代码比如有些配置有问题，线上机器就全部完蛋了。 可以两个方面处理，一个是在启动10台服务，流量0，保证新的项目起来后，逐步10%、20%。。。100%把流量切过来，亦或是10台里面找一台先发布，并在启动后进行观察，是否有问题，在逐步启动其他机器。 4.A/B Testing使用老版本，一部分用户开始用新版本，如果用户对新版本没有什么意见，那么逐步扩大范围，把所有用户都迁移到新版本上面来 某些功能上线前后区别很大，此时可以后端两套逻辑，基于用户唯一id取模，保证相同的用户每次进来看到的都是相同的，不要每次进来一会新版，一会老版。 AB很大程度上是基于用户维度做一些事情，而流量灰度则不care这些，整体流量百分比来的，可能用户一会走到新代码里，一会老代码里 五、容量评估1.全链路压测压测指的基于当前负责的系统为入口，而全链路指的是基于页面访问的接口作为入口。 类似于618，业务会根据往年流量来预估本次的流量，技术侧也同样会基于往年每次的增量来进行预估，看是否可以包住业务的预估数据，每个接口都会梳理进4-5次大促的流量峰值数据，以次进行推算，最终决定出来本年度全链路压测量级、每个接口的压测量级。之后压测量级同步下游，最外层压测接口比如要求tps10w/s，下游各接口根据实际情况进行数值调整，没有循环调用的可以定位11-12w/s，有循环调用的下游需要进行相应的翻倍操作，作为底层服务的需要沟通各上游调用方，汇总得出系统本次需要支持的tps量级。 接下来就是各业务系统压测各自接口，都达标后，进行多轮全链路压测（军演），还会在线上进行破坏性压测，量级达到目标后继续往上走，直到下游有服务扛不住为止，对系统设计、代码编写都有非常大的考验，下游系统挂了，如何尽可能的不影响自身业务，并且等待下游恢复后如何重试这段时间内的请求等等～ 2.容量规划服务上线申请多少台服务器？多少台服务器可以应对本次大促流量峰值？ eg：比如一个项目中只有一个普通的没有使用多线程的接口，接口tp999在200ms，可以预估一秒一个线程处理5个请求，容器（如tomcat）配置200线程，那么单机可以支撑1000tps，要求1w tps，最少需要10台机器，为了一些极端情况可以先申请12-15台进行压测，看服务器各项指标情况，在达标的时候cpu60-80%为宜，这样天然的微峰值留有一些余地。 使用多线程的接口，需要对线程进行编排，调整线程池的各项参数使性能达到最优，单服务器能够支持的并发肯定是低于上面这种情况，具体能够支持多少，需要压测来进行统计。 但是现实情况往往一个项目中有若干个接口，此时流量特别小的接口可以忽略，比如tps低于5的，之后统计各接口需要各自达到什么量级的目标，之后进行混压，要求各接口都要达标的情况下看需要多少机器。 3.弹性能力建设此处是类似于devops公司发版平台需要考虑的一些问题，配合流量预测和感知，发现目前服务承载不了请求了，动态对服务进行扩容 六、风险感知1.资源监控监控服务器，容器jvm，mysql，redis等相关资源的监控信息 eg 服务器内存、网络、io，mysql读/写tps、慢sql、内存、容量、连接数、网络流量等～ 配上相关阈值报警，开发、运维能够及时知道相关资源问题 2.业务监控针对业务数据制作实时数据报表、T+1数据报表，每日关注核心业务报表（统计图、统计表），观察同环比，是否有异常，及时发现问题 3.安全监控运维团队针对网络攻击（eg：DDoS攻击）进行监控并进行防御及报警 4.拨测项目中提供类似于/health的接口，接口内无任何逻辑即可，拨测平台定时（eg：每分钟）调用后只需判定http状态码200即可基本断定服务还活着，没有hang住并且有节点在提供服务。 5.系统巡检资源类的有问题基本都会有相关报警，有些代码报错等等之类的也可以通过自定义报警实现，相对于来说比较不容易发现的是业务数据类的问题，可以通过系统巡检机制保证。 此处尽量是建立在业务监控报表已经完毕的基础上，每日安排值班同学查看业务监控报表，查看业务数据量是否处于不合理的区间或者是流量激增以及流量极具减少，及时发现并分析原因，同步业务同学。 6.钉钉业务报警代码中关键的节点，需要增加报警机制，可以使用公司内部的报警控件亦或钉钉报警，在线上出现一些问题的时候，能够自动发出报警来通知开发、运维同学来及时解决问题。 7.tps/tp999/max监控监控平台监控每一个接口的tps，tp99，tp999，max，根据tp999可以进行接口优化，max抖动可以进一进行调优，看是机器问题，抑或是gc的stw问题、网络问题、跨机房问题等等。 TP90就是满足90%的网络请求所需要的最低耗时(90%的请求耗时情况)。 TP99就是满足99%的网络请求所需要的最低耗时。TP99=10ms，标识这段时间99%的请求都在10毫秒以内。 TP999就是满足999‰的网络请求所需要的最低耗时。 MAX就是这段时间内耗时最大的，比如MAX=1000ms，表示这段时间最耗时的一次请求是1s，max高表示偶有一次请求，耗时很大。","categories":[{"name":"11.方法论","slug":"11-方法论","permalink":"https://zhangxin66666.github.io/categories/11-%E6%96%B9%E6%B3%95%E8%AE%BA/"}],"tags":[{"name":"方法论","slug":"方法论","permalink":"https://zhangxin66666.github.io/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"}]},{"title":"行页索引底层结构","slug":"7.mysql/行页索引底层结构","date":"2022-01-04T16:00:00.000Z","updated":"2024-08-22T03:23:14.709Z","comments":true,"path":"2022/01/05/7.mysql/行页索引底层结构/","link":"","permalink":"https://zhangxin66666.github.io/2022/01/05/7.mysql/%E8%A1%8C%E9%A1%B5%E7%B4%A2%E5%BC%95%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/","excerpt":"","text":"前言首先明确下什么是索引呢？假设我一张数据库的表存了10亿条数据，如果要查找出其中的10条数据，如果逐条遍历匹配的话，效率上肯定是无法容忍的。所以为了提高数据查询的效率，就需要对数据进行一些格式化的存储，来方便我们更快的查找，这就是索引。 索引其实是对数据按照某种格式进行存储的文件。就InnoDB来讲，索引文件里面会有很多的基本单元【页】，为什么要有页呢？ 如果我们在查询数据的时候直接交互磁盘，效率显然又会很慢，所以真正处理数据的过程其实是在内存中，这样就需要把磁盘的数据加载到内存，如果是写操作，可能还要将内存的数据再次刷新到磁盘。如果内存与磁盘的数据交互过程是基于一条条记录来进行的，显然又会很慢，所以InnoDB采取的方式是将数据划分为若干个页，以页来作为内存和磁盘交互的基本单位，默认大小为16KB。 换句话讲，其实就是内存一次拉到磁盘的数据最少是16KB，内存写入磁盘的数据一次最少也是16KB。 上面解释清楚了索引和页，MySQL的InnoDB存储引擎是基于页来进行内存和磁盘的数据交互的。所以我们的数据或者叫记录，其实是以【行】的格式存储在页里面的，可以简单的理解成页里面的一行对应一条记录。 当然索引文件里面肯定不光只有页，还会有其余的东西，页里面也不光只有行格式，也会有额外的信息，这个下面我们会详细分析，至此我们仅仅需要明确一下索引的概念和层级关系。 明确了这个层级关系之后，接下来我们来从最基础的行格式来进行分析。 一、行我们平时都是以记录为单位向表中插入数据的，这些记录在磁盘上的存储形式被称为行格式或者记录格式，截至目前，一共有4种行格式。分别是 compact redundant dynamic compressed，MySQL5.7默认的行格式为dynamic。 1. 如何指定行格式首先来看下，我们如何来指定行格式呢？ 123CREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称 比如我们创建一张表来指定行格式： 123456create table record_format( c1 varchar(10), c2 varchar(10) not null, c3 char(10), c4 varchar(10))charset=ascii row_format=compact; 2.compact 行格式一条完整的行格式可以被分为两个部分：记录额外信息的部分&amp;记录真实数据的部分。 2.1 额外的信息先来看额外的信息，其实包含三部分：变长字段的长度列表，NULL值列表和记录头信息。 2.1.1 变长字段长度列表MySQL支持很多的变长字段，我们就以最经典的varchar来进行举例，变长字段的数据存储多少字节其实是不固定的，所以在存储真实的数据的时候，要记录一下真实数据的字节数，这样的话，一个变长字段列实际上就占用了两部分的空间来存储：【真实数据】&amp;【真实数据占用字节数】 注意：对于一个列varchar(100)，我们实际上存储一个10字节的数据，当在内存中为这个列的数据分配内存空间的时候，实际上会分配100字节，但是这个列的数据在磁盘上，实际上只会分配10字节。 在Compact行格式中，会把所有的变长字段占用的真实长度全部逆序存储在记录的开头位置，形成一个变长字段长度列表。 比如我们刚才创建的那张表，我们来分析一下： c1,c2,c4三个列都是变长字段，所以这三个列的值的长度其实都需要保存到变长字段长度列表，因为这张表的字符集的ASCII，所以每个字符实际只占用1字节来进行编码： 列名 储存内容 内容长度(十进制表示) 内容长度(十六进制表示) C1 ‘aaaa’ 4 0x04 C2 ‘bbb’ 3 0x03 C4 ‘d’ 1 0x01 因为这些长度是按照逆序来存放的，所以最终变长字段长度列表的字节串用十六进制表示的效果就是【010304】。 因为我们演示的这条记录中，c1,c2,c4列中的字符串都比较短，所以真实的数据占用的字节数就比较小，真实数据的长度用一个字节就可以表示，但是如果变长列的内容占用字节数比较多，可能就需要用2个字节来表示。对此InnoDB的规定是： 【W】：某个字符集中表示一个字符最多需要使用的字节数 【M】：当前列类型最多能存储的字符数(比如varchar(100),M=100),如果换算成字节数就是W*M 【L】：真实占用的字节数 如果M*W&lt;=255,那么使用1字节来表示字符串实际用到的字节数。 InnoDB在读记录的变长字段长度列表的时候会先去查看表结构，判断用几个字节去存储的。 如果M*W&gt;=255,这个时候再次分为两种情况： 如果L&lt;=127，那就用1个字节表示 否则就用2个字节表示 如果某个变长字段允许存储的最大字节数大于255的时候，怎么区分他正在读取的字节是一个单独的字段长度还是半个字段长度呢？ InnoDB用该字节的第一个二进制为作为标志位，0：单独的字段长度，1：半个字段长度。 对于一些占用字节数特别多的字段，单个页都无法存储的时候，InnoDB会把一部分数据放到所谓的溢出页，在变长字段长度列表中只会记录当前页的字段长度，所以用两个字节也可以存的下。 此外，变长字段的长度列表中只存储真实数据值为非NULL的列占用的长度，真实数据为NULL的列的长度是不存储的。 也并不是所有的记录都会有变长字段长度列表，假如表中的列要是没有变长字段，或者记录中的变长字段值都是NULL，那就没有变长字段长度列表了。 2.1.2 NULL值列表如果一条记录有多个字段的真实值为NULL，不统一管理的话就会比较占用空间，所以抽取出来了NULL值列表。 当然如果这个表的所有字段都是NOT NULL约束的，就不会有NULL值列表。 看一下处理过程： 首先统计出表中允许存储NULL的字段 如果表中没有NULL字段的列，那就没必要再往下了，否则将每个允许存储NULL的列对应的一个二进制位按照列的顺序逆序排列。1：NULL，0：不是NULL。 MySQL规定NULL值必须用整数个字节的位表示，如果使用的二进制位个数不是整数个字节，则在字节的高位补0。 以此类推，如果一个表中有9个字段允许为NULL，那么这个记录的NULL值列表部分就需要2个字节来表示。 这个时候再来看我们上面创建的表中的记录。 2.1.3 记录头信息由五个固定的字节组成，换算成二进制就是40位，每一部分代表不同的信息。 名称 大小(bit) 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型 0 ：普通记录，1：B+树非页节点记录，2：最小记录，3：最大记录 next_record 16 下一条记录的相对位置 除了表中显式定义的列，MySQL会往我们的表中放一些隐藏列。 列名 是否必须 占用空间 描述 row_id 否 6字节 行ID，唯一标识一条记录 transaction_id 是 6字节 事务ID roll_pointer 是 7字节 回滚指针 【row_id】：这个玩意，跟主键的选择有关，如果我们显式定义了表的主键，就不会有它，如果我们没显式定义主键，那么会去选择一个unique的列作为主键，如果unique的列也没有，那么就会生成一个row_id列作为隐藏的主键。 【transaction_id】&amp;【roll_pointer】和一致性非锁读(MVCC)有关,后面遇到的时候我会在分析介绍。 在完善下我们开头创建的那张表的记录形象。 至此，其实就剩下我们显式插入数据库的真实记录了，但是还有一个特殊的类型需要说明一下。 2.2.1 CHAR 也是变长的？在Compact行格式下只会把变长类型的列的长度逆序记录到变长字段长度列表，但是这其实和我们的字符集有关系，上面我们创建的表显式指定为ASCII字符集，这个时候一个字符只会用一个字节表示，但是假如我们指定的是其它字符集，比如utf8，这个时候一个字符用几个字节表示就不确定了，所以CHAR列的真实字节长度也会被记录到变长字段长度列表。 另外，变长字符集的CHAR(M)类型的列要求至少占用M个字节，而VARCHAR(M)就没有这个要求。 对于使用utf8字符集的CHAR(10)的列来说，该列存储的数据字节长度的范围是10～30个字节。即使我们向该列中存储一个空字符串也会占用10个字节，这是怕将来更新该列的值的字节长度大于原有值的字节长度而小于10个字节时，可以在该记录处直接更新，而不是在存储空间中重新分配一个新的记录空间，导致原有的记录空间成为所谓的碎片。 3. 行溢出上面提到了，如果一条记录的真实字节数太大，就会导致行溢出，把超出的一部分数据存储到其他行或者页。 3.1 varchar(M)最多能存储的数据varchar(M)的列最多可以占用65535个字节。其中M代表该类型最多存储的字符数量。 实际上，MySQL对一条记录占用的最大存储空间是有限制的，除了BLOB，TEXT类型的列之外，其他所有的列(不包含隐藏列和记录头信息)占用的字节长度加起来不能超过65535个字节。这个65535个字节除了列本身的数据之外，还包括一些其他的数据，比如说我们为了存储一个varchar列，其实还需要占用3部分空间。 真实数据 真实数据占用的字节长度 NULL值标识，如果该列有NOT_NULL属性则可以没有这部分存储空间 如果该varchar类型的列没有NOT NULL属性那最多只能存储65532个字节的数据，因为真实数据的长度可能占用2个字节，NULL值标识需要占用1个字节。 如果VARCHAR类型的列有NOT NULL属性，那最多只能存储65533个字节的数据，因为真实数据的长度可能占用2个字节，不需要NULL值标识。 如果VARCHAR(M)类型的列使用的不是ascii字符集，那会怎么样呢？ 如果VARCHAR(M)类型的列使用的不是ascii字符集，那M的最大取值取决于该字符集表示一个字符最多需要的字节数。在列的值允许为NULL的情况下，gbk字符集表示一个字符最多需要2个字节，那在该字符集下，M的最大取值就是32766（也就是：65532/2），也就是说最多能存储32766个字符；utf8字符集表示一个字符最多需要3个字节，那在该字符集下，M的最大取值就是21844，就是说最多能存储21844（也就是：65532/3）个字符。 上述所言在列的值允许为NULL的情况下，gbk字符集下M的最大取值就是32766，utf8字符集下M的最大取值就是21844，这都是在表中只有一个字段的情况下说的，一定要记住一个行中的所有列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过65535个字节！ 3.2 记录中的数据太多产生溢出MySQL中磁盘和内存交互的基本单位是页，也就是说MySQL是以页为基本单位来管理存储空间的，我们的记录都会被分配到某个页中存储。而一个页的大小一般是16KB，也就是16384字节，而一个VARCHAR(M)类型的列就最多可以存储65532个字节，这样就可能造成一个页存放不了一条记录的尴尬情况。 在Compact和Redundant行格式中，对于占用存储空间非常大的列，在记录的真实数据处只会存储该列的一部分数据，把剩余的数据分散存储在几个其他的页中，然后记录的真实数据处用20个字节存储指向这些页的地址（当然这20个字节中还包括这些分散在其他页面中的数据的占用的字节数），从而可以找到剩余数据所在的页。 从图中可以看出来，对于Compact和Redundant行格式来说，如果某一列中的数据非常多的话，在本记录的真实数据处只会存储该列的前768个字节的数据和一个指向其他页的地址，然后把剩下的数据存放到其他页中，这个过程也叫做行溢出，存储超出768字节的那些页面也被称为溢出页。画一个简图就是这样： 不只是 VARCHAR(M)类型的列，其他的 TEXT、BLOB 类型的列在存储数据非常多的时候也会发生行溢出。 3.3 行溢出的临界点发生行溢出的临界点是什么呢？也就是说在列存储多少字节的数据时就会发生行溢出？ MySQL中规定一个页中至少存放两行记录，至于为什么这么规定我们之后再说，现在看一下这个规定造成的影响。我们往表中插入亮条记录，每条记录最少插入多少字节的数据才会行溢出呢？ 分析一下页空间是如何利用的 每个页除了存放我们的记录以外，也需要存储一些额外的信息，乱七八糟的额外信息加起来需要132个字节的空间（现在只要知道这个数字就好了），其他的空间都可以被用来存储记录。 每个记录需要的额外信息是27字节。这27个字节包括下边这些部分： 内容 大小(字节) 真实数据的长度 2 列是否是NULL值 1 头信息 5 row_id 6 transaction_id 6 roll_pointer 7 因为表中具体有多少列不确定，所以没法确定具体的临界点，只需要知道插入的字段数据长度很大就会导致行溢出的现象。 4.Dynamic &amp; Compressed 行格式这俩行格式和Compact行格式挺像，只不过在处理行溢出数据时有点儿分歧，它们不会在记录的真实数据处存储字段真实数据的前768个字节，而是把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址，就像这样： Compressed行格式和Dynamic不同的一点是，Compressed行格式会采用压缩算法对页面进行压缩，以节省空间。 至此，行格式就分析的差不多了，接下来我们来看页的存储结构。 二、页InnoDB为了不同的目的设计了许多种页，比如存放表空间头部信息的页，存放 Insert Buffer信息的页，存放Innode信息的页，存放undo日志信息的页等等。 本节分析存放表中记录的页，官方称为索引页，为了分析方便，我们暂且叫做数据页。 系统变量innodb_page_size表明了InnoDB存储引擎中的页大小，默认值是16384字节，也就是16kb。 该变量只能在第一次初始化MySQL数据目录时指定，之后就再也不能更改了。 数据页代表的这块16kb的存储空间被划分为多个部分，不同部分有不同的功能。 从图中可以看出，一个InnoDB数据页的存储空间大致被划分为了7个部分，有的部分占用的字节数是确定的，有的占用的字节数不是确定的。 名称 中文名 占用空间大小（字节） 简单描述 File Header 文件头部 38 页的一些通用信息 Page Header 页面头部 56 数据页专有的一些信息 Infifmum + Supremum 最小记录和最大记录 26 两个虚拟的行记录 User Records 用户记录 不确定 实际存储的行记录内容 Free Space 空闲空间 不确定 页中尚未使用的空间 Page Directory 页面目录 不确定 页中某些记录的相对位置 File Trailer 文件尾部 8 校验页是否完整 1. 记录在页中的存储我们先来创建一张表 1234567mysql&gt; create table page_demo( -&gt; c1 int , -&gt; c2 int , -&gt; c3 varchar(10000), -&gt; primary key(c1) -&gt; ) charset=ascii row_format=Compact;Query OK, 0 rows affected (0.03 sec) 因为我们指定了主键，所以存储实际数据的列里面不会有隐藏的row_id,我们来看一下他的行格式。 再次回顾下记录头中5个字节表示的数据。 名称 大小(bit) 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型 0 ：普通记录，1：B+树非页节点记录，2：最小记录，3：最大记录 next_record 16 下一条记录的相对位置 针对当前这个表的行格式简化图： 接下来我们往表中插入几条数据： 1INSERT INTO page_demo VALUES(1, 100, &#x27;aaaa&#x27;), (2, 200, &#x27;bbbb&#x27;), (3, 300, &#x27;cccc&#x27;), (4, 400, &#x27;dddd&#x27;); 为了分析这些记录在页的User Records 部分中是怎么表示的，把记录头信息和实际的列数据都用十进制表示出来了（其实是一堆二进制位），所以这些记录的示意图就是： 分析一下头信息中的每个属性是什么意思。 1.1 delete_mask标记当前记录是否被删除，占用1个二进制位，0：未删除，1：删除。 被删除的记录不会立即从磁盘上删除，因为删除他们之后吧其他的记录在磁盘上重新排列需要性能消耗，所以只是打一个删除标记，所有被删掉的数据会组成一个垃圾链表，在这个链表中的记录占用的空间成为可重用空间，之后如果有新的记录插入到表中，可能会把这些删除的记录覆盖掉。 将delete_mask 设置为1 和 将被删除的记录加入到垃圾链表中其实是两个阶段。 1.2 min_rec_maskB+树的每层非叶子节点中的最小记录都会添加该标记，如果这个字段的值是0，意味着不是B+树的非叶子节点中的最小记录。 1.3 n_owned1.4 heap_no这个属性表示当前记录在本页中的位置，我们插入的四条记录在本页中的位置分别是 2，3，4 ，5 。为什么不见 0 和 1 的记录呢？ 这是因为InnoDB自动给每个页里边加了两个记录，由于这两个记录并不是我们自己插入的，所以有时候也称为虚拟记录。这两个伪记录一个代表最小记录，一个代表最大记录。 记录是如何比较大小的？对于一条完整的记录来说，比较记录大小就是比较主键的大小。比方说我们插入的4行记录的主键值分别为1，2，3，4，这也就意味着这四条记录的大小从大到小递增。 但是不管我们往页中插入了多少自己的记录，InnoDB都规定他们定义的两条伪记录分别为最小记录和最大记录。这两条记录的构造十分简单，都是由5字节大小的记录头信息和8字节大小的一个固定的部分组成的。 由于这两条记录不是我们自己定义的记录，所以他们并不存放在页的User Records部分，他们被单独放在一个称为Infimum+Supremum的部分。 从图中我们可以看出来，最小记录和最大记录的heap_no值分别是0 和 1 ， 也就是说他们的位置最靠前。 1.5 record_type这个属性表示当前记录的类型。0：普通记录，1：B+树非叶子节点记录，2：最小记录，3：最大记录。 我们自己插入的记录是普通记录 0 ， 而最大记录和最小记录record_type 分别为 2 和 3。 1.6 next_record表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量。这其实是一条链表，可以通过一条记录找到他的下一条记录，但是下一条记录指的并不是按照我们插入顺序的下一条记录，而是按照主键值由小到大的顺序的下一条记录。而且规定 infimum记录 的下一条记录就是本页主键值最小的用户记录，而本页中主键最大的用户记录的下一条记录就是supremum记录。 如果从中删除一条记录，这个链表也是会跟着变化的，假如现在删除第二条记录 1delete from page_demo where c1 =2 ; 删除第二条记录以后： 发生的变化： 第二条记录并没有从存储空间中移除，而是把该记录的delete_mask设置为1 第二条记录的next_records值变成了0，意味着该记录没有下一条记录了 第一条记录的next record指向了第三条记录 最大记录的 n_owned 值从5 变成了4 所以，不论我们怎么对页中的记录做增删改查操作，InnoDB始终会维护一条记录的单链表，链表中各个节点是按照主键值由小到大的顺序连接起来的。 next_records 为啥要指向记录头信息和真实数据之间的位置呢？为啥不干脆指向整条记录的开头位置，也就是记录的额外信息开头的位置呢？ 因为这个位置刚刚好，向左读取就是记录头信息，向右读取就是真实数据。我们前边还说过变长字段长度列表，null值列表中的信息都是逆序存放的，这样可以使记录中位置靠前的字段和他们对应的字段长度信息在内存中的距离更近，可能会提高高速缓存的命中率。 因为主键值为2的记录已经被我们删除了，但是存储空间并没有回收，如果再次把这条记录插入到表中，会发生什么？ 1INSERT INTO page_demo VALUES(2, 200, &#x27;bbbb&#x27;); 从图中可以看到，InnoDB并没有因为新记录的插入而为他申请新的存储空间，而是直接复用了原来删除的记录的存储空间。 2. Page Directory（页目录）如果我们想根据主键值查找页中某条记录该咋办？ 1select * from page_demo where c1 = 3; 将所有正常的记录(包括两条隐藏记录但是不包括已经标记为删除的记录)划分为几组 每个组的最后一条记录（也就是组内最大的那条记录）的头信息中的n_owned属性表示该组拥有多少条记录 将每个组的最后一条记录的地址偏移量单独提取出来按照顺序存储到靠近页的尾部的地方，这个地方就是所谓的【Page Directory】,也就是页目录。页目录中的这些地址偏移量被称为槽，所以页目录就是由槽组成的 比方说刚才创建的表中正常的记录由6条，InnoDB会把他们分成两组，第一组中只有一条最小记录，第二组中是剩余的5条记录。 现在页目录部分中有两个槽，也就意味着我们的记录被分成了两个组，槽1中的值为112，代表最大记录的地址偏移量；槽0的值为99，代表最小记录的地址偏移量。 注意最大和最小记录的头信息的n_owned属性： 最小记录中的n_owned值为1，这就代表着以最小记录结尾的这个分组中只有1条记录，也就是最小记录本身 最大记录中的n_owned值为5，这就代表着以最大记录结尾的这个分组中只有5条记录，包括最大记录本身还有我们自己插入的4条记录 【99】&amp;【112】这样的地址偏移量很不直观，我们用箭头指向的方式替代数字。 InnoDB对每个分组中的记录条数是有规定的：对于最小记录所在的分组只能有 1 条记录，最大记录所在的分组拥有的记录条数只能在 1~8 条之间，剩下的分组中记录的条数范围只能在是 4~8 条之间。所以分组是按照下边的步骤进行的： 初始情况下一个数据页里只有最小记录和最大记录两条记录，它们分属于两个分组。 之后每插入一条记录，都会从页目录中找到主键值比本记录的主键值大并且差值最小的槽，然后把该槽对应的记录的n_owned值加1，表示本组内又添加了一条记录，直到该组中的记录数等于8个。 在一个组中的记录数等于8个后再插入一条记录时，会将组中的记录拆分成两个组，一个组中4条记录，另一个5条记录。这个过程会在页目录中新增一个槽来记录这个新增分组中最大的那条记录的偏移量。 由于现在page_demo表中的记录太少，无法演示添加了页目录之后加快查找速度的过程，所以再往page_demo表中添加一些记录： 1INSERT INTO page_demo VALUES(5, 500, &#x27;eeee&#x27;), (6, 600, &#x27;ffff&#x27;), (7, 700, &#x27;gggg&#x27;), (8, 800, &#x27;hhhh&#x27;), (9, 900, &#x27;iiii&#x27;), (10, 1000, &#x27;jjjj&#x27;), (11, 1100, &#x27;kkkk&#x27;), (12, 1200, &#x27;llll&#x27;), (13, 1300, &#x27;mmmm&#x27;), (14, 1400, &#x27;nnnn&#x27;), (15, 1500, &#x27;oooo&#x27;), (16, 1600, &#x27;pppp&#x27;); 现在看怎么从这个页目录中查找记录。因为各个槽代表的记录的主键值都是从小到大排序的，所以我们可以使用所谓的二分法来进行快速查找。5个槽的编号分别是：0、1、2、3、4，所以初始情况下最低的槽就是low=0，最高的槽就是high=4。比方说我们想找主键值为6的记录，过程是这样的： 计算中间槽的位置：(0+4)/2=2，所以查看槽2对应记录的主键值为8，又因为8 &gt; 6，所以设置high=2，low保持不变。 重新计算中间槽的位置：(0+2)/2=1，所以查看槽1对应的主键值为4，又因为4 &lt; 6，所以设置low=1，high保持不变。 因为high - low的值为1，所以确定主键值为6的记录在槽2对应的组中。此刻我们需要找到槽2中主键值最小的那条记录，然后沿着单向链表遍历槽2中的记录。但是我们前边又说过，每个槽对应的记录都是该组中主键值最大的记录，这里槽2对应的记录是主键值为8的记录，怎么定位一个组中最小的记录呢？别忘了各个槽都是挨着的，我们可以很轻易的拿到槽1对应的记录（主键值为4），该条记录的下一条记录就是槽2中主键值最小的记录，该记录的主键值为5。所以我们可以从这条主键值为5的记录出发，遍历槽2中的各条记录，直到找到主键值为6的那条记录即可。由于一个组中包含的记录条数只能是1~8条，所以遍历一个组中的记录的代价是很小的。 所以在一个数据页中查找指定主键值的记录的过程分为两步： 通过二分法确定该记录所在的槽，并找到该槽所在分组中主键值最小的那条记录。 通过记录的next_record属性遍历该槽所在的组中的各个记录。 3.Page Header（页面头部）为了能得到一个数据页中存储的记录的状态信息，比如本页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等等，特意在页中定义了一个叫Page Header的部分，它是页结构的第二部分，这个部分占用固定的56个字节，专门存储各种状态信息。 名称 占用空间大小（字节） 描述 PAGE_N_DIR_SLOTS 2 在页目录中的槽数量 PAGE_HEAP_TOP 2 还未使用的空间最小地址，也就是说从该地址之后就是Free Space PAGE_N_HEAP 2 本页中的记录的数量（包括最小和最大记录以及标记为删除的记录） PAGE_FREE 2 第一个已经标记为删除的记录地址（各个已删除的记录通过next_record也会组成一个单链表，这个单链表中的记录可以被重新利用） PAGE_GARBAGE 2 已删除记录占用的字节数 PAGE_LAST_INSERT 2 最后插入记录的位置 PAGE_DIRECTION 2 记录插入的方向 PAGE_N_DIRECTION 2 一个方向连续插入的记录数量 PAGE_N_RECS 2 该页中记录的数量（不包括最小和最大记录以及被标记为删除的记录） PAGE_MAX_TRX_ID 8 修改当前页的最大事务ID，该值仅在二级索引中定义 PAGE_LEVEL 2 当前页在B+树中所处的层级 PAGE_INDEX_ID 8 索引ID，表示当前页属于哪个索引 PAGE_BTR_SEG_LEAF 10 B+树叶子段的头部信息，仅在B+树的Root页定义 PAGE_BTR_SEG_TOP 10 B+树非叶子段的头部信息，仅在B+树的Root页定义 4.File Header（文件头部）File Header针对各种类型的页都通用，也就是说不同类型的页都会以File Header作为第一个组成部分，它描述了一些针对各种页都通用的一些信息，比方说这个页的编号是多少，它的上一个页、下一个页是谁，这个部分占用固定的38个字节。 名称 占用空间大小（字节） 描述 FIL_PAGE_SPACE_OR_CHKSUM 4 页的校验和（checksum值） FIL_PAGE_OFFSET 4 页号 FIL_PAGE_PREV 4 上一个页的页号 FIL_PAGE_NEXT 4 下一个页的页号 FIL_PAGE_LSN 8 页面被最后修改时对应的日志序列位置（英文名是：Log Sequence Number） FIL_PAGE_TYPE 2 该页的类型 FIL_PAGE_FILE_FLUSH_LSN 8 仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值 FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4 页属于哪个表空间 InnoDB为了不同的目的而把页分为不同的类型，我们上边介绍的其实都是存储记录的数据页，其实还有很多别的类型的页，具体如下表： 类型名称 十六进制 描述 FIL_PAGE_TYPE_ALLOCATED 0x0000 最新分配，还没使用 FIL_PAGE_UNDO_LOG 0x0002 Undo日志页 FIL_PAGE_INODE 0x0003 段信息节点 FIL_PAGE_IBUF_FREE_LIST 0x0004 Insert Buffer空闲列表 FIL_PAGE_IBUF_BITMAP 0x0005 Insert Buffer位图 FIL_PAGE_TYPE_SYS 0x0006 系统页 FIL_PAGE_TYPE_TRX_SYS 0x0007 事务系统数据 FIL_PAGE_TYPE_FSP_HDR 0x0008 表空间头部信息 FIL_PAGE_TYPE_XDES 0x0009 扩展描述页 FIL_PAGE_TYPE_BLOB 0x000A 溢出页 FIL_PAGE_INDEX 0x45BF 索引页，也就是我们所说的数据页 我们存放记录的数据页的类型其实是FIL_PAGE_INDEX，也就是所谓的索引页。 有时候我们存放某种类型的数据占用的空间非常大（比方说一张表中可以有成千上万条记录），InnoDB可能不可以一次性为这么多数据分配一个非常大的存储空间，如果分散到多个不连续的页中存储的话需要把这些页关联起来，FIL_PAGE_PREV和FIL_PAGE_NEXT就分别代表本页的上一个和下一个页的页号。这样通过建立一个双向链表把许许多多的页就都串联起来了，而无需这些页在物理上真正连着。需要注意的是，并不是所有类型的页都有上一个和下一个页的属性，不过我们现在分析的数据页（也就是类型为FIL_PAGE_INDEX的页）是有这两个属性的，所以所有的数据页其实是一个双链表。 5.File Trailer(文件尾部)如果页中的数据在内存中被修改了，那么在修改后的某个时间需要把数据同步到磁盘中。但是在同步了一半的时候中断电了咋办？ 为了检测一个页是否完整，在每个页的尾部都加了一个File Trailer部分，这个部分由8个字节组成，可以分成2个小部分： 前四个字节代表校验和 后四个字节代表页面被最后修改时对应的日志序列位置 这个File Trailer &amp; File Header 类似，都是所有类型的页通用的。 至此，整个数据页的结构我们也基本上分析完了，现在在回头看一下开头我们那张恐怖的图，是不是感觉清晰很多了呢？接下来，我们来分析索引的结构。 三、索引1.索引结构1.1 聚簇索引上边介绍的B+树本身就是一个目录，或者说本身就是一个索引。它有两个特点： 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表。 B+树的叶子节点存储的是完整的用户记录。 所谓完整的用户记录，就是指这个记录中存储了所有列的值（包括隐藏列）。 我们把具有这两种特性的B+树称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这种聚簇索引并不需要我们在MySQL语句中显式的使用INDEX语句去创建，InnoDB存储引擎会自动的为我们创建聚簇索引。另外，在InnoDB存储引擎中，聚簇索引就是数据的存储方式（所有的用户记录都存储在了叶子节点），也就是所谓的索引即数据，数据即索引。 1.2 二级索引聚簇索引只能在搜索条件是主键值时才能发挥作用，因为B+树中的数据都是按照主键进行排序的。那如果我们想以别的列作为搜索条件怎么办？ 我们可以多建几棵B+树，不同的B+树中的数据采用不同的排序规则。比方说我们用c2列的大小作为数据页、页中记录的排序规则，再建一棵B+树，效果如下图所示： 这个B+树与上边介绍的聚簇索引有几处不同： 使用记录c2列的大小进行记录和页的排序，这包括三个方面的含义： a页内的记录是按照c2列的大小顺序排成一个单向链表。 b各个存放用户记录的页也是根据页中记录的c2列大小顺序排成一个双向链表。 c存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的c2列大小顺序排成一个双向链表。 B+树的叶子节点存储的并不是完整的用户记录，而只是c2列+主键这两个列的值。 目录项记录中不再是主键+页号的搭配，而变成了c2列+页号的搭配。 所以如果我们现在想通过c2列的值查找某些记录的话就可以使用我们刚刚建好的这个B+树了。以查找c2列的值为4的记录为例，查找过程如下： 1确定目录项记录页 根据根页面，也就是页44，可以快速定位到目录项记录所在的页为页42（因为2 &lt; 4 &lt; 9）。 2通过目录项记录页确定用户记录真实所在的页。 在页42中可以快速定位到实际存储用户记录的页，但是由于c2列并没有唯一性约束，所以c2列值为4的记录可能分布在多个数据页中，又因为2 &lt; 4 ≤ 4，所以确定实际存储用户记录的页在页34和页35中。 3在真实存储用户记录的页中定位到具体的记录. 到页34和页35中定位到具体的记录。 4但是这个B+树的叶子节点中的记录只存储了c2和c1（也就是主键）两个列，所以我们必须再根据主键值去聚簇索引中再查找一遍完整的用户记录。 我们根据这个以c2列大小排序的B+树只能确定我们要查找记录的主键值，所以如果我们想根据c2列的值查找到完整的用户记录的话，仍然需要到聚簇索引中再查一遍，这个过程也被称为回表。也就是根据c2列的值查询一条完整的用户记录需要使用到2棵B+树！！！ 为什么我们还需要一次回表操作呢？直接把完整的用户记录放到叶子节点不就好了么？ 如果把完整的用户记录放到叶子节点是可以不用回表，相当于每建立一棵B+树都需要把所有的用户记录再都拷贝一遍，这就有点太浪费存储空间了。因为这种按照非主键列建立的B+树需要一次回表操作才可以定位到完整的用户记录，所以这种B+树也被称为二级索引（英文名secondary index），或者辅助索引。由于我们使用的是c2列的大小作为B+树的排序规则，所以我们也称这个B+树为为c2列建立的索引。 假设我们的查询结果是十条，那就是要进行10次回表，那这样的话，效率不是又慢了？ 在MySQL5.6对这种情况进行了优化，如果发现查询结果会导致多次回表，那么就会进行IO合并，拿到所有的主键再去进行回表。 1.3 联合索引我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+树按照c2和c3列的大小进行排序，这个包含两层含义： ●先把各个记录和页按照c2列进行排序。 ●在记录的c2列相同的情况下，采用c3列进行排序 为c2和c3列建立的索引的示意图如下： 四、总结：一次通过聚簇索引定位数据的过程大家熟悉的三个版本索引结构图如下 普通索引 二级索引 组合索引索引 相对完整些的三种索引结构见（三、索引） 下图为聚簇索引完整版结构 一次按照主键id查询id=5的数据的全流程 1.确定目录项记录页，找到页50 2.二分查找确定所属槽位1，然后遍历槽位1里的链表得知下次该继续查询页55 3.二分查找确定所属槽位1，然后遍历槽位1里的链表得知下次该继续查询页61 4.二分查找确定所属槽位2，然后遍历槽位2里的链表得到id为4的数据","categories":[{"name":"7.mysql","slug":"7-mysql","permalink":"https://zhangxin66666.github.io/categories/7-mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"}]},{"title":"mysql三大日志","slug":"7.mysql/mysql三大日志","date":"2021-12-26T16:00:00.000Z","updated":"2024-08-22T03:23:14.706Z","comments":true,"path":"2021/12/27/7.mysql/mysql三大日志/","link":"","permalink":"https://zhangxin66666.github.io/2021/12/27/7.mysql/mysql%E4%B8%89%E5%A4%A7%E6%97%A5%E5%BF%97/","excerpt":"","text":"日志是 mysql 数据库的重要组成部分，记录着数据库运行期间各种状态信息。mysql日志主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。 作为开发，我们重点需要关注的是二进制日志( binlog )和事务日志(包括redo log 和 undo log )，本文接下来会详细介绍这三种日志。 binlogbinlog 用于记录数据库执行的写入性操作(不包括查询)信息，以二进制的形式保存在磁盘中。binlog 是 mysql的逻辑日志，并且由 Server 层进行记录，使用任何存储引擎的 mysql 数据库都会记录 binlog 日志。 逻辑日志：可以简单理解为记录的就是sql语句 。 物理日志：mysql 数据最终是保存在数据页中的，物理日志记录的就是数据页变更 。 binlog 是通过追加的方式进行写入的，可以通过max_binlog_size 参数设置每个 binlog文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志。 binlog使用场景在实际应用中， binlog 的主要使用场景有两个，分别是 主从复制 和 数据恢复 。 主从复制 ：在 Master 端开启 binlog ，然后将 binlog发送到各个 Slave 端， Slave 端重放 binlog 从而达到主从数据一致。 数据恢复 ：通过使用 mysqlbinlog 工具来恢复数据。 binlog刷盘时机对于 InnoDB 存储引擎而言，只有在事务提交时才会记录biglog ，此时记录还在内存中，那么 biglog是什么时候刷到磁盘中的呢？ mysql 通过 sync_binlog 参数控制 biglog 的刷盘时机，取值范围是 0-N： 0：不去强制要求，由系统自行判断何时写入磁盘； 1：每次 commit 的时候都要将 binlog 写入磁盘； N：每N个事务，才会将 binlog 写入磁盘。 从上面可以看出， sync_binlog 最安全的是设置是 1 ，这也是MySQL 5.7.7之后版本的默认值。但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。 binlog日志格式binlog 日志有三种格式，分别为 STATMENT 、 ROW 和 MIXED。 在 MySQL 5.7.7 之前，默认的格式是 STATEMENT ， MySQL 5.7.7 之后，默认值是 ROW。日志格式通过 binlog-format 指定。 STATMENT：基于SQL 语句的复制( statement-based replication, SBR )，每一条会修改数据的sql语句会记录到binlog 中 。 优点：不需要记录每一行的变化，减少了 binlog 日志量，节约了 IO , 从而提高了性能； 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate() 、 slepp() 等 。 ROW：基于行的复制(row-based replication, RBR )，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了 。 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题 ； 缺点：会产生大量的日志，尤其是alter table 的时候会让日志暴涨 MIXED：基于STATMENT 和 ROW 两种模式的混合复制(mixed-based replication, MBR )，一般的复制使用STATEMENT 模式保存 binlog ，对于 STATEMENT 模式无法复制的操作使用 ROW 模式保存 binlog redo log为什么需要redo log我们都知道，事务的四大特性里面有一个是 持久性 ，具体来说就是只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态 。 那么 mysql是如何保证一致性的呢？ 最简单的做法是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。但是这么做会有严重的性能问题，主要体现在两个方面： 因为 Innodb 是以 页 为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！ 一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差！ 因此 mysql 设计了 redo log ， 具体来说就是只记录事务对数据页做了哪些****修改，这样就能完美地解决性能问题了(相对而言文件更小并且是顺序IO)。 redo log基本概念redo log 包括两部分：一个是内存中的日志缓冲( redo log buffer )，另一个是磁盘上的日志文件( redo logfile)。 mysql 每执行一条 DML 语句，先将记录写入 redo log buffer，后续某个时间点再一次性将多个操作记录写到 redo log file。这种 先写日志，再写磁盘 的技术就是 MySQL里经常说到的 WAL(Write-Ahead Logging) 技术。 在计算机操作系统中，用户空间( user space )下的缓冲区数据一般情况下是无法直接写入磁盘的，中间必须经过操作系统内核空间( kernel space )缓冲区( OS Buffer )。 因此， redo log buffer 写入 redo logfile 实际上是先写入 OS Buffer ，然后再通过系统调用 fsync() 将其刷到 redo log file中，过程如下： mysql 支持三种将 redo log buffer 写入 redo log file 的时机，可以通过 innodb_flush_log_at_trx_commit 参数配置，各参数值含义如下： redo log记录形式前面说过， redo log 实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此 redo log实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。 如下图： 同时我们很容易得知， 在innodb中，既有redo log 需要刷盘，还有 数据页 也需要刷盘， redo log存在的意义主要就是降低对 数据页 刷盘的要求。 在上图中， write pos 表示 redo log 当前记录的 LSN (逻辑序列号)位置， check point 表示 数据页更改记录 刷盘后对应 redo log 所处的 LSN(逻辑序列号)位置。 write pos 到 check point 之间的部分是 redo log 空着的部分，用于记录新的记录；check point 到 write pos 之间是 redo log 待落盘的数据页更改记录。当 write pos追上check point 时，会先推动 check point 向前移动，空出位置再记录新的日志。 启动 innodb 的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。因为 redo log记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志(如 binlog )要快很多。 重启innodb 时，首先会检查磁盘中数据页的 LSN ，如果数据页的LSN 小于日志中的 LSN ，则会从 checkpoint 开始恢复。 还有一种情况，在宕机前正处于checkpoint 的刷盘过程，且数据页的刷盘进度超过了日志页的刷盘进度，此时会出现数据页中记录的 LSN 大于日志中的 LSN，这时超出日志进度的部分将不会重做，因为这本身就表示已经做过的事情，无需再重做。 redo log与binlog区别 由 binlog 和 redo log 的区别可知：binlog 日志只用于归档，只依靠 binlog 是没有 crash-safe 能力的。 但只有 redo log 也不行，因为 redo log 是 InnoDB特有的，且日志上的记录落盘后会被覆盖掉。因此需要 binlog和 redo log二者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失。 undo log数据库事务四大特性中有一个是 原子性 ，具体来说就是 原子性是指对数据库的一系列操作，要么全部成功，要么全部失败，不可能出现部分成功的情况。 实际上， 原子性 底层就是通过 undo log 实现的。undo log主要记录了数据的逻辑变化，比如一条 INSERT 语句，对应一条DELETE 的 undo log ，对于每个 UPDATE 语句，对应一条相反的 UPDATE 的 undo log ，这样在发生错误时，就能回滚到事务之前的数据状态。 同时， undo log 也是 MVCC(多版本并发控制)实现的关键。 Undo log是InnoDB MVCC事务特性的重要组成部分。当我们对记录做了变更操作时就会产生Undo记录，Undo记录默认被记录到系统表空间(ibdata)中，但从5.6开始，也可以使用独立的Undo 表空间。 Undo记录中存储的是老版本数据，当一个旧的事务需要读取数据时，为了能读取到老版本的数据，需要顺着undo链找到满足其可见性的记录。当版本链很长时，通常可以认为这是个比较耗时的操作。 Undo Log 格式 在InnoDB引擎中，undo log分为： insert undo log： insert undo log是指在insert操作中产生的undo log，因为insert操作的记录，只对事务本身可见，对其他事务不可见（这是事务隔离性的要求），故该undo log可以在事务提交后直接删除，不需要进行purge操作。 update undo log： update undo log记录的是delete和update操作产生的undo log。该undo log可能需要提供MVCC机制，因此不能在事务提交时就进行删除，提交时放入undo log链表，等待purge线程进行最后的删除。下面是两种undo log的结构图。","categories":[{"name":"7.mysql","slug":"7-mysql","permalink":"https://zhangxin66666.github.io/categories/7-mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"}]},{"title":"redolog","slug":"7.mysql/redo日志","date":"2021-12-26T16:00:00.000Z","updated":"2024-08-22T03:23:14.707Z","comments":true,"path":"2021/12/27/7.mysql/redo日志/","link":"","permalink":"https://zhangxin66666.github.io/2021/12/27/7.mysql/redo%E6%97%A5%E5%BF%97/","excerpt":"","text":"一，什么是redo日志InnoDB存储引擎是以页为单位来管理存储空间的，我们的CRUD操作其实都是在访问页面。在真正访问页面之前，需要把磁盘中的页加载到内存的BufferPool，之后才能访问，但是因为事务要保持持久性，如果我们仅仅在内存的缓冲池修改了页面，假设事务提交后突然发生故障，导致内存的数据都消失了，那么这个已经提交的事务在数据库做的更改就丢失了。 如何保证持久性呢？可以在事务提交完成之前，把事务修改的所有页面都刷新到磁盘。不过这样做存在一些问题： 刷新一个完整的数据页过于浪费 随机IO效率比较低 事实上仅仅是为了保证事务的持久性，没有必要每次提交事务的时候就把该事务在内存修改过的全部页面刷新到磁盘，只需要把修改的内容记录一下就好，这样在事务提交的时候，就会把这个记录刷新到磁盘。即使系统因为崩溃而重启只需要按照记录的内容重新更新数据页即可恢复数据，上述记录修改的内容就叫做重做日志（redo log）。 相比于在事务提交的时候将所有修改过的内存中的页面刷新到磁盘，重做日志有以下好处： redo日志占用空间小：在存储表空间ID，页号，偏移量以及需要更新的值时，需要的存储空间很小。 redo日志是顺序写入磁盘的：在执行事务过程中，每执行一条语句，就可能产生若干条redo日志，这些日志是按照产生的顺序写入磁盘的，也就是使用顺序IO。 二，redo日志格式重做日志本质上仅仅是记录了一下事务对数据库进行了哪些修改。针对事务对数据库的不同修改场景MySQL定义了很多种重做日志，但是大部分类型的重做日志都有以下的通用结构。 type 重做日志的类型 space ID 表空间ID page number 页号 Data 日志的具体内容 1. 简单的redo日志类型行格式里面有一个隐藏列叫做row_id。为row_id进行赋值的方式如下： 服务器会在内存中维护一个全局变量，每当像某个包含row_id隐藏列的表插入一条记录的时候，就会把这个全局变量的值当做新记录row_id的值，并且把这个全局变量自增1。 每当这个全局变量的值是256的整数倍的时候，就会把这个变量的值刷新到系统表空间页号为7的页面中一个叫做Max Row ID的属性中。 当系统启动的时候，会将Max Row ID属性加载到内存，并把这个值加上256之后赋值给前面提到的全局变量。 这个Max Row ID占用的存储空间是8字节。当某个事务向某个包含row_id的表插入一条记录并且该记录分配的row_id值为256的整数倍的时候，就会像系统表空间页号为7的页面的相应偏移量处写入8字节的值。但是这个写入操作实际上是在内存缓冲区完成的，我们需要把这次修改以redo日志的形式记录下来，这样在事务提交之后，即使系统崩溃，也可以将该页面恢复成崩溃前的状态。在这种对页面的修改特别简单的时候，重做日志仅仅需要记录一下在某个页面的某个偏移量处修改了几个字节的值，具体修改后的内容是什么就可以了。这也叫做物理日志。 offset表示页面中的偏移量。如果写入的是字节序列类型的重做日志，还需要有一个len属性记录实际写入的长度。 2.复杂的redo日志类型有时候执行一条语句会修改非常多的页面，包括系统数据页面和用户数据页面（用户数据指的就是聚簇索引和二级索引对应的B+树） 这时我们如果使用简单的物理redo日志来记录这些修改时，可以有两种解决方案： 方案一：在每个修改的地方都记录一条redo日志。也就是有多少个修改的记录，就写多少条物理redo日志。这样子记录redo日志的缺点是显而易见的，因为被修改的地方是在太多了，可能记录的redo日志占用的空间都比整个页面占用的空间都多。 方案二：将整个页面的第一个被修改的字节到最后一个修改的字节之间所有的数据当成是一条物理redo日志中的具体数据。第一个被修改的字节到最后一个修改的字节之间仍然有许多没有修改过的数据，我们把这些没有修改的数据也加入到redo日志中太浪费了。 正因为上述两种使用物理redo日志的方式来记录某个页面中做了哪些修改比较浪费，InnoDB提出了一些新的redo日志类型。 这些类型的redo日志既包含物理层面的意思，也包含逻辑层面的意思，具体指： 物理层面看，这些日志都指明了对哪个表空间的哪个页进行了修改。 逻辑层面看，在系统崩溃重启时，并不能直接根据这些日志里的记载，将页面内的某个偏移量处恢复成某个数据，而是需要调用一些事先准备好的函数，执行完这些函数后才可以将页面恢复成系统崩溃前的样子。 这个类型为MLOG_COMP_REC_INSERT的redo日志并没有记录PAGE_N_DIR_SLOTS的值修改为了什么，PAGE_HEAP_TOP的值修改为了什么，PAGE_N_HEAP的值修改为了什么等等这些信息，而只是把在本页面中插入一条记录所有必备的要素记了下来，之后系统崩溃重启时，服务器会调用相关向某个页面插入一条记录的那个函数，而redo日志中的那些数据就可以被当成是调用这个函数所需的参数，在调用完该函数后，页面中的PAGE_N_DIR_SLOTS、PAGE_HEAP_TOP、PAGE_N_HEAP等等的值也就都被恢复到系统崩溃前的样子了。这就是所谓的逻辑日志的意思。 日志格式说了一堆核心其实就是：重做日志会把事务执行过程中对数据库所做的所有修改都记录下来，在之后系统因为崩溃而重启后可以把事务所做的任何修改都恢复过来。 为了节省重做日志占用的空间大小，InnoDB还对重做日志中的某些数据进行了压缩处理，比如表空间ID&amp;page number 一般占用4字节来存储，但是经过压缩之后占用的空间就更小了。 三，Mini-Transcation1.以组的形式写入redo日志语句在执行过程中可能修改若干个页面。比如我们前边说的一条INSERT语句可能修改系统表空间页号为7的页面的Max Row ID属性（当然也可能更新别的系统页面，只不过我们没有都列举出来而已），还会更新聚簇索引和二级索引对应B+树中的页面。由于对这些页面的更改都发生在Buffer Pool中，所以在修改完页面之后，需要记录一下相应的redo日志。在执行语句的过程中产生的redo日志被InnoDB人为的划分成了若干个不可分割的组，比如： 更新Max Row ID属性时产生的redo日志是不可分割的。 向聚簇索引对应B+树的页面中插入一条记录时产生的redo日志是不可分割的。 向某个二级索引对应B+树的页面中插入一条记录时产生的redo日志是不可分割的。 还有其他的一些对页面的访问操作时产生的redo日志是不可分割的。。。 怎么理解这个不可分割的意思呢？我们以向某个索引对应的B+树插入一条记录为例，在向B+树中插入这条记录之前，需要先定位到这条记录应该被插入到哪个叶子节点代表的数据页中，定位到具体的数据页之后，有两种可能的情况： 情况一：该数据页的剩余的空闲空间充足，足够容纳这一条待插入记录，那么事情很简单，直接把记录插入到这个数据页中，记录一条类型为MLOG_COMP_REC_INSERT的redo日志就好了，我们把这种情况称之为乐观插入。假如某个索引对应的B+树长这样：现在我们要插入一条键值为10的记录，很显然需要被插入到页b中，由于页b现在有足够的空间容纳一条记录，所以直接将该记录插入到页b中就好了，就像这样： 情况二：该数据页剩余的空闲空间不足，那么事情就悲剧了，我们前边说过，遇到这种情况要进行所谓的页分裂操作，也就是新建一个叶子节点，然后把原先数据页中的一部分记录复制到这个新的数据页中，然后再把记录插入进去，把这个叶子节点插入到叶子节点链表中，最后还要在内节点中添加一条目录项记录指向这个新创建的页面。很显然，这个过程要对多个页面进行修改，也就意味着会产生多条redo日志，我们把这种情况称之为悲观插入。假如某个索引对应的B+树长这样：现在我们要插入一条键值为10的记录，很显然需要被插入到页b中，但是从图中也可以看出来，此时页b已经塞满了记录，没有更多的空闲空间来容纳这条新记录了，所以我们需要进行页面的分裂操作，就像这样：如果作为内节点的页a的剩余空闲空间也不足以容纳增加一条目录项记录，那需要继续做内节点页a的分裂操作，也就意味着会修改更多的页面，从而产生更多的redo日志。另外，对于悲观插入来说，由于需要新申请数据页，还需要改动一些系统页面，比方说要修改各种段、区的统计信息信息，各种链表的统计信息（比如什么FREE链表、FSP_FREE_FRAG链表等，我们在介绍表空间那一篇中介绍过的各种东西)，反正总共需要记录的redo日志有二、三十条。 其实不光是悲观插入一条记录会生成许多条redo日志，InnoDB为了其他的一些功能，在乐观插入时也可能产生多条redo日志。 InnoDB认为向某个索引对应的B+树中插入一条记录的这个过程必须是原子的，不能说插了一半之后就停止了。比方说在悲观插入过程中，新的页面已经分配好了，数据也复制过去了，新的记录也插入到页面中了，可是没有向内节点中插入一条目录项记录，这个插入过程就是不完整的，这样会形成一棵不正确的B+树。我们知道redo日志是为了在系统崩溃重启时恢复崩溃前的状态，如果在悲观插入的过程中只记录了一部分redo日志，那么在系统崩溃重启时会将索引对应的B+树恢复成一种不正确的状态，这是InnoDB所不能忍受的。所以他们规定在执行这些需要保证原子性的操作时必须以组的形式来记录的redo日志，在进行系统崩溃重启恢复时，针对某个组中的redo日志，要么把全部的日志都恢复掉，要么一条也不恢复。怎么做到的呢？这得分情况讨论： 有的需要保证原子性的操作会生成多条redo日志，比如向某个索引对应的B+树中进行一次悲观插入就需要生成许多条redo日志。如何把这些redo日志划分到一个组里边儿呢？InnoDB做了一个很简单的操作，就是在该组中的最后一条redo日志后边加上一条特殊类型的redo日志，该类型名称为MLOG_MULTI_REC_END，type字段对应的十进制数字为31，该类型的redo日志结构很简单，只有一个type字段：所以某个需要保证原子性的操作产生的一系列redo日志必须要以一个类型为MLOG_MULTI_REC_END结尾，就像这样：这样在系统崩溃重启进行恢复时，只有当解析到类型为MLOG_MULTI_REC_END的redo日志，才认为解析到了一组完整的redo日志，才会进行恢复。否则的话直接放弃前边解析到的redo日志。 有的需要保证原子性的操作只生成一条redo日志，比如更新Max Row ID属性的操作就只会生成一条redo日志。其实在一条日志后边跟一个类型为MLOG_MULTI_REC_END的redo日志也是可以的，InnoDB不想浪费一个比特位。虽然redo日志的类型比较多，但撑死了也就是几十种，是小于127这个数字的，也就是说我们用7个比特位就足以包括所有的redo日志类型，而type字段其实是占用1个字节的，也就是说我们可以省出来一个比特位用来表示该需要保证原子性的操作只产生单一的一条redo日志，示意图如下：如果type字段的第一个比特位为1，代表该需要保证原子性的操作只产生了单一的一条redo日志，否则表示该需要保证原子性的操作产生了一系列的redo日志。 2.Mini-TransactionMySQL把对底层页面中的一次原子访问的过程称之为一个Mini-Transaction，简称mtr，比如上边所说的修改一次Max Row ID的值算是一个Mini-Transaction，向某个索引对应的B+树中插入一条记录的过程也算是一个Mini-Transaction。一个所谓的mtr可以包含一组redo日志，在进行崩溃恢复时这一组redo日志作为一个不可分割的整体。 一个事务可以包含若干条语句，每一条语句其实是由若干个mtr组成，每一个mtr又可以包含若干条redo日志，画个图表示它们的关系就是这样： 四，redo日志的写入过程1.redo log blockInnoDB为了更好的进行系统崩溃恢复，他们把通过mtr生成的redo日志都放在了大小为512字节的页中。为了和表空间中的页做区别，我们这里把用来存储redo日志的页称为block。一个redo log block的示意图如下： 真正的redo日志都是存储到占用496字节大小的log block body中，图中的log block header和log block trailer存储的是一些管理信息。 其中log block header的几个属性的意思分别如下： LOG_BLOCK_HDR_NO：每一个block都有一个大于0的唯一标号，本属性就表示该标号值。 LOG_BLOCK_HDR_DATA_LEN：表示block中已经使用了多少字节，初始值为12（因为log block body从第12个字节处开始）。随着往block中写入的redo日志越来也多，本属性值也跟着增长。如果log block body已经被全部写满，那么本属性的值被设置为512。 LOG_BLOCK_FIRST_REC_GROUP：一条redo日志也可以称之为一条redo日志记录（redo log record），一个mtr会生产多条redo日志记录，这些redo日志记录被称之为一个redo日志记录组（redo log record group）。LOG_BLOCK_FIRST_REC_GROUP就代表该block中第一个mtr生成的redo日志记录组的偏移量（其实也就是这个block里第一个mtr生成的第一条redo日志的偏移量）。 LOG_BLOCK_CHECKPOINT_NO：表示所谓的checkpoint的序号，checkpoint是我们后续内容的重点，现在先不用清楚它的意思，稍安勿躁。 log block trailer中属性的意思如下： LOG_BLOCK_CHECKSUM：表示block的校验值，用于正确性校验，我们暂时不关心它。 2.redo 日志缓冲区InnoDB为了解决磁盘速度过慢的问题而引入了Buffer Pool。同理，写入redo日志时也不能直接直接写到磁盘上，实际上在服务器启动时就向操作系统申请了一大片称之为redo log buffer的连续内存空间，翻译成中文就是redo日志缓冲区，也可以简称为log buffer。这片内存空间被划分成若干个连续的redo log block，就像这样： 我们可以通过启动参数innodb_log_buffer_size来指定log buffer的大小，在MySQL 5.7.21这个版本中，该启动参数的默认值为16MB。 3.redo log 日志写入log buffer向log buffer中写入redo日志的过程是顺序的，也就是先往前边的block中写，当该block的空闲空间用完之后再往下一个block中写。当我们想往log buffer中写入redo日志时，第一个遇到的问题就是应该写在哪个block的哪个偏移量处，所以InnoDB特意提供了一个称之为buf_free的全局变量，该变量指明后续写入的redo日志应该写入到log buffer中的哪个位置，如图所示： 一个mtr执行过程中可能产生若干条redo日志，这些redo日志是一个不可分割的组，所以其实并不是每生成一条redo日志，就将其插入到log buffer中，而是每个mtr运行过程中产生的日志先暂时存到一个地方，当该mtr结束的时候，将过程中产生的一组redo日志再全部复制到log buffer中。我们现在假设有两个名为T1、T2的事务，每个事务都包含2个mtr，我们给这几个mtr命名一下： 事务T1的两个mtr分别称为mtr_T1_1和mtr_T1_2。 事务T2的两个mtr分别称为mtr_T2_1和mtr_T2_2。 每个mtr都会产生一组redo日志，不同的事务可能是并发执行的，所以T1、T2之间的mtr可能是交替执行的。每当一个mtr执行完成时，伴随该mtr生成的一组redo日志就需要被复制到log buffer中，也就是说不同事务的mtr可能是交替写入log buffer的，我们画个示意图（为了美观，我们把一个mtr中产生的所有的redo日志当作一个整体来画）： 从示意图中我们可以看出来，不同的mtr产生的一组redo日志占用的存储空间可能不一样，有的mtr产生的redo日志量很少，比如mtr_t1_1、mtr_t2_1就被放到同一个block中存储，有的mtr产生的redo日志量非常大，比如mtr_t1_2产生的redo日志甚至占用了3个block来存储。 五，redo 日志文件1.redo日志刷盘时机mtr运行过程中产生的一组redo日志在mtr结束时会被复制到log buffer`中，在一些情况下它们会被刷新到磁盘里，比如： log buffer空间不足时log buffer的大小是有限的（通过系统变量innodb_log_buffer_size指定），如果不停的往这个有限大小的log buffer里塞入日志，很快它就会被填满。InnoDB认为如果当前写入log buffer的redo日志量已经占满了log buffer总容量的大约一半左右，就需要把这些日志刷新到磁盘上。 事务提交时之所以使用redo日志主要是因为它占用的空间少，还是顺序写，在事务提交时可以不把修改过的Buffer Pool页面刷新到磁盘，但是为了保证持久性，必须要把修改这些页面对应的redo日志刷新到磁盘。 将某个脏页刷新到磁盘前，会保证先将该脏页对应的 redo 日志刷新到磁盘中（再一次 强调，redo 日志是顺序刷新的，所以在将某个脏页对应的 redo 日志从 redo log buffer 刷新到磁盘时，也会保证将在其之前产生的 redo 日志也刷新到磁盘）。 后台线程不停的刷后台有一个线程，大约每秒都会刷新一次log buffer中的redo日志到磁盘。 正常关闭服务器时 做所谓的checkpoint时 其他的一些情况… 2.redo日志文件组MySQL的数据目录（使用SHOW VARIABLES LIKE &#39;datadir&#39;查看）下默认有两个名为ib_logfile0和ib_logfile1的文件，log buffer中的日志默认情况下就是刷新到这两个磁盘文件中。如果我们对默认的redo日志文件不满意，可以通过下边几个启动参数来调节： innodb_log_group_home_dir该参数指定了redo日志文件所在的目录，默认值就是当前的数据目录。 innodb_log_file_size该参数指定了每个redo日志文件的大小，在MySQL 5.7.21这个版本中的默认值为48MB， innodb_log_files_in_group该参数指定redo日志文件的个数，默认值为2，最大值为100。 磁盘上的redo日志文件不只一个，而是以一个日志文件组的形式出现的。这些文件以ib_logfile[数字]（数字可以是0、1、2…）的形式进行命名。在将redo日志写入日志文件组时，是从ib_logfile0开始写，如果ib_logfile0写满了，就接着ib_logfile1写，同理，ib_logfile1写满了就去写ib_logfile2，依此类推。如果写到最后一个文件该咋办？那就重新转到ib_logfile0继续写，所以整个过程如下图所示： 总共的redo日志文件大小其实就是：innodb_log_file_size × innodb_log_files_in_group。 如果采用循环使用的方式向redo日志文件组里写数据的话，那岂不是要追尾，也就是后写入的redo日志覆盖掉前边写的redo日志？当然可能了！所以InnoDB提出了checkpoint的概念。 3.redo日志文件格式log buffer本质上是一片连续的内存空间，被划分成了若干个512字节大小的block。将log buffer中的redo日志刷新到磁盘的本质就是把block的镜像写入日志文件中，所以redo日志文件其实也是由若干个512字节大小的block组成。 redo日志文件组中的每个文件大小都一样，格式也一样，都是由两部分组成： 前2048个字节，也就是前4个block是用来存储一些管理信息的。 从第2048字节往后是用来存储log buffer中的block镜像的。 所以我们前边所说的循环使用redo日志文件，其实是从每个日志文件的第2048个字节开始算，画个示意图就是这样： 普通block的格式我们在了解log buffer的时候都说过了，就是log block header、log block body、log block trialer这三个部分。这里需要介绍一下每个redo日志文件前2048个字节，也就是前4个特殊block的格式都是什么作用。 从图中可以看出来，这4个block分别是： log file header：描述该redo日志文件的一些整体属性各个属性的具体释义如下： 属性名 长度（单位：字节） 描述 LOG_HEADER_FORMAT 4 redo日志的版本，在MySQL 5.7.21中该值永远为1 LOG_HEADER_PAD1 4 做字节填充用的，没什么实际意义，忽略～ LOG_HEADER_START_LSN 8 标记本redo日志文件开始的LSN值，也就是文件偏移量为2048字节初对应的LSN值。 LOG_HEADER_CREATOR 32 一个字符串，标记本redo日志文件的创建者是谁。正常运行时该值为MySQL的版本号，比如：&quot;MySQL 5.7.21&quot;，使用mysqlbackup命令创建的redo日志文件的该值为&quot;ibbackup&quot;和创建时间。 LOG_BLOCK_CHECKSUM 4 本block的校验值，所有block都有，我们不关心 checkpoint1：记录关于checkpoint的一些属性，看一下它的结构：各个属性的具体释义如下： 属性名 长度（单位：字节） 描述 LOG_CHECKPOINT_NO 8 服务器做checkpoint的编号，每做一次checkpoint，该值就加1。 LOG_CHECKPOINT_LSN 8 服务器做checkpoint结束时对应的LSN值，系统崩溃恢复时将从该值开始。 LOG_CHECKPOINT_OFFSET 8 上个属性中的LSN值在redo日志文件组中的偏移量 LOG_CHECKPOINT_LOG_BUF_SIZE 8 服务器在做checkpoint操作时对应的log buffer的大小 LOG_BLOCK_CHECKSUM 4 本block的校验值，所有block都有，我们不关心 第三个block未使用，忽略 checkpoint2：结构和checkpoint1一样。 六，Log Sequence Number自系统开始运行，就不断的在修改页面，也就意味着会不断的生成redo日志。redo日志的量在不断的递增。InnoDB为记录已经写入的redo日志量，设计了一个称之为Log Sequence Number的全局变量，翻译过来就是：日志序列号，简称lsn。InnoDB规定初始的lsn值为8704（也就是一条redo日志也没写入时，lsn的值为8704）。 在向log buffer中写入redo日志时不是一条一条写入的，而是以一个mtr生成的一组redo日志为单位进行写入的。而且实际上是把日志内容写在了log block body处。但是在统计lsn的增长量时，是按照实际写入的日志量加上占用的log block header和log block trailer来计算的。我们来看一个例子： 系统第一次启动后初始化log buffer时，buf_free（就是标记下一条redo日志应该写入到log buffer的位置的变量）就会指向第一个block的偏移量为12字节（log block header的大小）的地方，那么lsn值也会跟着增加12： 如果某个mtr产生的一组redo日志占用的存储空间比较小，也就是待插入的block剩余空闲空间能容纳这个mtr提交的日志时，lsn增长的量就是该mtr生成的redo日志占用的字节数，就像这样：我们假设上图中mtr_1产生的redo日志量为200字节，那么lsn就要在8716的基础上增加200，变为8916。 如果某个mtr产生的一组redo日志占用的存储空间比较大，也就是待插入的block剩余空闲空间不足以容纳这个mtr提交的日志时，lsn增长的量就是该mtr生成的redo日志占用的字节数加上额外占用的log block header和log block trailer的字节数，就像这样：我们假设上图中mtr_2产生的redo日志量为1000字节，为了将mtr_2产生的redo日志写入log buffer，我们不得不额外多分配两个block，所以lsn的值需要在8916的基础上增加1000 + 12×2 + 4 × 2 = 1032。 从上边的描述中可以看出来，每一组由mtr生成的redo日志都有一个唯一的LSN值与其对应，LSN值越小，说明redo日志产生的越早。 1.flushed_to_disk_lsnredo日志是首先写到log buffer中，之后才会被刷新到磁盘上的redo日志文件。所以InnoDB提出了一个称之为buf_next_to_write的全局变量，标记当前log buffer中已经有哪些日志被刷新到磁盘中了。画个图表示就是这样： lsn是表示当前系统中写入的redo日志量，这包括了写到log buffer而没有刷新到磁盘的日志，相应的，InnoDB提出了一个表示刷新到磁盘中的redo日志量的全局变量，称之为flushed_to_disk_lsn。系统第一次启动时，该变量的值和初始的lsn值是相同的，都是8704。随着系统的运行，redo日志被不断写入log buffer，但是并不会立即刷新到磁盘，lsn的值就和flushed_to_disk_lsn的值拉开了差距。我们推理一下： 系统第一次启动后，向log buffer中写入了mtr_1、mtr_2、mtr_3这三个mtr产生的redo日志，假设这三个mtr开始和结束时对应的lsn值分别是： mtr_1：8716 ～ 8916 mtr_2：8916 ～ 9948 mtr_3：9948 ～ 10000 此时的lsn已经增长到了10000，但是由于没有刷新操作，所以此时flushed_to_disk_lsn的值仍为8704，如图： 随后进行将log buffer中的block刷新到redo日志文件的操作，假设将mtr_1和mtr_2的日志刷新到磁盘，那么flushed_to_disk_lsn就应该增长mtr_1和mtr_2写入的日志量，所以flushed_to_disk_lsn的值增长到了9948，如图： 综上所述，当有新的redo日志写入到log buffer时，首先lsn的值会增长，但flushed_to_disk_lsn不变，随后随着不断有log buffer中的日志被刷新到磁盘上，flushed_to_disk_lsn的值也跟着增长。如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中了。 应用程序向磁盘写入文件时其实是先写到操作系统的缓冲区中去，如果某个写入操作要等到操作系统确认已经写到磁盘时才返回，那需要调用一下操作系统提供的fsync函数。其实只有当系统执行了fsync函数后，flushed_to_disk_lsn的值才会跟着增长，当仅仅把log buffer中的日志写入到操作系统缓冲区却没有显式的刷新到磁盘时，另外的一个称之为write_lsn的值跟着增长。 2.lsn值和redo日志文件偏移量的对应关系因为lsn的值是代表系统写入的redo日志量的一个总和，一个mtr中产生多少日志，lsn的值就增加多少（当然有时候要加上log block header和log block trailer的大小），这样mtr产生的日志写到磁盘中时，很容易计算某一个lsn值在redo日志文件组中的偏移量，如图： 初始时的LSN值是8704，对应文件偏移量2048，之后每个mtr向磁盘中写入多少字节日志，lsn的值就增长多少。 3.flush链表中的LSN一个mtr代表一次对底层页面的原子访问，在访问过程中可能会产生一组不可分割的redo日志，在mtr结束时，会把这一组redo日志写入到log buffer中。除此之外，在mtr结束时还有一件非常重要的事情要做，就是把在mtr执行过程中可能修改过的页面加入到Buffer Pool的flush链表。 当第一次修改某个缓存在Buffer Pool中的页面时，就会把这个页面对应的控制块插入到flush链表的头部，之后再修改该页面时由于它已经在flush链表中了，就不再次插入了。也就是说flush链表中的脏页是按照页面的第一次修改时间从大到小进行排序的。在这个过程中会在缓存页对应的控制块中记录两个关于页面何时修改的属性： oldest_modification：如果某个页面被加载到Buffer Pool后进行第一次修改，那么就将修改该页面的mtr开始时对应的lsn值写入这个属性。 newest_modification：每修改一次页面，都会将修改该页面的mtr结束时对应的lsn值写入这个属性。也就是说该属性表示页面最近一次修改后对应的系统lsn值。 接着上边flushed_to_disk_lsn的例子看一下： 假设mtr_1执行过程中修改了页a，那么在mtr_1执行结束时，就会将页a对应的控制块加入到flush链表的头部。并且将mtr_1开始时对应的lsn，也就是8716写入页a对应的控制块的oldest_modification属性中，把mtr_1结束时对应的lsn，也就是8916写入页a对应的控制块的newest_modification属性中。画个图表示一下（oldest_modification缩写成了o_m，newest_modification缩写成了n_m）： 接着假设mtr_2执行过程中又修改了页b和页c两个页面，那么在mtr_2执行结束时，就会将页b和页c对应的控制块都加入到flush链表的头部。并且将mtr_2开始时对应的lsn，也就是8916写入页b和页c对应的控制块的oldest_modification属性中，把mtr_2结束时对应的lsn，也就是9948写入页b和页c对应的控制块的newest_modification属性中。画个图表示一下：从图中可以看出来，每次新插入到flush链表中的节点都是被放在了头部，也就是说flush链表中前边的脏页修改的时间比较晚，后边的脏页修改时间比较早。 接着假设mtr_3执行过程中修改了页b和页d，不过页b之前已经被修改过了，所以它对应的控制块已经被插入到了flush链表，所以在mtr_3执行结束时，只需要将页d对应的控制块都加入到flush链表的头部即可。所以需要将mtr_3开始时对应的lsn，也就是9948写入页d对应的控制块的oldest_modification属性中，把mtr_3结束时对应的lsn，也就是10000写入页d对应的控制块的newest_modification属性中。另外，由于页b在mtr_3执行过程中又发生了一次修改，所以需要更新页b对应的控制块中newest_modification的值为10000。画个图表示一下： 总结一下上边说的，就是：flush链表中的脏页按照修改发生的时间顺序进行排序，也就是按照oldest_modification代表的LSN值进行排序，被多次更新的页面不会重复插入到flush链表中，但是会更新newest_modification属性的值。 七，checkpointredo日志文件组容量是有限的，我们不得不选择循环使用redo日志文件组中的文件，但是这会造成最后写的redo日志与最开始写的redo日志追尾，这时应该想到：redo日志只是为了系统崩溃后恢复脏页用的，如果对应的脏页已经刷新到了磁盘，也就是说即使现在系统崩溃，那么在重启后也用不着使用redo日志恢复该页面了，所以该redo日志也就没有存在的必要了，那么它占用的磁盘空间就可以被后续的redo日志所重用。也就是说：判断某些redo日志占用的磁盘空间是否可以覆盖的依据就是它对应的脏页是否已经刷新到磁盘里。我们看一下前边的那个例子： 如图，虽然mtr_1和mtr_2生成的redo日志都已经被写到了磁盘上，但是它们修改的脏页仍然留在Buffer Pool中，所以它们生成的redo日志在磁盘上的空间是不可以被覆盖的。之后随着系统的运行，如果页a被刷新到了磁盘，那么它对应的控制块就会从flush链表中移除，就像这样子： 这样mtr_1生成的redo日志就没有用了，它们占用的磁盘空间就可以被覆盖掉了。InnoDB提出了一个全局变量checkpoint_lsn来代表当前系统中可以被覆盖的redo日志总量是多少，这个变量初始值也是8704。 比方说现在页a被刷新到了磁盘，mtr_1生成的redo日志就可以被覆盖了，所以我们可以进行一个增加checkpoint_lsn的操作，我们把这个过程称之为做一次checkpoint。做一次checkpoint其实可以分为两个步骤： 步骤一：计算一下当前系统中可以被覆盖的redo日志对应的lsn值最大是多少。redo日志可以被覆盖，意味着它对应的脏页被刷到了磁盘，只要我们计算出当前系统中被最早修改的脏页对应的oldest_modification值，那凡是在系统lsn值小于该节点的oldest_modification值时产生的redo日志都是可以被覆盖掉的，我们就把该脏页的oldest_modification赋值给checkpoint_lsn。比方说当前系统中页a已经被刷新到磁盘，那么flush链表的尾节点就是页c，该节点就是当前系统中最早修改的脏页了，它的oldest_modification值为8916，我们就把8916赋值给checkpoint_lsn（也就是说在redo日志对应的lsn值小于8916时就可以被覆盖掉）。 步骤二：将checkpoint_lsn和对应的redo日志文件组偏移量以及此次checkpint的编号写到日志文件的管理信息（就是checkpoint1或者checkpoint2）中。InnoDB维护了一个目前系统做了多少次checkpoint的变量checkpoint_no，每做一次checkpoint，该变量的值就加1。我们前边说过计算一个lsn值对应的redo日志文件组偏移量是很容易的，所以可以计算得到该checkpoint_lsn在redo日志文件组中对应的偏移量checkpoint_offset，然后把这三个值都写到redo日志文件组的管理信息中。我们说过，每一个redo日志文件都有2048个字节的管理信息，但是上述关于checkpoint的信息只会被写到日志文件组的第一个日志文件的管理信息中。不过我们是存储到checkpoint1中还是checkpoint2中呢？InnoDB规定，当checkpoint_no的值是偶数时，就写到checkpoint1中，是奇数时，就写到checkpoint2中。 记录完checkpoint的信息之后，redo日志文件组中各个lsn值的关系就像这样： 1.批量从flush链表中刷出脏页一般情况下都是后台的线程在对LRU链表和flush链表进行刷脏操作，这主要因为刷脏操作比较慢，不想影响用户线程处理请求。但是如果当前系统修改页面的操作十分频繁，这样就导致写日志操作十分频繁，系统lsn值增长过快。如果后台的刷脏操作不能将脏页刷出，那么系统无法及时做checkpoint，可能就需要用户线程同步的从flush链表中把那些最早修改的脏页（oldest_modification最小的脏页）刷新到磁盘，这样这些脏页对应的redo日志就没用了，然后就可以去做checkpoint了。 2.查看系统中的各种LSN值我们可以使用SHOW ENGINE INNODB STATUS命令查看当前InnoDB存储引擎中的各种LSN值的情况，比如： 12345678910111213mysql&gt; SHOW ENGINE INNODB STATUS\\G(...省略前边的许多状态)LOG---Log sequence number 124476971Log flushed up to 124099769Pages flushed up to 124052503Last checkpoint at 1240524940 pending log flushes, 0 pending chkp writes24 log i/o&#x27;s done, 2.00 log i/o&#x27;s/second----------------------(...省略后边的许多状态) 其中： Log sequence number：代表系统中的lsn值，也就是当前系统已经写入的redo日志量，包括写入log buffer中的日志。 Log flushed up to：代表flushed_to_disk_lsn的值，也就是当前系统已经写入磁盘的redo日志量。 Pages flushed up to：代表flush链表中被最早修改的那个页面对应的oldest_modification属性值。 Last checkpoint at：当前系统的checkpoint_lsn值。 3.innodb_flush_log_at_trx_commit的用法为了保证事务的持久性，用户线程在事务提交时需要将该事务执行过程中产生的所有redo日志都刷新到磁盘上。这一条要求太狠了，会很明显的降低数据库性能。如果对事务的持久性要求不是那么强烈的话，可以选择修改一个称为innodb_flush_log_at_trx_commit的系统变量的值，该变量有3个可选的值： 0：当该系统变量值为0时，表示在事务提交时不立即向磁盘中同步redo日志，这个任务是交给后台线程做的。这样很明显会加快请求处理速度，但是如果事务提交后服务器挂了，后台线程没有及时将redo日志刷新到磁盘，那么该事务对页面的修改会丢失。 1：当该系统变量值为1时，表示在事务提交时需要将redo日志同步到磁盘，可以保证事务的持久性。1也是innodb_flush_log_at_trx_commit的默认值。 2：当该系统变量值为2时，表示在事务提交时需要将redo日志写到操作系统的缓冲区中，但并不需要保证将日志真正的刷新到磁盘。这种情况下如果数据库挂了，操作系统没挂的话，事务的持久性还是可以保证的，但是操作系统也挂了的话，那就不能保证持久性了。 八，崩溃恢复在服务器不挂的情况下，redo日志不仅没用，反而让性能变得更差。但是万一数据库挂了，我们就可以在重启时根据redo日志中的记录就可以将页面恢复到系统崩溃前的状态。我们接下来大致看一下恢复过程。 1.确定恢复的起点checkpoint_lsn之前的redo日志都可以被覆盖，也就是说这些redo日志对应的脏页都已经被刷新到磁盘中了，既然它们已经被刷盘，我们就没必要恢复它们了。对于checkpoint_lsn之后的redo日志，它们对应的脏页可能没被刷盘，也可能被刷盘了，我们不能确定，所以需要从checkpoint_lsn开始读取redo日志来恢复页面。 当然，redo日志文件组的第一个文件的管理信息中有两个block都存储了checkpoint_lsn的信息，我们当然是要选取最近发生的那次checkpoint的信息。衡量checkpoint发生时间早晚的信息就是所谓的checkpoint_no，只要把checkpoint1和checkpoint2这两个block中的checkpoint_no值读出来比一下大小，哪个的checkpoint_no值更大，说明哪个block存储的就是最近的一次checkpoint信息。这样我们就能拿到最近发生的checkpoint对应的checkpoint_lsn值以及它在redo日志文件组中的偏移量checkpoint_offset。 2.确定恢复的终点redo日志恢复的起点确定了，那终点是哪个呢？这个还得从block的结构说起。在写redo日志的时候都是顺序写的，写满了一个block之后会再往下一个block中写。 普通block的log block header部分有一个称之为LOG_BLOCK_HDR_DATA_LEN的属性，该属性值记录了当前block里使用了多少字节的空间。对于被填满的block来说，该值永远为512。如果该属性的值不为512，那么就是它了，它就是此次崩溃恢复中需要扫描的最后一个block。 3.怎么恢复确定了需要扫描哪些redo日志进行崩溃恢复之后，接下来就是怎么进行恢复了。假设现在的redo日志文件中有5条redo日志，如图： 由于redo 0在checkpoint_lsn后前边，恢复时可以不管它。现在可以按照redo日志的顺序依次扫描checkpoint_lsn之后的各条redo日志，按照日志中记载的内容将对应的页面恢复出来。这样没什么问题，不过InnoDB还是想了一些办法加快这个恢复的过程： 使用哈希表根据redo日志的space ID和page number属性计算出散列值，把space ID和page number相同的redo日志放到哈希表的同一个槽里，如果有多个space ID和page number都相同的redo日志，那么它们之间使用链表连接起来，按照生成的先后顺序链接起来的，如图所示：之后就可以遍历哈希表，因为对同一个页面进行修改的redo日志都放在了一个槽里，所以可以一次性将一个页面修复好（避免了很多读取页面的随机IO)，这样可以加快恢复速度。另外需要注意一点的是，同一个页面的redo日志是按照生成时间顺序进行排序的，所以恢复的时候也是按照这个顺序进行恢复，如果不按照生成时间顺序进行排序的话，那么可能出现错误。比如原先的修改操作是先插入一条记录，再删除该条记录，如果恢复时不按照这个顺序来，就可能变成先删除一条记录，再插入一条记录，这显然是错误的。 跳过已经刷新到磁盘的页面checkpoint_lsn之前的redo日志对应的脏页确定都已经刷到磁盘了，但是checkpoint_lsn之后的redo日志我们不能确定是否已经刷到磁盘，主要是因为在最近做的一次checkpoint后，可能后台线程又不断的从LRU链表和flush链表中将一些脏页刷出Buffer Pool。这些在checkpoint_lsn之后的redo日志，如果它们对应的脏页在崩溃发生时已经刷新到磁盘，那在恢复时也就没有必要根据redo日志的内容修改该页面了。那在恢复时怎么知道某个redo日志对应的脏页是否在崩溃发生时已经刷新到磁盘了呢？这还得从页面的结构说起，每个页面都有一个称之为File Header的部分，在File Header里有一个称之为FIL_PAGE_LSN的属性，该属性记载了最近一次修改页面时对应的lsn值（其实就是页面控制块中的newest_modification值）。如果在做了某次checkpoint之后有脏页被刷新到磁盘中，那么该页对应的FIL_PAGE_LSN代表的lsn值肯定大于checkpoint_lsn的值，凡是符合这种情况的页面就不需要重复执行lsn值小于FIL_PAGE_LSN的redo日志了，所以更进一步提升了崩溃恢复的速度。 九，LOG_BLOCK_HDR_NO是如何计算的对于实际存储redo日志的普通的log block来说，在log block header处有一个称之为LOG_BLOCK_HDR_NO的属性，我们说这个属性代表一个唯一的标号。这个属性是初次使用该block时分配的，跟当时的系统lsn值有关。使用下边的公式计算该block的LOG_BLOCK_HDR_NO值： 1((lsn / 512) &amp; 0x3FFFFFFFUL) + 1 从图中可以看出，0x3FFFFFFFUL对应的二进制数的前2位为0，后30位的值都为1。一个二进制位与0做与运算（&amp;）的结果肯定是0，一个二进制位与1做与运算（&amp;）的结果就是原值。让一个数和0x3FFFFFFFUL做与运算的意思就是要将该值的前2个比特位的值置为0，这样该值就肯定小于或等于0x3FFFFFFFUL了。这也就说明了，不论lsn多大，((lsn / 512) &amp; 0x3FFFFFFFUL)的值肯定在0``0x3FFFFFFFUL之间，再加1的话肯定在1``0x40000000UL之间。而0x40000000UL这个值就代表着1GB。也就是说系统最多能产生不重复的LOG_BLOCK_HDR_NO值只有1GB个。InnoDB规定redo日志文件组中包含的所有文件大小总和不得超过512GB，一个block大小是512字节，也就是说redo日志文件组中包含的block块最多为1GB个，所以有1GB个不重复的编号值也就够用了。 另外，LOG_BLOCK_HDR_NO值的第一个比特位比较特殊，称之为flush bit，如果该值为1，代表着本block是在某次将log buffer中的block刷新到磁盘的操作中的第一个被刷入的block。 十，double write1.脏页刷盘风险关于IO的最小单位： 数据库IO的最小单位是16K（MySQL默认，oracle是8K） 文件系统IO的最小单位是4K（也有1K的） 磁盘IO的最小单位是512字节 因此，存在IO写入导致page损坏的风险： 2.doublewrite：两次写提高innodb的可靠性，用来解决部分写失败(partial page write页断裂)。 2.1 Double write解决了什么问题一个数据页的大小是16K，假设在把内存中的脏页写到数据库的时候，写了2K突然掉电，也就是说前2K数据是新的，后14K是旧的，那么磁盘数据库这个数据页就是不完整的，是一个坏掉的数据页。redo只能加上旧、校检完整的数据页恢复一个脏块，不能修复坏掉的数据页，所以这个数据就丢失了，可能会造成数据不一致，所以需要double write。 2.2使用情景当数据库正在从内存想磁盘写一个数据页是，数据库宕机，从而导致这个页只写了部分数据，这就是部分写失效，它会导致数据丢失。这时是无法通过重做日志恢复的，因为重做日志记录的是对页的物理修改，如果页本身已经损坏，重做日志也无能为力。 2.3 double write工作流程 doublewrite由两部分组成，一部分为内存中的doublewrite buffer，其大小为2MB，另一部分是磁盘上共享表空间(ibdata x)中连续的128个页，即2个区(extent)，大小也是2M。 当一系列机制触发数据缓冲池中的脏页刷新时，并不直接写入磁盘数据文件中，而是先拷贝至内存中的doublewrite buffer中； 接着从两次写缓冲区分两次写入磁盘共享表空间中(连续存储，顺序写，性能很高)，每次写1MB； 待第二步完成后，再将doublewrite buffer中的脏页数据写入实际的各个表空间文件(离散写)；(脏页数据固化后，即进行标记对应doublewrite数据可覆盖) 2.4 doublewrite的崩溃恢复如果操作系统在将页写入磁盘的过程中发生崩溃，在恢复过程中，innodb存储引擎可以从共享表空间的doublewrite中找到该页的一个最近的副本，将其复制到表空间文件，再应用redo log，就完成了恢复过程。因为有副本所以也不担心表空间中数据页是否损坏。 Q：为什么*log write*不需要*doublewrite*的支持？ A：因为*redolog*写入的单位就是512字节，也就是磁盘IO的最小单位，所以无所谓数据损坏。 3.doublewrite的副作用3.1 double write带来的写负载 double write是一个buffer, 但其实它是开在物理文件上的一个buffer, 其实也就是file, 所以它会导致系统有更多的fsync操作, 而硬盘的fsync性能是很慢的, 所以它会降低mysql的整体性能。 但是，doublewrite buffer写入磁盘共享表空间这个过程是连续存储，是顺序写，性能非常高，(约占写的10%)，牺牲一点写性能来保证数据页的完整还是很有必要的。 3.2 监控double write工作负载12345678mysql&gt; show global status like &#x27;%dblwr%&#x27;;+----------------------------+-------+| Variable_name | Value |+----------------------------+-------+| Innodb_dblwr_pages_written | 7 || Innodb_dblwr_writes | 3 |+----------------------------+-------+2 rows in set (0.00 sec) 关注点：Innodb_dblwr_pages_written / Innodb_dblwr_writes 开启doublewrite后，每次脏页刷新必须要先写doublewrite，而doublewrite存在于磁盘上的是两个连续的区，每个区由连续的页组成，一般情况下一个区最多有64个页，所以一次IO写入应该可以最多写64个页。 而根据以上系统Innodb_dblwr_pages_written与Innodb_dblwr_writes的比例来看，大概在3左右，远远还没到64(如果约等于64，那么说明系统的写压力非常大，有大量的脏页要往磁盘上写)，所以从这个角度也可以看出，系统写入压力并不高。 3.3 关闭double write适合的场景 海量DML 不惧怕数据损坏和丢失 系统写负载成为主要负载 1234567mysql&gt; show variables like &#x27;%double%&#x27;;+--------------------+-------+| Variable_name | Value |+--------------------+-------+| innodb_doublewrite | ON |+--------------------+-------+1 row in set (0.04 sec) 作为InnoDB的一个关键特性，doublewrite功能默认是开启的，但是在上述特殊的一些场景也可以视情况关闭，来提高数据库写性能。静态参数，配置文件修改，重启数据库。 3.4 为什么没有把double write里面的数据写到data page里面呢？ double write里面的数据是连续的，如果直接写到data page里面，而data page的页又是离散的，写入会很慢。 double write里面的数据没有办法被及时的覆盖掉，导致double write的压力很大；短时间内可能会出现double write溢出的情况。 十一，总结redo日志记录了事务执行过程中都修改了哪些内容。 事务提交时只将执行过程中产生的redo日志刷新到磁盘，而不是将所有修改过的页面都刷新到磁盘。这样做有两个好处： redo日志占用的空间非常小 redo日志是顺序写入磁盘的 一条redo日志由下面几部分组成。 type：这条redo日志的类型 space ID:表空间ID page number :页号 data：这条redo日志的具体内容 redo日志的类型有简单和复杂之分。简单类型的redo日志是纯粹的物理日志，复杂类型的redo日志兼有物理日志和逻辑日志的特性。 一个MTR可以包含一组redo日志。在进行崩溃恢复时，这一组redo日志作为一个不可分割的整体来处理。 redo日志存放在大小为512字节的block中。每一个block被分为3部分： log block header log block body log block trailer redo日志缓冲区是一片连续的内存空间，由若干个block组成；可以通过启动选项innodb_log_buffer_size 来调整他的大小。 redo日志文件组由若干个日志文件组成，这些redo日志文件是被循环使用的。redo日志文件组中每个文件的大小都一样，格式也一样，都是由两部分组成的： 前2048字节用来存储一些管理信息 从第2048字节往后的字节用来存储log buffer中的block镜像 lsn指已经写入的redo日志量，flushed_to_disk_lsn指刷新到磁盘中的redo日志量，flush链表中的脏页按照修改发生的时间顺序进行排序，也就是按照oldest_modification代表的lsn值进行排序。被多次更新的页面不会重复插入到flush链表，但是会更新newest_modification属性的值。checkpoint_lsn表示当前系统中可以被覆盖的redo日志总量是多少。 redo日志占用的磁盘空间在他对应的脏页已经被刷新到磁盘后即可被覆盖。执行一次checkpoint的意思就是增加checkpoint_lsn的值，然后把相关信息放到日志文件的管理信息中。 innodb_flush_log_at_trx_commit系统变量控制着在事务提交时是否将该事务运行过程中产生的redo刷新到磁盘。 在崩溃恢复过程中，从redo日志文件组第一个文件的管理信息中取出最近发生的那次checkpoint信息，然后从checkpoint_lsn在日志文件组中对应的偏移量开始，一直扫描日志文件中的block，直到某个block的LOG_BLOCK_HDR_DATA_LEN值不等于512为止。再恢复过程中，使用hash表可加快恢复过程，并且会跳过已经刷新到磁盘的页面。","categories":[{"name":"7.mysql","slug":"7-mysql","permalink":"https://zhangxin66666.github.io/categories/7-mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"}]},{"title":"undolog","slug":"7.mysql/undo日志","date":"2021-12-26T16:00:00.000Z","updated":"2024-08-22T03:23:14.708Z","comments":true,"path":"2021/12/27/7.mysql/undo日志/","link":"","permalink":"https://zhangxin66666.github.io/2021/12/27/7.mysql/undo%E6%97%A5%E5%BF%97/","excerpt":"","text":"1.事务回滚的需求我们说过事务需要保证原子性，也就是事务中的操作要么全部完成，要么什么也不做。但是偏偏有时候事务执行到一半会出现一些情况，比如： 情况一：事务执行过程中可能遇到各种错误，比如服务器本身的错误，操作系统错误，甚至是突然断电导致的错误。 情况二：程序员可以在事务执行过程中手动输入ROLLBACK语句结束当前的事务的执行。 这两种情况都会导致事务执行到一半就结束，但是事务执行过程中可能已经修改了很多东西，为了保证事务的原子性，我们需要把东西改回原先的样子，这个过程就称之为回滚（英文名：rollback），这样就可以造成一个假象：这个事务看起来什么都没做，所以符合原子性要求。 每当我们要对一条记录做改动时（这里的改动可以指INSERT、DELETE、UPDATE），都需要留一手 —— 把回滚时所需的东西都给记下来。比方说： 你插入一条记录时，至少要把这条记录的主键值记下来，之后回滚的时候只需要把这个主键值对应的记录删掉就好了。 你删除了一条记录，至少要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了。 你修改了一条记录，至少要把修改这条记录前的旧值都记录下来，这样之后回滚时再把这条记录更新为旧值就好了。 数据库把这些为了回滚而记录的这些东西称之为撤销日志，英文名为undo log，我们也可以土洋结合，称之为undo日志。这里需要注意的一点是，由于查询操作（SELECT）并不会修改任何用户记录，所以在查询操作执行时，并不需要记录相应的undo日志。在真实的InnoDB中，undo日志其实并不像我们上边所说的那么简单，不同类型的操作产生的undo日志的格式也是不同的，不过先暂时把这些具体细节放一放，我们先回过头来看看事务id。 2.事务id2.1给事务分配id的时机一个事务可以是一个只读事务，或者是一个读写事务： 我们可以通过START TRANSACTION READ ONLY语句开启一个只读事务。在只读事务中不可以对普通的表（其他事务也能访问到的表）进行增、删、改操作，但可以对临时表做增、删、改操作。 我们可以通过START TRANSACTION READ WRITE语句开启一个读写事务，或者使用BEGIN、START TRANSACTION语句开启的事务默认也算是读写事务。在读写事务中可以对表执行增删改查操作。 如果某个事务执行过程中对某个表执行了增、删、改操作，那么InnoDB存储引擎就会给它分配一个独一无二的事务id，分配方式如下： 对于只读事务来说，只有在它第一次对某个用户创建的临时表执行增、删、改操作时才会为这个事务分配一个事务id，否则的话是不分配事务id的。 对某个查询语句执行EXPLAIN分析它的查询计划时，有时候在Extra列会看到Using temporary的提示，这个表明在执行该查询语句时会用到内部临时表。这个所谓的内部临时表和我们手动用CREATE TEMPORARY TABLE创建的用户临时表并不一样，在事务回滚时并不需要把执行SELECT语句过程中用到的内部临时表也回滚，在执行SELECT语句用到内部临时表时并不会为它分配事务id。 对于读写事务来说，只有在它第一次对某个表（包括用户创建的临时表）执行增、删、改操作时才会为这个事务分配一个事务id，否则的话也是不分配事务id的。有的时候虽然我们开启了一个读写事务，但是在这个事务中全是查询语句，并没有执行增、删、改的语句，那也就意味着这个事务并不会被分配一个事务id。 只有在事务对表中的记录做改动时才会为这个事务分配一个唯一的**事务id**。 上边描述的事务id分配策略是针对MySQL 5.7来说的，前边的版本的分配方式可能不同。 2.2事务id是怎么生成的这个事务id本质上就是一个数字，它的分配策略和对隐藏列row_id（当用户没有为表创建主键和UNIQUE键时InnoDB自动创建的列）的分配策略大抵相同，具体策略如下： 服务器会在内存中维护一个全局变量，每当需要为某个事务分配一个事务id时，就会把该变量的值当作事务id分配给该事务，并且把该变量自增1。 每当这个变量的值为256的倍数时，就会将该变量的值刷新到系统表空间的页号为5的页面中一个称之为Max Trx ID的属性处，这个属性占用8个字节的存储空间。 当系统下一次重新启动时，会将上边提到的Max Trx ID属性加载到内存中，将该值加上256之后赋值给我们前边提到的全局变量（因为在上次关机时该全局变量的值可能大于Max Trx ID属性值）。 这样就可以保证整个系统中分配的事务id值是一个递增的数字。先被分配id的事务得到的是较小的事务id，后被分配id的事务得到的是较大的事务id。 2.3trx_id隐藏列聚簇索引的记录除了会保存完整的用户数据以外，而且还会自动添加名为trx_id、roll_pointer的隐藏列，如果用户没有在表中定义主键以及UNIQUE键，还会自动添加一个名为row_id的隐藏列。所以一条记录在页面中的真实结构看起来就是这样的： 其中的trx_id列其实还蛮好理解的，就是某个对这个聚簇索引记录做改动的语句所在的事务对应的事务id而已（此处的改动可以是INSERT、DELETE、UPDATE操作）。至于roll_pointer隐藏列我们后边分析。 3.undo日志的格式为了实现事务的原子性，InnoDB存储引擎在实际进行增、删、改一条记录时，都需要先把对应的undo日志记下来。一般每对一条记录做一次改动，就对应着一条undo日志，但在某些更新记录的操作中，也可能会对应着2条undo日志。一个事务在执行过程中可能新增、删除、更新若干条记录，也就是说需要记录很多条对应的undo日志，这些undo日志会被从0开始编号，也就是说根据生成的顺序分别被称为第0号undo日志、第1号undo日志、…、第n号undo日志等，这个编号也被称之为undo no。 这些undo日志是被记录到类型为FIL_PAGE_UNDO_LOG的页面中。这些页面可以从系统表空间中分配，也可以从一种专门存放undo日志的表空间，也就是所谓的undo tablespace中分配。先来看看不同操作都会产生什么样子的undo日志吧～我们先来创建一个名为undo_demo的表： 1234567CREATE TABLE undo_demo ( id INT NOT NULL, key1 VARCHAR(100), col VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1))Engine=InnoDB CHARSET=utf8; 这个表中有3个列，其中id列是主键，我们为key1列建立了一个二级索引，col列是一个普通的列。每个表都会被分配一个唯一的table id，我们可以通过系统数据库information_schema中的innodb_sys_tables表来查看某个表对应的table id是什么，现在我们查看一下undo_demo对应的table id是多少： 1234567mysql&gt; SELECT * FROM information_schema.innodb_sys_tables WHERE name = &#x27;yhd/undo_demo&#x27;;+----------+---------------------+------+--------+-------+-------------+------------+---------------+------------+| TABLE_ID | NAME | FLAG | N_COLS | SPACE | FILE_FORMAT | ROW_FORMAT | ZIP_PAGE_SIZE | SPACE_TYPE |+----------+---------------------+------+--------+-------+-------------+------------+---------------+------------+| 138 | yhd/undo_demo | 33 | 6 | 482 | Barracuda | Dynamic | 0 | Single |+----------+---------------------+------+--------+-------+-------------+------------+---------------+------------+1 row in set (0.01 sec) 从查询结果可以看出，undo_demo表对应的table id为138。 3.1INSERT操作对应的undo日志当我们向表中插入一条记录时会有乐观插入和悲观插入的区分，但是不管怎么插入，最终导致的结果就是这条记录被放到了一个数据页中。如果希望回滚这个插入操作，那么把这条记录删除就好了，也就是说在写对应的undo日志时，主要是把这条记录的主键信息记上。所以InnoDB设计了一个类型为TRX_UNDO_INSERT_REC的undo日志，它的完整结构如下图所示： 根据示意图我们强调几点： undo no在一个事务中是从0开始递增的，也就是说只要事务没提交，每生成一条undo日志，那么该条日志的undo no就增1。 如果记录中的主键只包含一个列，那么在类型为TRX_UNDO_INSERT_REC的undo日志中只需要把该列占用的存储空间大小和真实值记录下来，如果记录中的主键包含多个列，那么每个列占用的存储空间大小和对应的真实值都需要记录下来（图中的len就代表列占用的存储空间大小，value就代表列的真实值）。 当我们向某个表中插入一条记录时，实际上需要向聚簇索引和所有的二级索引都插入一条记录。不过记录undo日志时，我们只需要考虑向聚簇索引插入记录时的情况就好了，因为其实聚簇索引记录和二级索引记录是一一对应的，我们在回滚插入操作时，只需要知道这条记录的主键信息，然后根据主键信息做对应的删除操作，做删除操作时就会顺带着把所有二级索引中相应的记录也删除掉。后边说到的DELETE操作和UPDATE操作对应的undo日志也都是针对聚簇索引记录而言的。 现在我们向undo_demo中插入两条记录： 12345BEGIN; # 显式开启一个事务，假设该事务的id为100# 插入两条记录INSERT INTO undo_demo(id, key1, col) VALUES (1, &#x27;AWM&#x27;, &#x27;狙击枪&#x27;), (2, &#x27;M416&#x27;, &#x27;步枪&#x27;); 因为记录的主键只包含一个id列，所以我们在对应的undo日志中只需要将待插入记录的id列占用的存储空间长度（id列的类型为INT，INT类型占用的存储空间长度为4个字节）和真实值记录下来。本例中插入了两条记录，所以会产生两条类型为TRX_UNDO_INSERT_REC的undo日志: 第一条undo日志的undo no为0，记录主键占用的存储空间长度为4，真实值为1。画一个示意图就是这样： 第二条undo日志的undo no为1，记录主键占用的存储空间长度为4，真实值为2。画一个示意图就是这样（与第一条undo日志对比，undo no和主键各列信息有不同）： 为了最大限度的节省undo日志占用的存储空间，和我们前边说过的redo日志类似，InnoDB会给undo日志中的某些属性进行压缩处理。 ①roll_pointer隐藏列的含义roll_pointer本质上就是一个指向记录对应的undo日志的一个指针。比方说我们上边向undo_demo表里插入了2条记录，每条记录都有与其对应的一条undo日志。记录被存储到了类型为FIL_PAGE_INDEX的页面中（就是我们前边一直所说的数据页），undo日志被存放到了类型为FIL_PAGE_UNDO_LOG的页面中。效果如图所示： **roll_pointer**本质就是一个指针，指向记录对应的undo日志。 3.2 DELETE操作对应的undo日志插入到页面中的记录会根据记录头信息中的next_record属性组成一个单向链表，我们把这个链表称之为正常记录链表；被删除的记录其实也会根据记录头信息中的next_record属性组成一个链表，只不过这个链表中的记录占用的存储空间可以被重新利用，所以也称这个链表为垃圾链表。Page Header部分有一个称之为PAGE_FREE的属性，它指向由被删除记录组成的垃圾链表中的头节点。我们先画一个图，假设此刻某个页面中的记录分布情况是这样的（这个不是undo_demo表中的记录，只是我们随便举的一个例子）： 为了突出主题，在这个简化版的示意图中，我们只把记录的delete_mask标志位展示了出来。从图中可以看出，正常记录链表中包含了3条正常记录，垃圾链表里包含了2条已删除记录，在垃圾链表中的这些记录占用的存储空间可以被重新利用。页面的Page Header部分的PAGE_FREE属性的值代表指向垃圾链表头节点的指针。假设现在我们准备使用DELETE语句把正常记录链表中的最后一条记录给删除掉，其实这个删除的过程需要经历两个阶段： 阶段一：仅仅将记录的delete_mask标识位设置为1，其他的不做修改（其实会修改记录的trx_id、roll_pointer这些隐藏列的值）。设计InnoDB的大叔把这个阶段称之为delete mark。把这个过程画下来就是这样：可以看到，正常记录链表中的最后一条记录的delete_mask值被设置为1，但是并没有被加入到垃圾链表。也就是此时记录处于一个中间状态。在删除语句所在的事务提交之前，被删除的记录一直都处于这种所谓的中间状态。 为啥会有这种奇怪的中间状态呢？其实主要是为了实现一个称之为MVCC的功能。 阶段二：当该删除语句所在的事务提交之后，会有专门的线程后来真正的把记录删除掉。所谓真正的删除就是把该记录从正常记录链表中移除，并且加入到垃圾链表中，然后还要调整一些页面的其他信息，比如页面中的用户记录数量PAGE_N_RECS、上次插入记录的位置PAGE_LAST_INSERT、垃圾链表头节点的指针PAGE_FREE、页面中可重用的字节数量PAGE_GARBAGE、还有页目录的一些信息等等。InnoDB把这个阶段称之为purge。把阶段二执行完了，这条记录就算是真正的被删除掉了。这条已删除记录占用的存储空间也可以被重新利用了。画下来就是这样：将被删除记录加入到垃圾链表时，实际上加入到链表的头节点处，会跟着修改PAGE_FREE属性的值。 页面的Page Header部分有一个PAGE_GARBAGE属性，该属性记录着当前页面中可重用存储空间占用的总字节数。每当有已删除记录被加入到垃圾链表后，都会把这个PAGE_GARBAGE属性的值加上该已删除记录占用的存储空间大小。PAGE_FREE指向垃圾链表的头节点，之后每当新插入记录时，首先判断PAGE_FREE指向的头节点代表的已删除记录占用的存储空间是否足够容纳这条新插入的记录，如果不可以容纳，就直接向页面中申请新的空间来存储这条记录（并不会尝试遍历整个垃圾链表，找到一个可以容纳新记录的节点）。如果可以容纳，那么直接重用这条已删除记录的存储空间，并且把PAGE_FREE指向垃圾链表中的下一条已删除记录。但是这里有一个问题，如果新插入的那条记录占用的存储空间大小小于垃圾链表的头节点占用的存储空间大小，那就意味头节点对应的记录占用的存储空间里有一部分空间用不到，这部分空间就被称之为碎片空间。那这些碎片空间岂不是永远都用不到了么？其实也不是，这些碎片空间占用的存储空间大小会被统计到PAGE_GARBAGE属性中，这些碎片空间在整个页面快使用完前并不会被重新利用，不过当页面快满时，如果再插入一条记录，此时页面中并不能分配一条完整记录的空间，这时候会首先看一看PAGE_GARBAGE的空间和剩余可利用的空间加起来是不是可以容纳下这条记录，如果可以的话，InnoDB会尝试重新组织页内的记录，重新组织的过程就是先开辟一个临时页面，把页面内的记录依次插入一遍，因为依次插入时并不会产生碎片，之后再把临时页面的内容复制到本页面，这样就可以把那些碎片空间都解放出来（很显然重新组织页面内的记录比较耗费性能）。 从上边的描述中我们也可以看出来，在删除语句所在的事务提交之前，只会经历阶段一，也就是delete mark阶段（提交之后我们就不用回滚了，所以只需考虑对删除操作的阶段一做的影响进行回滚）。InnoDB为此设计了一种称之为TRX_UNDO_DEL_MARK_REC类型的undo日志，它的完整结构如下图所示： 在对一条记录进行delete mark操作前，需要把该记录的旧的trx_id和roll_pointer隐藏列的值都给记到对应的undo日志中来，就是我们图中显示的old trx_id和old roll_pointer属性。这样有一个好处，那就是可以通过undo日志的old roll_pointer找到记录在修改之前对应的undo日志。比方说在一个事务中，我们先插入了一条记录，然后又执行对该记录的删除操作，这个过程的示意图就是这样：从图中可以看出来，执行完delete mark操作后，它对应的undo日志和INSERT操作对应的undo日志就串成了一个链表。这个链表就称之为版本链。 与类型为TRX_UNDO_INSERT_REC的undo日志不同，类型为TRX_UNDO_DEL_MARK_REC的undo日志还多了一个索引列各列信息的内容，也就是说如果某个列被包含在某个索引中，那么它的相关信息就应该被记录到这个索引列各列信息部分，所谓的相关信息包括该列在记录中的位置（用pos表示），该列占用的存储空间大小（用len表示），该列实际值（用value表示）。所以索引列各列信息存储的内容实质上就是&lt;pos, len, value&gt;的一个列表。这部分信息主要是用在事务提交后，对该中间状态记录做真正删除的阶段二，也就是purge阶段中使用的。 现在继续在上边那个事务id为100的事务中删除一条记录，比如我们把id为1的那条记录删除掉： 12345678BEGIN; # 显式开启一个事务，假设该事务的id为100# 插入两条记录INSERT INTO undo_demo(id, key1, col) VALUES (1, &#x27;AWM&#x27;, &#x27;狙击枪&#x27;), (2, &#x27;M416&#x27;, &#x27;步枪&#x27;); # 删除一条记录 DELETE FROM undo_demo WHERE id = 1; 这个delete mark操作对应的undo日志的结构就是这样： 对照着这个图，我们得注意下边几点： 因为这条undo日志是id为100的事务中产生的第3条undo日志，所以它对应的undo no就是2。 在对记录做delete mark操作时，记录的trx_id隐藏列的值是100（也就是说对该记录最近的一次修改就发生在本事务中），所以把100填入old trx_id属性中。然后把记录的roll_pointer隐藏列的值取出来，填入old roll_pointer属性中，这样就可以通过old roll_pointer属性值找到最近一次对该记录做改动时产生的undo日志。 由于undo_demo表中有2个索引：一个是聚簇索引，一个是二级索引idx_key1。只要是包含在索引中的列，那么这个列在记录中的位置（pos），占用存储空间大小（len）和实际值（value）就需要存储到undo日志中。 对于主键来说，只包含一个id列，存储到undo日志中的相关信息分别是： pos：id列是主键，也就是在记录的第一个列，它对应的pos值为0。pos占用1个字节来存储。 len：id列的类型为INT，占用4个字节，所以len的值为4。len占用1个字节来存储。 value：在被删除的记录中id列的值为1，也就是value的值为1。value占用4个字节来存储。 画一个图演示一下就是这样： 所以对于id列来说，最终存储的结果就是&lt;0, 4, 1&gt;，存储这些信息占用的存储空间大小为1 + 1 + 4 = 6个字节。 对于idx_key1来说，只包含一个key1列，存储到undo日志中的相关信息分别是： pos：key1列是排在id列、trx_id列、roll_pointer列之后的，它对应的pos值为3。pos占用1个字节来存储。 len：key1列的类型为VARCHAR(100)，使用utf8字符集，被删除的记录实际存储的内容是AWM，所以一共占用3个字节，也就是所以len的值为3。len占用1个字节来存储。 value：在被删除的记录中key1列的值为AWM，也就是value的值为AWM。value占用3个字节来存储。 画一个图演示一下就是这样： 所以对于key1列来说，最终存储的结果就是&lt;3, 3, &#39;AWM&#39;&gt;，存储这些信息占用的存储空间大小为1 + 1 + 3 = 5个字节。从上边的叙述中可以看到，&lt;0, 4, 1&gt;和&lt;3, 3, &#39;AWM&#39;&gt;共占用11个字节。然后index_col_info len本身占用2个字节，所以加起来一共占用13个字节，把数字13就填到了index_col_info len的属性中。 3.3 UPDATE操作对应的undo日志在执行UPDATE语句时，InnoDB对更新主键和不更新主键这两种情况有截然不同的处理方案。 ①不更新主键的情况在不更新主键的情况下，又可以细分为被更新的列占用的存储空间不发生变化和发生变化的情况。 就地更新（in-place update）更新记录时，对于被更新的每个列来说，如果更新后的列和更新前的列占用的存储空间都一样大，那么就可以进行就地更新，也就是直接在原记录的基础上修改对应列的值。 先删除掉旧记录，再插入新记录在不更新主键的情况下，如果有任何一个被更新的列更新前和更新后占用的存储空间大小不一致，那么就需要先把这条旧的记录从聚簇索引页面中删除掉，然后再根据更新后列的值创建一条新的记录插入到页面中。注意，这里所说的删除并不是delete mark操作，而是真正的删除掉，也就是把这条记录从正常记录链表中移除并加入到垃圾链表中，并且修改页面中相应的统计信息（比如PAGE_FREE、PAGE_GARBAGE等这些信息）。不过这里做真正删除操作的线程并不是在DELETE语句中做purge操作时使用的另外专门的线程，而是由用户线程同步执行真正的删除操作，真正删除之后紧接着就要根据各个列更新后的值创建的新记录插入。这里如果新创建的记录占用的存储空间大小不超过旧记录占用的空间，那么可以直接重用被加入到垃圾链表中的旧记录所占用的存储空间，否则的话需要在页面中新申请一段空间以供新记录使用，如果本页面内已经没有可用的空间的话，那就需要进行页面分裂操作，然后再插入新记录。 针对UPDATE不更新主键的情况（包括上边所说的就地更新和先删除旧记录再插入新记录），InnoDB设计了一种类型为TRX_UNDO_UPD_EXIST_REC的undo日志`，它的完整结构如下： 其实大部分属性和我们介绍过的TRX_UNDO_DEL_MARK_REC类型的undo日志是类似的，不过还是要注意这么几点： n_updated属性表示本条UPDATE语句执行后将有几个列被更新，后边跟着的&lt;pos, old_len, old_value&gt;分别表示被更新列在记录中的位置、更新前该列占用的存储空间大小、更新前该列的真实值。 如果在UPDATE语句中更新的列包含索引列，那么也会添加索引列各列信息这个部分，否则的话是不会添加这个部分的。 现在继续在上边那个事务id为100的事务中更新一条记录，比如我们把id为2的那条记录更新一下： 12345678910111213BEGIN; # 显式开启一个事务，假设该事务的id为100# 插入两条记录INSERT INTO undo_demo(id, key1, col) VALUES (1, &#x27;AWM&#x27;, &#x27;狙击枪&#x27;), (2, &#x27;M416&#x27;, &#x27;步枪&#x27;); # 删除一条记录 DELETE FROM undo_demo WHERE id = 1; # 更新一条记录UPDATE undo_demo SET key1 = &#x27;M249&#x27;, col = &#x27;机枪&#x27; WHERE id = 2; 这个UPDATE语句更新的列大小都没有改动，所以可以采用就地更新的方式来执行，在真正改动页面记录时，会先记录一条类型为TRX_UNDO_UPD_EXIST_REC的undo日志，长这样： 对照着这个图我们注意一下这几个地方： 因为这条undo日志是id为100的事务中产生的第4条undo日志，所以它对应的undo no就是3。 这条日志的roll_pointer指向undo no为1的那条日志，也就是插入主键值为2的记录时产生的那条undo日志，也就是最近一次对该记录做改动时产生的undo日志。 由于本条UPDATE语句中更新了索引列key1的值，所以需要记录一下索引列各列信息部分，也就是把主键和key1列更新前的信息填入。 ②更新主键的情况在聚簇索引中，记录是按照主键值的大小连成了一个单向链表的，如果我们更新了某条记录的主键值，意味着这条记录在聚簇索引中的位置将会发生改变，比如你将记录的主键值从1更新为10000，如果还有非常多的记录的主键值分布在1 ~ 10000之间的话，那么这两条记录在聚簇索引中就有可能离得非常远，甚至中间隔了好多个页面。针对UPDATE语句中更新了记录主键值的这种情况，InnoDB在聚簇索引中分了两步处理： 将旧记录进行delete mark操作高能注意：这里是delete mark操作！这里是delete mark操作！这里是delete mark操作！也就是说在UPDATE语句所在的事务提交前，对旧记录只做一个delete mark操作，在事务提交后才由专门的线程做purge操作，把它加入到垃圾链表中。 之所以只对旧记录做delete mark操作，是因为别的事务同时也可能访问这条记录，如果把它真正的删除加入到垃圾链表后，别的事务就访问不到了。这个功能就是所谓的MVCC。 根据更新后各列的值创建一条新记录，并将其插入到聚簇索引中（需重新定位插入的位置）。由于更新后的记录主键值发生了改变，所以需要重新从聚簇索引中定位这条记录所在的位置，然后把它插进去。 针对UPDATE语句更新记录主键值的这种情况，在对该记录进行delete mark操作前，会记录一条类型为TRX_UNDO_DEL_MARK_REC的undo日志；之后插入新记录时，会记录一条类型为TRX_UNDO_INSERT_REC的undo日志，也就是说每对一条记录的主键值做改动时，会记录2条undo日志。 4.通用链表结构在写入undo日志的过程中会使用到多个链表，很多链表都有同样的节点结构，如图所示： 在某个表空间内，我们可以通过一个页的页号和在页内的偏移量来唯一定位一个节点的位置，这两个信息也就相当于指向这个节点的一个指针。所以： Pre Node Page Number和Pre Node Offset的组合就是指向前一个节点的指针 Next Node Page Number和Next Node Offset的组合就是指向后一个节点的指针。 整个List Node占用12个字节的存储空间。 为了更好的管理链表，InnoDB还提出了一个基节点的结构，里边存储了这个链表的头节点、尾节点以及链表长度信息，基节点的结构示意图如下： 其中： List Length表明该链表一共有多少节点。 First Node Page Number和First Node Offset的组合就是指向链表头节点的指针。 Last Node Page Number和Last Node Offset的组合就是指向链表尾节点的指针。 整个List Base Node占用16个字节的存储空间。 所以使用List Base Node和List Node这两个结构组成的链表的示意图就是这样： 5.FIL_PAGE_UNDO_LOG页面表空间其实是由许许多多的页面构成的，页面默认大小为16KB。这些页面有不同的类型，比如类型为FIL_PAGE_INDEX的页面用于存储聚簇索引以及二级索引，类型为FIL_PAGE_TYPE_FSP_HDR的页面用于存储表空间头部信息的，还有其他各种类型的页面，其中有一种称之为FIL_PAGE_UNDO_LOG类型的页面是专门用来存储undo日志的，这种类型的页面的通用结构如下图所示（以默认的16KB大小为例）： 我们就简称为Undo页面，上图中的File Header和File Trailer是各种页面都有的通用结构。Undo Page Header是Undo页面所特有的，我们来看一下它的结构： 其中各个属性的意思如下： TRX_UNDO_PAGE_TYPE：本页面准备存储什么种类的undo日志。前边介绍了好几种类型的undo日志，它们可以被分为两个大类： TRX_UNDO_INSERT（使用十进制1表示）：类型为TRX_UNDO_INSERT_REC的undo日志属于此大类，一般由INSERT语句产生，或者在UPDATE语句中有更新主键的情况也会产生此类型的undo日志。 TRX_UNDO_UPDATE（使用十进制2表示），除了类型为TRX_UNDO_INSERT_REC的undo日志，其他类型的undo日志都属于这个大类，比如我们前边说的TRX_UNDO_DEL_MARK_REC、TRX_UNDO_UPD_EXIST_REC啥的，一般由DELETE、UPDATE语句产生的undo日志属于这个大类。 这个TRX_UNDO_PAGE_TYPE属性可选的值就是上边的两个，用来标记本页面用于存储哪个大类的undo日志，不同大类的undo日志不能混着存储，比如一个Undo页面的TRX_UNDO_PAGE_TYPE属性值为TRX_UNDO_INSERT，那么这个页面就只能存储类型为TRX_UNDO_INSERT_REC的undo日志，其他类型的undo日志就不能放到这个页面中了。 之所以把undo日志分成两个大类，是因为类型为TRX_UNDO_INSERT_REC的undo日志在事务提交后可以直接删除掉，而其他类型的undo日志还需要为所谓的MVCC服务，不能直接删除掉，对它们的处理需要区别对待。 TRX_UNDO_PAGE_START：表示在当前页面中是从什么位置开始存储undo日志的，或者说表示第一条undo日志在本页面中的起始偏移量。 TRX_UNDO_PAGE_FREE：与上边的TRX_UNDO_PAGE_START对应，表示当前页面中存储的最后一条undo日志结束时的偏移量，或者说从这个位置开始，可以继续写入新的undo日志。假设现在向页面中写入了3条undo日志，那么TRX_UNDO_PAGE_START和TRX_UNDO_PAGE_FREE的示意图就是这样：当然，在最初一条undo日志也没写入的情况下，TRX_UNDO_PAGE_START和TRX_UNDO_PAGE_FREE的值是相同的。 TRX_UNDO_PAGE_NODE：代表一个List Node结构（链表的普通节点，我们上边刚说的）。 6.Undo页面链表6.1单个事务中的Undo页面链表因为一个事务可能包含多个语句，而且一个语句可能对若干条记录进行改动，而对每条记录进行改动前，都需要记录1条或2条的undo日志，所以在一个事务执行过程中可能产生很多undo日志，这些日志可能一个页面放不下，需要放到多个页面中，这些页面就通过我们上边介绍的TRX_UNDO_PAGE_NODE属性连成了链表： 我特意把链表中的第一个Undo页面给标了出来，称它为first undo page，其余的Undo页面称之为normal undo page，这是因为在first undo page中除了记录Undo Page Header之外，还会记录其他的一些管理信息。 在一个事务执行过程中，可能混着执行INSERT、DELETE、UPDATE语句，也就意味着会产生不同类型的undo日志。但是，同一个Undo页面要么只存储TRX_UNDO_INSERT大类的undo日志，要么只存储TRX_UNDO_UPDATE大类的undo日志，反正不能混着存，所以在一个事务执行过程中就可能需要2个Undo页面的链表，一个称之为insert undo链表，另一个称之为update undo链表，画个示意图就是这样： 另外，InnoDB规定对普通表和临时表的记录改动时产生的undo日志要分别记录（我们稍后阐释为啥这么做），所以在一个事务中最多有4个以Undo页面为节点组成的链表： 当然，并不是在事务一开始就会为这个事务分配这4个链表，具体分配策略如下： 刚刚开启事务时，一个Undo页面链表也不分配。 当事务执行过程中向普通表中插入记录或者执行更新记录主键的操作之后，就会为其分配一个普通表的insert undo链表。 当事务执行过程中删除或者更新了普通表中的记录之后，就会为其分配一个普通表的update undo链表。 当事务执行过程中向临时表中插入记录或者执行更新记录主键的操作之后，就会为其分配一个临时表的insert undo链表。 当事务执行过程中删除或者更新了临时表中的记录之后，就会为其分配一个临时表的update undo链表。 总结一句就是：按需分配，啥时候需要啥时候再分配，不需要就不分配。 6.2多个事务中的Undo页面链表为了尽可能提高undo日志的写入效率，不同事务执行过程中产生的undo日志需要被写入到不同的Undo页面链表中。比方说现在有事务id分别为1、2的两个事务，我们分别称之为trx 1和trx 2，假设在这两个事务执行过程中： trx 1对普通表做了DELETE操作，对临时表做了INSERT和UPDATE操作。InnoDB会为trx 1分配3个链表，分别是： 针对普通表的update undo链表 针对临时表的insert undo链表 针对临时表的update undo链表。 trx 2对普通表做了INSERT、UPDATE和DELETE操作，没有对临时表做改动。InnoDB会为trx 2分配2个链表，分别是： 针对普通表的insert undo链表 针对普通表的update undo链表。 综上所述，在trx 1和trx 2执行过程中，InnoDB共需为这两个事务分配5个Undo页面链表，画个图就是这样： 如果有更多的事务，那就意味着可能会产生更多的Undo页面链表。 7.undo日志具体写入过程7.1段（Segment）的概念段是一个逻辑上的概念，本质上是由若干个零散页面和若干个完整的区组成的。比如一个B+树索引被划分成两个段，一个叶子节点段，一个非叶子节点段，这样叶子节点就可以被尽可能的存到一起，非叶子节点被尽可能的存到一起。每一个段对应一个INODE Entry结构，这个INODE Entry结构描述了这个段的各种信息，比如段的ID，段内的各种链表基节点，零散页面的页号有哪些等信息。我为了定位一个INODE Entry，InnoDB设计了一个Segment Header的结构： 整个Segment Header占用10个字节大小，各个属性的意思如下： Space ID of the INODE Entry：INODE Entry结构所在的表空间ID。 Page Number of the INODE Entry：INODE Entry结构所在的页面页号。 Byte Offset of the INODE Ent：INODE Entry结构在该页面中的偏移量 知道了表空间ID、页号、页内偏移量，就可以唯一定位一个INODE Entry的地址。 7.2Undo Log Segment HeaderInnoDB规定，每一个Undo页面链表都对应着一个段，称之为Undo Log Segment。也就是说链表中的页面都是从这个段里边申请的，所以他们在Undo页面链表的第一个页面，也就是上边提到的first undo page中设计了一个称之为Undo Log Segment Header的部分，这个部分中包含了该链表对应的段的segment header信息以及其他的一些关于这个段的信息，所以Undo页面链表的第一个页面其实长这样： 可以看到这个Undo链表的第一个页面比普通页面多了个Undo Log Segment Header，我们来看一下它的结构： 其中各个属性的意思如下： TRX_UNDO_STATE：本Undo页面链表处在什么状态。一个Undo Log Segment可能处在的状态包括： TRX_UNDO_ACTIVE：活跃状态，也就是一个活跃的事务正在往这个段里边写入undo日志。 TRX_UNDO_CACHED：被缓存的状态。处在该状态的Undo页面链表等待着之后被其他事务重用。 TRX_UNDO_TO_FREE：对于insert undo链表来说，如果在它对应的事务提交之后，该链表不能被重用，那么就会处于这种状态。 TRX_UNDO_TO_PURGE：对于update undo链表来说，如果在它对应的事务提交之后，该链表不能被重用，那么就会处于这种状态。 TRX_UNDO_PREPARED：包含处于PREPARE阶段的事务产生的undo日志。 TRX_UNDO_LAST_LOG：本Undo页面链表中最后一个Undo Log Header的位置。 TRX_UNDO_FSEG_HEADER：本Undo页面链表对应的段的Segment Header信息。 TRX_UNDO_PAGE_LIST：Undo页面链表的基节点。 Undo页面的Undo Page Header部分有一个12字节大小的TRX_UNDO_PAGE_NODE属性，这个属性代表一个List Node结构。每一个Undo页面都包含Undo Page Header结构，这些页面就可以通过这个属性连成一个链表。这个TRX_UNDO_PAGE_LIST属性代表着这个链表的基节点，当然这个基节点只存在于Undo页面链表的第一个页面，也就是first undo page中。 7.3Undo Log Header一个事务在向Undo页面中写入undo日志时的方式是十分简单暴力的，就是直接往里怼，写完一条紧接着写另一条，各条undo日志之间是亲密无间的。写完一个Undo页面后，再从段里申请一个新页面，然后把这个页面插入到Undo页面链表中，继续往这个新申请的页面中写。InnoDB认为同一个事务向一个Undo页面链表中写入的undo日志算是一个组，比方说我们上边介绍的trx 1由于会分配3个Undo页面链表，也就会写入3个组的undo日志；trx 2由于会分配2个Undo页面链表，也就会写入2个组的undo日志。在每写入一组undo日志时，都会在这组undo日志前先记录一下关于这个组的一些属性，InnoDB把存储这些属性的地方称之为Undo Log Header。所以Undo页面链表的第一个页面在真正写入undo日志前，其实都会被填充Undo Page Header、Undo Log Segment Header、Undo Log Header这3个部分，如图所示： 这个Undo Log Header具体的结构如下： 我们先大致看一下它们都是啥意思： TRX_UNDO_TRX_ID：生成本组undo日志的事务id。 TRX_UNDO_TRX_NO：事务提交后生成的一个需要序号，使用此序号来标记事务的提交顺序（先提交的此序号小，后提交的此序号大）。 TRX_UNDO_DEL_MARKS：标记本组undo日志中是否包含由于Delete mark操作产生的undo日志。 TRX_UNDO_LOG_START：表示本组undo日志中第一条undo日志的在页面中的偏移量。 TRX_UNDO_XID_EXISTS：本组undo日志是否包含XID信息。 TRX_UNDO_DICT_TRANS：标记本组undo日志是不是由DDL语句产生的。 TRX_UNDO_TABLE_ID：如果TRX_UNDO_DICT_TRANS为真，那么本属性表示DDL语句操作的表的table id。 TRX_UNDO_NEXT_LOG：下一组的undo日志在页面中开始的偏移量。 TRX_UNDO_PREV_LOG：上一组的undo日志在页面中开始的偏移量。 一般来说一个Undo页面链表只存储一个事务执行过程中产生的一组undo日志，但是在某些情况下，可能会在一个事务提交之后，之后开启的事务重复利用这个Undo页面链表，这样就会导致一个Undo页面中可能存放多组Undo日志，TRX_UNDO_NEXT_LOG和TRX_UNDO_PREV_LOG就是用来标记下一组和上一组undo日志在页面中的偏移量的。 TRX_UNDO_HISTORY_NODE：一个12字节的List Node结构，代表一个称之为History链表的节点。 7.4小结对于没有被重用的Undo页面链表来说，链表的第一个页面，也就是first undo page在真正写入undo日志前，会填充Undo Page Header、Undo Log Segment Header、Undo Log Header这3个部分，之后才开始正式写入undo日志。对于其他的页面来说，也就是normal undo page在真正写入undo日志前，只会填充Undo Page Header。链表的List Base Node存放到first undo page的Undo Log Segment Header部分，List Node信息存放到每一个Undo页面的undo Page Header部分，所以画一个Undo页面链表的示意图就是这样： 8.重用Undo页面为了能提高并发执行的多个事务写入undo日志的性能，InnoDB决定为每个事务单独分配相应的Undo页面链表（最多可能单独分配4个链表）。但是这样也造成了一些问题，比如其实大部分事务执行过程中可能只修改了一条或几条记录，针对某个Undo页面链表只产生了非常少的undo日志，这些undo日志可能只占用一点存储空间，每开启一个事务就新创建一个Undo页面链表（虽然这个链表中只有一个页面）来存储这么一点undo日志岂不是太浪费了么？的确是挺浪费，于是InnoDB决定在事务提交后在某些情况下重用该事务的Undo页面链表。一个Undo页面链表是否可以被重用的条件很简单： 该链表中只包含一个Undo页面。如果一个事务执行过程中产生了非常多的undo日志，那么它可能申请非常多的页面加入到Undo页面链表中。在该事物提交后，如果将整个链表中的页面都重用，那就意味着即使新的事务并没有向该Undo页面链表中写入很多undo日志，那该链表中也得维护非常多的页面，那些用不到的页面也不能被别的事务所使用，这样就造成了另一种浪费。所以InnoDB规定，只有在Undo页面链表中只包含一个Undo页面时，该链表才可以被下一个事务所重用。 该Undo页面已经使用的空间小于整个页面空间的3/4。 Undo页面链表按照存储的undo日志所属的大类可以被分为insert undo链表和update undo链表两种，这两种链表在被重用时的策略也是不同的，我们分别看一下： insert undo链表insert undo链表中只存储类型为TRX_UNDO_INSERT_REC的undo日志，这种类型的undo日志在事务提交之后就没用了，就可以被清除掉。所以在某个事务提交后，重用这个事务的insert undo链表（这个链表中只有一个页面）时，可以直接把之前事务写入的一组undo日志覆盖掉，从头开始写入新事务的一组undo日志，如下图所示：如图所示，假设有一个事务使用的insert undo链表，到事务提交时，只向insert undo链表中插入了3条undo日志，这个insert undo链表只申请了一个Undo页面。假设此刻该页面已使用的空间小于整个页面大小的3/4，那么下一个事务就可以重用这个insert undo链表（链表中只有一个页面)。假设此时有一个新事务重用了该insert undo链表，那么可以直接把旧的一组undo日志覆盖掉，写入一组新的undo日志。 在重用Undo页面链表写入新的一组undo日志时，不仅会写入新的Undo Log Header，还会适当调整Undo Page Header、Undo Log Segment Header、Undo Log Header中的一些属性，比如TRX_UNDO_PAGE_START、TRX_UNDO_PAGE_FREE等等。 update undo链表在一个事务提交后，它的update undo链表中的undo日志也不能立即删除掉（这些日志用于MVCC）。所以如果之后的事务想重用update undo链表时，就不能覆盖之前事务写入的undo日志。这样就相当于在同一个Undo页面中写入了多组的undo日志，效果看起来就是这样： 9.回滚段9.1回滚段的概念一个事务在执行过程中最多可以分配4个Undo页面链表，在同一时刻不同事务拥有的Undo页面链表是不一样的，所以在同一时刻系统里其实可以有许许多多个Undo页面链表存在。为了更好的管理这些链表，InnoDB又设计了一个称之为Rollback Segment Header的页面，在这个页面中存放了各个Undo页面链表的frist undo page的页号，这些页号称之为undo slot。可以这样理解，每个Undo页面链表都相当于是一个班，这个链表的first undo page就相当于这个班的班长，找到了这个班的班长，就可以找到班里的其他同学（其他同学相当于normal undo page）。有时候学校需要向这些班级传达一下精神，就需要把班长都召集在会议室，这个Rollback Segment Header就相当于是一个会议室。 我们看一下这个称之为Rollback Segment Header的页面长啥样（以默认的16KB为例）： InnoDB规定，每一个Rollback Segment Header页面都对应着一个段，这个段就称为Rollback Segment，翻译过来就是回滚段。与之前介绍的各种段不同的是，这个Rollback Segment里其实只有一个页面。 了解了Rollback Segment的含义之后，我们再来看看这个称之为Rollback Segment Header的页面的各个部分的含义都是啥意思： TRX_RSEG_MAX_SIZE：本Rollback Segment中管理的所有Undo页面链表中的Undo页面数量之和的最大值。换句话说，本Rollback Segment中所有Undo页面链表中的Undo页面数量之和不能超过TRX_RSEG_MAX_SIZE代表的值。该属性的值默认为无限大，也就是我们想写多少Undo页面都可以。 无限大其实也只是个夸张的说法，4个字节能表示最大的数也就是0xFFFFFFFF，但是0xFFFFFFFF这个数有特殊用途，所以实际上TRX_RSEG_MAX_SIZE的值为0xFFFFFFFE。 TRX_RSEG_HISTORY_SIZE：History链表占用的页面数量。 TRX_RSEG_HISTORY：History链表的基节点。 TRX_RSEG_FSEG_HEADER：本Rollback Segment对应的10字节大小的Segment Header结构，通过它可以找到本段对应的INODE Entry。 TRX_RSEG_UNDO_SLOTS：各个Undo页面链表的first undo page的页号集合，也就是undo slot集合。一个页号占用4个字节，对于16KB大小的页面来说，这个TRX_RSEG_UNDO_SLOTS部分共存储了1024个undo slot，所以共需1024 × 4 = 4096个字节。 9.2 从回滚段中申请Undo页面链表初始情况下，由于未向任何事务分配任何Undo页面链表，所以对于一个Rollback Segment Header页面来说，它的各个undo slot都被设置成了一个特殊的值：FIL_NULL（对应的十六进制就是0xFFFFFFFF），表示该undo slot不指向任何页面。 随着时间的流逝，开始有事务需要分配Undo页面链表了，就从回滚段的第一个undo slot开始，看看该undo slot的值是不是FIL_NULL： 如果是FIL_NULL，那么在表空间中新创建一个段（也就是Undo Log Segment），然后从段里申请一个页面作为Undo页面链表的first undo page，然后把该undo slot的值设置为刚刚申请的这个页面的页号，这样也就意味着这个undo slot被分配给了这个事务。 如果不是FIL_NULL，说明该undo slot已经指向了一个undo链表，也就是说这个undo slot已经被别的事务占用了，那就跳到下一个undo slot，判断该undo slot的值是不是FIL_NULL，重复上边的步骤。 一个Rollback Segment Header页面中包含1024个undo slot，如果这1024个undo slot的值都不为FIL_NULL，这就意味着这1024个undo slot都已经被分配给了某个事务，此时由于新事务无法再获得新的Undo页面链表，就会回滚这个事务并且给用户报错： 1Too many active concurrent transactions 用户看到这个错误，可以选择重新执行这个事务（可能重新执行时有别的事务提交了，该事务就可以被分配Undo页面链表了）。 当一个事务提交时，它所占用的undo slot有两种命运： 如果该undo slot指向的Undo页面链表符合被重用的条件（就是我们上边说的Undo页面链表只占用一个页面并且已使用空间小于整个页面的3/4）。该undo slot就处于被缓存的状态，InnoDB规定这时该Undo页面链表的TRX_UNDO_STATE属性（该属性在first undo page的Undo Log Segment Header部分）会被设置为TRX_UNDO_CACHED。 被缓存的undo slot都会被加入到一个链表，根据对应的Undo页面`链表的类型不同，也会被加入到不同的链表： 如果对应的Undo页面链表是insert undo链表，则该undo slot会被加入insert undo cached链表。 如果对应的Undo页面链表是update undo链表，则该undo slot会被加入update undo cached链表。 一个回滚段就对应着上述两个cached链表，如果有新事务要分配undo slot时，先从对应的cached链表中找。如果没有被缓存的undo slot，才会到回滚段的Rollback Segment Header页面中再去找。 如果该undo slot指向的Undo页面链表不符合被重用的条件，那么针对该undo slot对应的Undo页面链表类型不同，也会有不同的处理： 如果对应的Undo页面链表是insert undo链表，则该Undo页面链表的TRX_UNDO_STATE属性会被设置为TRX_UNDO_TO_FREE，之后该Undo页面链表对应的段会被释放掉（也就意味着段中的页面可以被挪作他用），然后把该undo slot的值设置为FIL_NULL。 如果对应的Undo页面链表是update undo链表，则该Undo页面链表的TRX_UNDO_STATE属性会被设置为TRX_UNDO_TO_PRUGE，则会将该undo slot的值设置为FIL_NULL，然后将本次事务写入的一组undo日志放到所谓的History链表中（需要注意的是，这里并不会将Undo页面链表对应的段给释放掉，因为这些undo日志还有用呢～）。 9.3多个回滚段一个事务执行过程中最多分配4个Undo页面链表，而一个回滚段里只有1024个undo slot，很显然undo slot的数量有点少。即使假设一个读写事务执行过程中只分配1个Undo页面链表，那1024个undo slot也只能支持1024个读写事务同时执行，再多了就崩溃了 在InnoDB的早期发展阶段的确只有一个回滚段，但是InnoDB后来意识到了这个问题，所以InnoDB一口气定义了128个回滚段，也就相当于有了128 × 1024 = 131072个undo slot。假设一个读写事务执行过程中只分配1个Undo页面链表，那么就可以同时支持131072个读写事务并发执行。 只读事务并不需要分配Undo页面链表，MySQL 5.7中所有刚开启的事务默认都是只读事务，只有在事务执行过程中对记录做了某些改动时才会被升级为读写事务。 每个回滚段都对应着一个Rollback Segment Header页面，有128个回滚段，自然就要有128个Rollback Segment Header页面，这些页面的地址需要找个地方存一下！于是InnoDB在系统表空间的第5号页面的某个区域包含了128个8字节大小的格子： 每个8字节的格子的构造就像这样： 如果所示，每个8字节的格子其实由两部分组成： 4字节大小的Space ID，代表一个表空间的ID。 4字节大小的Page number，代表一个页号。 也就是说每个8字节大小的格子相当于一个指针，指向某个表空间中的某个页面，这些页面就是Rollback Segment Header。这里需要注意的一点事，要定位一个Rollback Segment Header还需要知道对应的表空间ID，这也就意味着不同的回滚段可能分布在不同的表空间中。 所以通过上边的叙述我们可以大致清楚，在系统表空间的第5号页面中存储了128个Rollback Segment Header页面地址，每个Rollback Segment Header就相当于一个回滚段。在Rollback Segment Header页面中，又包含1024个undo slot，每个undo slot都对应一个Undo页面链表。我们画个示意图： 9.4回滚段的分类我们把这128个回滚段给编一下号，最开始的回滚段称之为第0号回滚段，之后依次递增，最后一个回滚段就称之为第127号回滚段。这128个回滚段可以被分成两大类： 第0号、第33～127号回滚段属于一类。其中第0号回滚段必须在系统表空间中（就是说第0号回滚段对应的Rollback Segment Header页面必须在系统表空间中），第33～127号回滚段既可以在系统表空间中，也可以在自己配置的undo表空间中。如果一个事务在执行过程中由于对普通表的记录做了改动需要分配Undo页面链表时，必须从这一类的段中分配相应的undo slot。 第1～32号回滚段属于一类。这些回滚段必须在临时表空间（对应着数据目录中的ibtmp1文件）中。如果一个事务在执行过程中由于对临时表的记录做了改动需要分配Undo页面链表时，必须从这一类的段中分配相应的undo slot。 也就是说如果一个事务在执行过程中既对普通表的记录做了改动，又对临时表的记录做了改动，那么需要为这个记录分配2个回滚段，再分别到这两个回滚段中分配对应的undo slot。 为啥要把针对普通表和临时表来划分不同种类的回滚段呢？这个还得从Undo页面本身说起，我们说Undo页面其实是类型为FIL_PAGE_UNDO_LOG的页面的简称，说到底它也是一个普通的页面。我们前边说过，在修改页面之前一定要先把对应的redo日志写上，这样在系统奔溃重启时才能恢复到奔溃前的状态。我们向Undo页面写入undo日志本身也是一个写页面的过程，InnoDB为此还设计了许多种redo日志的类型，比方说MLOG_UNDO_HDR_CREATE、MLOG_UNDO_INSERT、MLOG_UNDO_INIT等等等等，也就是说我们对Undo页面做的任何改动都会记录相应类型的redo日志。但是对于临时表来说，因为修改临时表而产生的undo日志只需要在系统运行过程中有效，如果系统奔溃了，那么在重启时也不需要恢复这些undo日志所在的页面，所以在写针对临时表的Undo页面时，并不需要记录相应的redo日志。总结一下针对普通表和临时表划分不同种类的回滚段的原因：在修改针对普通表的回滚段中的Undo页面时，需要记录对应的redo日志，而修改针对临时表的回滚段中的Undo页面时，不需要记录对应的redo日志。 实际上在MySQL 5.7.21这个版本中，如果我们仅仅对普通表的记录做了改动，那么只会为该事务分配针对普通表的回滚段，不分配针对临时表的回滚段。但是如果我们仅仅对临时表的记录做了改动，那么既会为该事务分配针对普通表的回滚段，又会为其分配针对临时表的回滚段（不过分配了回滚段并不会立即分配undo slot，只有在真正需要Undo页面链表时才会去分配回滚段中的undo slot）。 9.5为事务分配Undo页面链表详细过程接下来以事务对普通表的记录做改动为例，梳理一下事务执行过程中分配Undo页面链表时的完整过程： 事务在执行过程中对普通表的记录首次做改动之前，首先会到系统表空间的第5号页面中分配一个回滚段（其实就是获取一个Rollback Segment Header页面的地址）。一旦某个回滚段被分配给了这个事务，那么之后该事务中再对普通表的记录做改动时，就不会重复分配了。使用round-robin（循环使用）方式来分配回滚段。比如当前事务分配了第0号回滚段，那么下一个事务就要分配第33号回滚段，下下个事务就要分配第34号回滚段，简单一点的说就是这些回滚段被轮着分配给不同的事务。 在分配到回滚段后，首先看一下这个回滚段的两个cached链表有没有已经缓存了的undo slot，比如如果事务做的是INSERT操作，就去回滚段对应的insert undo cached链表中看看有没有缓存的undo slot；如果事务做的是DELETE操作，就去回滚段对应的update undo cached链表中看看有没有缓存的undo slot。如果有缓存的undo slot，那么就把这个缓存的undo slot分配给该事务。 如果没有缓存的undo slot可供分配，那么就要到Rollback Segment Header页面中找一个可用的undo slot分配给当前事务。从Rollback Segment Header页面中分配可用的undo slot的方式我们上边也说过了，就是从第0个undo slot开始，如果该undo slot的值为FIL_NULL，意味着这个undo slot是空闲的，就把这个undo slot分配给当前事务，否则查看第1个undo slot是否满足条件，依次类推，直到最后一个undo slot。如果这1024个undo slot都没有值为FIL_NULL的情况，就直接报错（一般不会出现这种情况）。 找到可用的undo slot后，如果该undo slot是从cached链表中获取的，那么它对应的Undo Log Segment已经分配了，否则的话需要重新分配一个Undo Log Segment，然后从该Undo Log Segment中申请一个页面作为Undo页面链表的first undo page。 然后事务就可以把undo日志写入到上边申请的Undo页面链表了！ 对临时表的记录做改动的步骤和上述的一样。不过需要再次强调一次，如果一个事务在执行过程中既对普通表的记录做了改动，又对临时表的记录做了改动，那么需要为这个记录分配2个回滚段。并发执行的不同事务其实也可以被分配相同的回滚段，只要分配不同的undo slot就可以了。 10.回滚段相关配置10.1配置回滚段数量系统中一共有128个回滚段，其实这只是默认值，我们可以通过启动参数innodb_rollback_segments来配置回滚段的数量，可配置的范围是1~128。但是这个参数并不会影响针对临时表的回滚段数量，针对临时表的回滚段数量一直是32，也就是说： 如果我们把innodb_rollback_segments的值设置为1，那么只会有1个针对普通表的可用回滚段，但是仍然有32个针对临时表的可用回滚段。 如果我们把innodb_rollback_segments的值设置为2～33之间的数，效果和将其设置为1是一样的。 如果我们把innodb_rollback_segments设置为大于33的数，那么针对普通表的可用回滚段数量就是该值减去32。 10.2 配置undo表空间默认情况下，针对普通表设立的回滚段（第0号以及第33~127号回滚段）都是被分配到系统表空间的。其中的第0号回滚段是一直在系统表空间的，但是第33~127号回滚段可以通过配置放到自定义的undo表空间中。但是这种配置只能在系统初始化（创建数据目录时）的时候使用，一旦初始化完成，之后就不能再次更改了。我们看一下相关启动参数： 通过innodb_undo_directory指定undo表空间所在的目录，如果没有指定该参数，则默认undo表空间所在的目录就是数据目录。 通过innodb_undo_tablespaces定义undo表空间的数量。该参数的默认值为0，表明不创建任何undo表空间。第33~127号回滚段可以平均分布到不同的undo表空间中。 如果我们在系统初始化的时候指定了创建了undo表空间，那么系统表空间中的第0号回滚段将处于不可用状态。 比如我们在系统初始化时指定的innodb_rollback_segments为35，innodb_undo_tablespaces为2，这样就会将第33、34号回滚段分别分布到一个undo表空间中。 设立undo表空间的一个好处就是在undo表空间中的文件大到一定程度时，可以自动的将该undo表空间截断（truncate）成一个小文件。而系统表空间的大小只能不断的增大，却不能截断。 11.总结为了保证事务的原子性设计，InnoDB引入了undo日志。undo日志记载了回滚一个段所需的必要内容。 在事务对表中的记录进行改动的时候，才会为这个事务分配一个唯一的ID。事务ID值是一个递增的数字。先被分配ID的事务得到的是较小的事务ID，后被分配ID的事务得到的是较大的事务ID。未被分配事务ID的事务ID默认是0。聚簇索引记录中有一个trx_id隐藏列，他代表对这个聚簇索引隐藏记录进行改动的语句所在的事务对应的事务ID。 InnoDB针对不同的场景设计了不同类型的undo日志。 类型为FIL_PAGE_UNDO_LOG的页面是专门用来存储undo日志的，简称为undo页面。 在一个事务执行过程中，最多分配四个undo页面链表： 针对普通表的insert undo链表 针对普通表的update undo链表 针对临时表的insert undo链表 针对临时表的update undo链表 只有在真正用到这些链表的时候才会去创建他们。 每个undo页面链表都对应一个undo log segment。undo页面链表的第一个页面中有一个名为undo log segment header 的部分，专门用来存储关于这个段的一些信息。 同一个事务向一个undo页面链表中写入的undo日志算是一个组，每个组都以一个undo log header部分开头。 一个undo页面链表如果可以被重用，需要符合两个条件： 该链表只包含一个undo页面 该undo页面已经使用的空间小于整个页面空间的3/4 每一个Rollback segmrnt header 页面都对应一个回滚段，每个回滚段包含1024个undo slot，一个undo slot代表一个undo页面链表的第一个页面的页号。目前，InnoDB最多支持128个回滚段，其中第0号，第33127号回滚段是针对普通表设计的，第132号回滚段是针对临时表设计的。 我们可以选择将undo日志记录到专门的undo表空间中，在undo表空间中的文件大到一定程度时，可以自动将该undo表空间截断为小文件。","categories":[{"name":"7.mysql","slug":"7-mysql","permalink":"https://zhangxin66666.github.io/categories/7-mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"}]}],"categories":[{"name":"1.基础知识","slug":"1-基础知识","permalink":"https://zhangxin66666.github.io/categories/1-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"7.mysql","slug":"7-mysql","permalink":"https://zhangxin66666.github.io/categories/7-mysql/"},{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/categories/%E9%9D%A2%E8%AF%95/"},{"name":"11.方法论","slug":"11-方法论","permalink":"https://zhangxin66666.github.io/categories/11-%E6%96%B9%E6%B3%95%E8%AE%BA/"}],"tags":[{"name":"集合源码分析","slug":"集合源码分析","permalink":"https://zhangxin66666.github.io/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"Mysql","slug":"Mysql","permalink":"https://zhangxin66666.github.io/tags/Mysql/"},{"name":"面试","slug":"面试","permalink":"https://zhangxin66666.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"算法","slug":"算法","permalink":"https://zhangxin66666.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"方法论","slug":"方法论","permalink":"https://zhangxin66666.github.io/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"}]}